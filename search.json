[
  {
    "objectID": "blogs.html",
    "href": "blogs.html",
    "title": "Blogs",
    "section": "",
    "text": "Vision Language Models\n\n\n\nvlm\n\nquantization\n\n\n\nVLMs, their memory usage and quantization\n\n\n\n\n\nMay 30, 2025\n\n\nShataxi Dubey\n\n\n\n\n\n\n\n\n\n\n\n\nObject detection pipeline\n\n\n\nobject detection\n\n\n\nObject detection pipeline\n\n\n\n\n\nAug 1, 2024\n\n\nShataxi Dubey\n\n\n\n\n\n\n\n\n\n\n\n\nNon max suppression\n\n\n\nobject detection\n\nnms\n\n\n\nNon max suppression suppresses bounding box with non max confidence scores\n\n\n\n\n\nAug 1, 2024\n\n\nShataxi Dubey\n\n\n\n\n\n\n\n\n\n\n\n\nPrecision and Recall\n\n\n\nmetrics\n\n\n\nSignificance of precision and recall\n\n\n\n\n\nAug 1, 2024\n\n\nShataxi Dubey\n\n\n\n\n\n\n\n\n\n\n\n\nMean Average Precision\n\n\n\nobject detection\n\nMAP\n\n\n\nAn important metric in Object detection tasks\n\n\n\n\n\nMay 23, 2024\n\n\nShataxi Dubey\n\n\n\n\n\n\n\n\n\n\n\n\nEvolution in object detection\n\n\n\nobject detection\n\nSOTA\n\n\n\nstate of the art models\n\n\n\n\n\nMay 23, 2024\n\n\nShataxi Dubey\n\n\n\n\n\n\n\n\n\n\n\n\nPython Format Specifier\n\n\n\nprecision\n\nformat\n\n\n\nFormatting numbers\n\n\n\n\n\nMay 9, 2024\n\n\nShataxi Dubey\n\n\n\n\n\n\n\n\n\n\n\n\nImage Transformation\n\n\n\nimages\n\nopencv\n\n\n\nVarious ways to transform image\n\n\n\n\n\nDec 24, 2023\n\n\nShataxi Dubey\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Image-transformation.html",
    "href": "posts/Image-transformation.html",
    "title": "Image Transformation",
    "section": "",
    "text": "import numpy as np\nimport cv2\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/Image-transformation.html#reading-image",
    "href": "posts/Image-transformation.html#reading-image",
    "title": "Image Transformation",
    "section": "Reading Image",
    "text": "Reading Image\n\n# read the input image\nimg = cv2.imread(\"../images/trees.jpg\")\n# convert from BGR to RGB so we can plot using matplotlib\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n# disable x & y axis\nplt.axis('off')\n# show the image\nplt.imshow(img)\nplt.show()"
  },
  {
    "objectID": "posts/Image-transformation.html#translation-of-image",
    "href": "posts/Image-transformation.html#translation-of-image",
    "title": "Image Transformation",
    "section": "Translation of image",
    "text": "Translation of image\n\n# get the image shape\nprint(img.shape)\nrows, cols, dim = img.shape\n# transformation matrix for translation\nM = np.float32([[1, 0, 500],\n                [0, 1, 500],\n                [0, 0, 1]])\n# apply a perspective transformation to the image\ntranslated_img = cv2.warpPerspective(img, M, (cols, rows))\n# disable x & y axis\nplt.axis('off')\n# show the resulting image\nplt.imshow(translated_img)\nplt.show()\n\n(1606, 2222, 3)"
  },
  {
    "objectID": "posts/Image-transformation.html#scaling-of-image",
    "href": "posts/Image-transformation.html#scaling-of-image",
    "title": "Image Transformation",
    "section": "Scaling of Image",
    "text": "Scaling of Image\n\n# get the image shape\nprint(img.shape)\nrows, cols, dim = img.shape\n#transformation matrix for Scaling\nM = np.float32([[1.5, 0  , 0],\n                [0,   1.8, 0],\n                [0,   0,   1]])\n# apply a perspective transformation to the image\nscaled_img = cv2.warpPerspective(img,M,(cols,rows))\n# disable x & y axis\nplt.axis('off')\n# show the resulting image\nplt.imshow(scaled_img)\nplt.show()\n\n(1606, 2222, 3)"
  },
  {
    "objectID": "posts/Image-transformation.html#shearing-of-image",
    "href": "posts/Image-transformation.html#shearing-of-image",
    "title": "Image Transformation",
    "section": "Shearing of Image",
    "text": "Shearing of Image\n\n# get the image shape\nprint(img.shape)\nrows, cols, dim = img.shape\n# transformation matrix for Shearing\n# shearing applied to x-axis\nM = np.float32([[1, 0.5, 0],\n                [0, 1  , 0],\n                [0, 0  , 1]])\n# shearing applied to y-axis\n# M = np.float32([[1,   0, 0],\n#                 [0.5, 1, 0],\n#                 [0,   0, 1]])\n# apply a perspective transformation to the image                \nsheared_img = cv2.warpPerspective(img,M,(cols,rows))\n# disable x & y axis\nplt.axis('off')\n# show the resulting image\nplt.imshow(sheared_img)\nplt.show()\n\n(1606, 2222, 3)"
  },
  {
    "objectID": "posts/Image-transformation.html#reflection-of-image",
    "href": "posts/Image-transformation.html#reflection-of-image",
    "title": "Image Transformation",
    "section": "Reflection of Image",
    "text": "Reflection of Image\n\n# get the image shape\nrows, cols, dim = img.shape\n# transformation matrix for x-axis reflection \nM1 = np.float32([[1,  0, 0   ],\n                [0, -1, rows],\n                [0,  0, 1   ]])\n# transformation matrix for y-axis reflection\nM2 = np.float32([[-1, 0, cols],\n                [ 0, 1, 0   ],\n                [ 0, 0, 1   ]])\n# apply a perspective transformation to the image\nh_reflected_img = cv2.warpPerspective(img,M1,(cols,rows))\nv_reflected_img = cv2.warpPerspective(img,M2,(cols,rows))\n# disable x & y axis\nplt.axis('off')\n# show the resulting image\ncombined=np.hstack((h_reflected_img,v_reflected_img))\n\nplt.title('X-axis reflection and Y-axis reflection')\nplt.imshow(combined)\nplt.show()"
  },
  {
    "objectID": "posts/Image-transformation.html#rotation-of-image",
    "href": "posts/Image-transformation.html#rotation-of-image",
    "title": "Image Transformation",
    "section": "Rotation of Image",
    "text": "Rotation of Image\n\n# get the image shape\nrows, cols, dim = img.shape\n#angle from degree to radian\nangle = np.radians(10)\n#transformation matrix for Rotation\nM = np.float32([[np.cos(angle), -(np.sin(angle)), 0],\n                [np.sin(angle), np.cos(angle), 0],\n                [0, 0, 1]])\n# apply a perspective transformation to the image\nrotated_img = cv2.warpPerspective(img, M, (int(cols),int(rows)))\n# disable x & y axis\nplt.axis('off')\n# show the resulting image\nplt.imshow(rotated_img)\nplt.show()"
  },
  {
    "objectID": "posts/Image-transformation.html#cropping-of-image",
    "href": "posts/Image-transformation.html#cropping-of-image",
    "title": "Image Transformation",
    "section": "Cropping of Image",
    "text": "Cropping of Image\n\n# get 200 pixels from 100 to 300 on both x-axis & y-axis\n# change that if you will, just make sure you don't exceed cols & rows\ncropped_img = img[100:300, 100:300]\n# disable x & y axis\nplt.axis('off')\n# show the resulting image\nplt.imshow(cropped_img)\nplt.show()"
  },
  {
    "objectID": "posts/Image-transformation.html#center-crop-of-image",
    "href": "posts/Image-transformation.html#center-crop-of-image",
    "title": "Image Transformation",
    "section": "Center Crop of Image",
    "text": "Center Crop of Image\n\n# get the image width & height\nwidth, height = img.shape[1], img.shape[0]\n\n# get the image cropped width & height\ncrop_width , crop_height = 500, 600\n\nhalf_crop_width , half_crop_height = int(crop_width/2) , int(crop_height/2)\n\nmid_x, mid_y = int(width/2), int(height/2)\n\ncenter_cropped_img=img[mid_y-half_crop_height : mid_y+half_crop_height, mid_x-half_crop_width : mid_x+half_crop_width]\n\n# disable x & y axis\nplt.axis('off')\n# show the resulting image\nplt.imshow(center_cropped_img)\nplt.show()"
  },
  {
    "objectID": "posts/non_max_suppression.html",
    "href": "posts/non_max_suppression.html",
    "title": "Non max suppression",
    "section": "",
    "text": "Remove all the bounding boxes whose confidence is less than confidence threshld\nFor a particular category, find the bounding box with highest confidence score\nCheck its IOU with the other bounding boxes of same category\nIf the IOU &gt; IOU threshold, this means that the box is bounding the same object in such case, we remove or suppress these boxes."
  },
  {
    "objectID": "posts/SOTA_object_detection.html",
    "href": "posts/SOTA_object_detection.html",
    "title": "Evolution in object detection",
    "section": "",
    "text": "two stage detector â€“&gt; one stage detector â€“&gt; anchor free"
  },
  {
    "objectID": "posts/object_detection_pipeline.html",
    "href": "posts/object_detection_pipeline.html",
    "title": "Object detection pipeline",
    "section": "",
    "text": "Object detection Pipeline"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Shataxi Dubey",
    "section": "",
    "text": "Hi! I am Shataxi Dubey an M.Tech Computer Science & Engineering student at Indian Institute of Technology Gandhinagar.\nMy interest area lies in Machine Learning and Computer Vision. Currently exploring the Earth through satellite imagery to unveil the hidden patterns and facts. I am working on my thesis Brick Kiln detection from Satellite Imagery using Vision Language Models under the guidance of Prof.Â Nipun Batra.\nHoping to add sustainable solutions to make world better!"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Shataxi Dubey",
    "section": "Education",
    "text": "Education\nB.Tech Computer Science and Engineering | University of Allahabad | Prayagraj, Uttar Pradesh | July 2018 - May 2022"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Shataxi Dubey",
    "section": "Experience",
    "text": "Experience\nAssociate Engineer | Indiamart Intermesh Ltd | May 2022 - July 2023"
  },
  {
    "objectID": "posts/mean_average_precision.html",
    "href": "posts/mean_average_precision.html",
    "title": "Mean Average Precision",
    "section": "",
    "text": "https://youtu.be/FppOzcDvaDI?si=Dwv7FEAoaS9CLlGH\nIt is a graph between precision and recall for different confidence score with a paricular IOU. The precision recall curve will be different for different IOUs.\nThe area under the Precision-Recall curve is the average precision. Higher the area under curve, higher the average precision.\nAverage Precision(AP) is calculated for each class.\nMean Average Precision is calculated by taking average of the AP of two classes.\nSuppose we have two classes : Dogs and Cats\nNow, we will calculate the Precision-Recall curve for dog class. Suppose there are two images of dog in which\nthe number of ground truth boxes are : 4 the number of predicted boxes are : 2\nFor first image :\nthe number of ground truth boxes are : 1 the number of predicted boxes are : 2\nFor Second image :\nthe number of ground truth boxes are : 3 the number of predicted boxes are : 2\nSo we will start from first image with its first predicted bounding box.\nWe will check:\n\nis the predicted box has IOU greater than threshold â€“&gt; If yes then the predicted box is true positive\nIf No then predicted box is false positive.\n\nSuppose the first predicted box has IOU greater than threshold then\nPrecision till now : 1 divided by total predicted boxes till now which is equal to 1 , so precision is 1\nRecall till now: 1 divided by total number of ground truth boxes which is equal to 4, so recall is 0.25\nNow come to second predicted box of first image.\nSuppose the second predicted box has IOU less than threshold then\nPrecision till now : 1 divided by total predicted boxes till now which is equal to 2 , so precision is 0.5\nRecall till now: 1 divided by total number of ground truth boxes which is equal to 4, so recall is 0.25\nNext, we will go to second image.\nSuppose the first predicted box has IOU greater than threshold then\nPrecision till now : 2 divided by total predicted boxes till now which is equal to 3 , so precision is 0.66 (Why true positives are 2 because we have total correctly predicted boxes till now equal to 2)\nRecall till now: 2 divided by total number of ground truth boxes which is equal to 4, so recall is 0.5\nNow come to second predicted box of second image.\nSuppose the second predicted box has IOU less than threshold then\nPrecision till now : 2 divided by total predicted boxes till now which is equal to 4 , so precision is 0.5\nRecall till now: 2 divided by total number of ground truth boxes which is equal to 4, so recall is 0.5\nNow for plotting the P-R curve, we need all precision values: 1, 0.5, 0.66, 0.5 and all recall values: 0.25, 0.25, 0.5, 0.5\nThis will plot the curve for a particular IOU threshold. Now calculate the area under the curve, which will give the average precision at a particular threshold for Dog class.\nSimilarly, we can do for Cat class and can get the average precision.\nTo get the ean average precision, we take the average of the Average precision of the two classes."
  },
  {
    "objectID": "posts/python-format-specifier.html",
    "href": "posts/python-format-specifier.html",
    "title": "Python Format Specifier",
    "section": "",
    "text": "To format a number to sum precision point using format\n\n'{:.3f}'.format(3.145667)\n\n'3.146'\n\n\n\n'{:.3f}'.format(3.1)\n\n'3.100'"
  },
  {
    "objectID": "posts/precision_recall.html",
    "href": "posts/precision_recall.html",
    "title": "Precision and Recall",
    "section": "",
    "text": "When a model has high recall but low precision, then the model classifies most of the positive samples correctly but it has many false positives (i.e.Â classifies many Negative samples as Positive). When a model has high precision but low recall, then the model is accurate when it classifies a sample as Positive but it may classify only some of the positive samples.\nNote that as the recall increases, the precision decreases. The reason is that when the number of positive samples increases (high recall), the accuracy of classifying each sample correctly decreases (low precision). This is expected, as the model is more likely to fail when there are many samples.\n\nimport numpy as np\n\ny_true = [\"positive\", \"negative\", \"negative\", \"positive\", \"positive\", \"positive\", \"negative\", \"positive\", \"negative\", \"positive\", \"positive\", \"positive\", \"positive\", \"negative\", \"negative\", \"negative\"]\n\npred_scores = [0.7, 0.3, 0.5, 0.6, 0.55, 0.9, 0.4, 0.2, 0.4, 0.3, 0.7, 0.5, 0.8, 0.2, 0.3, 0.35]\n\nthresholds = np.arange(start=0.2, stop=0.7, step=0.05)\n\n\nimport sklearn.metrics\n\ndef precision_recall_curve(y_true, pred_scores, thresholds):\n    precisions = []\n    recalls = []\n    \n    for threshold in thresholds:\n        y_pred = [\"positive\" if score &gt;= threshold else \"negative\" for score in pred_scores]\n\n        precision = sklearn.metrics.precision_score(y_true=y_true, y_pred=y_pred, pos_label=\"positive\")\n        recall = sklearn.metrics.recall_score(y_true=y_true, y_pred=y_pred, pos_label=\"positive\")\n        \n        precisions.append(precision)\n        recalls.append(recall)\n\n    return precisions, recalls\n\n\n# when threshold(0.2) is low, all predicted samples become positive and it will definetly include actual positives \n# so recall will be high but precision will be low because total predicted positives are more than actual positives\ny_pred = [\"positive\" if score &gt;= 0.2 else \"negative\" for score in pred_scores]\nprecision = sklearn.metrics.precision_score(y_true=y_true, y_pred=y_pred, pos_label=\"positive\")\nrecall = sklearn.metrics.recall_score(y_true=y_true, y_pred=y_pred, pos_label=\"positive\")\nprint(f'Precision {precision}, Recall {recall}')\n\nPrecision 0.5625, Recall 1.0\n\n\n\n# when threshold(0.9) is high, positive samples will be actual positive samples so they become true positive which makes precision high\n# that is the model is more than 0.9 sure that the sample is positive, so it will be an actual positive but still you are not covering all \n# positives because of keeping high threshold\ny_pred = [\"positive\" if score &gt;= 0.9 else \"negative\" for score in pred_scores]\nprecision = sklearn.metrics.precision_score(y_true=y_true, y_pred=y_pred, pos_label=\"positive\")\nrecall = sklearn.metrics.recall_score(y_true=y_true, y_pred=y_pred, pos_label=\"positive\")\nprint(f'Precision {precision}, Recall {recall}')\n\nPrecision 1.0, Recall 0.1111111111111111\n\n\n\nprecisions, recalls = precision_recall_curve(y_true, pred_scores, thresholds)\nprint(f'Precision list {precisions}')\nprint(f'Recall list {recalls}')\n\nPrecision list [0.5625, 0.5714285714285714, 0.5714285714285714, 0.6363636363636364, 0.7, 0.875, 0.875, 1.0, 1.0, 1.0]\nRecall list [1.0, 0.8888888888888888, 0.8888888888888888, 0.7777777777777778, 0.7777777777777778, 0.7777777777777778, 0.7777777777777778, 0.6666666666666666, 0.5555555555555556, 0.4444444444444444]\n\n\n\nimport matplotlib.pyplot as plt\nplt.plot(recalls, precisions, color=\"red\")\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.title(\"Precision-Recall Curve obtained by varying score threshold \\n score threshold is increasing from right to left\")\nplt.show()\n\n\n\n\n\n\n\n\nSimilarly, if we vary IOU threshold then we will get another precision recall curve\n\ndef compute_ap(recall, precision):\n    #from ultralytics\n    # Append sentinel values to beginning and end\n    mrec = np.concatenate(([0.0], recall, [1.0]))\n    mpre = np.concatenate(([1.0], precision, [0.0]))\n    print(f'mpre {mpre}')\n    print(f'np.flip {np.flip(mpre)}')\n    print(f'np.accumulate.maximum {np.maximum.accumulate(np.flip(mpre))}')\n    # Compute the precision envelope\n    mpre = np.flip(np.maximum.accumulate(np.flip(mpre)))\n\n    # Integrate area under curve\n    method = \"interp\"  # methods: 'continuous', 'interp'\n    if method == \"interp\":\n        x = np.linspace(0, 1, 101)  # 101-point interp (COCO)\n        ap = np.trapz(np.interp(x, mrec, mpre), x)  # integrate\n    else:  # 'continuous'\n        i = np.where(mrec[1:] != mrec[:-1])[0]  # points where x-axis (recall) changes\n        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])  # area under curve\n\n    return ap, mpre, mrec\n\n\ndef smooth(y, f=0.05):\n    \"\"\"Box filter of fraction f.\"\"\"\n    nf = round(len(y) * f * 2) // 2 + 1  # number of filter elements (must be odd)\n    p = np.ones(nf // 2)  # ones padding\n    yp = np.concatenate((p * y[0], y, p * y[-1]), 0)  # y padded\n    return np.convolve(yp, np.ones(nf) / nf, mode=\"valid\")  # y-smoothed\n\n\ndef ap_per_class(\n    tp, conf, pred_cls, target_cls, plot=False,eps=1e-16):\n    \n    # Sort by objectness\n    i = np.argsort(-conf)\n    tp, conf, pred_cls = tp[i], conf[i], pred_cls[i] # decreasing order of  confidences\n    tp = tp.reshape((tp.shape[0],1))\n    # Find unique classes\n    unique_classes, nt = np.unique(target_cls, return_counts=True)\n    nc = unique_classes.shape[0]  # number of classes, number of detections\n\n    # Create Precision-Recall curve and compute AP for each class\n    x, prec_values = np.linspace(0, 1, 1000), []\n\n    # Average precision, precision and recall curves\n    ap, p_curve, r_curve = np.zeros((nc, tp.shape[1])), np.zeros((nc, 1000)), np.zeros((nc, 1000))\n    for ci, c in enumerate(unique_classes):\n        i = pred_cls == c\n        n_l = nt[ci]  # number of labels or ground truth\n        n_p = i.sum()  # number of predictions\n        if n_p == 0 or n_l == 0:\n            continue\n\n        # Accumulate FPs and TPs\n        fpc = (1 - tp[i]).cumsum(0) \n        tpc = tp[i].cumsum(0)\n        \n        # Recall\n        recall = tpc / (n_l + eps)  # recall curve\n        print(f'recall {recall[:,0]}')\n        # print(f'-conf[i] {-conf[i]}')\n        r_curve[ci] = np.interp(-x, -conf[i], recall[:, 0], left=0)  # negative x, xp because xp decreases\n        # print(f'r_curve[ci] {r_curve[ci]}')\n        # Precision\n        precision = tpc / (tpc + fpc)  # precision curve\n        print(f'precision {precision[:,0]}')\n        # print(f'-conf[i] {-conf[i]}')\n        p_curve[ci] = np.interp(-x, -conf[i], precision[:, 0], left=1)  # p at pr_score\n        # print(f'p_curve[ci] {p_curve[ci]}')\n        # AP from recall-precision curve\n        for j in range(tp.shape[1]):\n            ap[ci, j], mpre, mrec = compute_ap(recall[:, j], precision[:, j])\n            print(f'AP {ap[ci,j]}')\n            if plot and j == 0:\n                prec_values.append(np.interp(x, mrec, mpre))  \n\n    prec_values = np.array(prec_values)  # (nc, 1000)\n\n    # Compute F1 (harmonic mean of precision and recall)\n    f1_curve = 2 * p_curve * r_curve / (p_curve + r_curve + eps)\n\n    i = smooth(f1_curve.mean(0), 0.1).argmax()  # max F1 index\n    p, r, f1 = p_curve[:, i], r_curve[:, i], f1_curve[:, i]  # max-F1 precision, recall, F1 values\n    tp = (r * nt).round()  # true positives\n    fp = (tp / (p + eps) - tp).round()  # false positives\n    # return tp, fp, p, r, ap, f1, unique_classes.astype(int), p_curve, r_curve, f1_curve, x, prec_values\n    return 1\n\nTrue positives are decided using IOU\nclass 1 p = 1, r = 0.33 p = 0.5, r = 0.33 p = 0.66, r = 0.66\nclass 2 p = 1, r = 0.5 p = 0.5, r = 0.5\n\ntp = np.array([1,0,1,1,0])\nconf = np.array([1,1,1,1,1])\npred_cls = np.array(['1','1','1','2','2'])\ntarget_cls = np.array(['1','1','1','2','2'])\nap_per_class(tp, conf, pred_cls, target_cls, plot=False,eps=1e-16)\n\nrecall [0.33333333 0.33333333 0.66666667]\nprecision [1.         0.5        0.66666667]\nmpre [1.         1.         0.5        0.66666667 0.        ]\nnp.flip [0.         0.66666667 0.5        1.         1.        ]\nnp.accumulate.maximum [0.         0.66666667 0.66666667 1.         1.        ]\nAP 0.6672\nrecall [0.5 0.5]\nprecision [1.  0.5]\nmpre [1.  1.  0.5 0. ]\nnp.flip [0.  0.5 1.  1. ]\nnp.accumulate.maximum [0.  0.5 1.  1. ]\nAP 0.6224999999999999\n\n\n1"
  },
  {
    "objectID": "posts/vlm_sizes.html",
    "href": "posts/vlm_sizes.html",
    "title": "Vision Language Models",
    "section": "",
    "text": "import os\nos.environ['CUDA_VISIBLE_DEVICES'] = '3'\n\nfrom transformers import BitsAndBytesConfig\nimport torch\nfrom transformers import Qwen2_5_VLForConditionalGeneration\nbnb_config = BitsAndBytesConfig(\n                load_in_4bit=True,\n                # bnb_4bit_use_double_quant=True,\n                bnb_4bit_quant_type=\"nf4\",\n                bnb_4bit_compute_type=torch.bfloat16,)\n\nmodel = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n    \"Qwen/Qwen2.5-VL-72B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\", quantization_config=bnb_config\n)\n\nmodel.get_memory_footprint()\n\n\n\n\n40448783184\n\n\n\nmodel\n\nQwen2_5_VLForConditionalGeneration(\n  (visual): Qwen2_5_VisionTransformerPretrainedModel(\n    (patch_embed): Qwen2_5_VisionPatchEmbed(\n      (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n    )\n    (rotary_pos_emb): Qwen2_5_VisionRotaryEmbedding()\n    (blocks): ModuleList(\n      (0-31): 32 x Qwen2_5_VLVisionBlock(\n        (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n        (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n        (attn): Qwen2_5_VLVisionSdpaAttention(\n          (qkv): Linear4bit(in_features=1280, out_features=3840, bias=True)\n          (proj): Linear4bit(in_features=1280, out_features=1280, bias=True)\n        )\n        (mlp): Qwen2_5_VLMLP(\n          (gate_proj): Linear4bit(in_features=1280, out_features=3456, bias=True)\n          (up_proj): Linear4bit(in_features=1280, out_features=3456, bias=True)\n          (down_proj): Linear4bit(in_features=3456, out_features=1280, bias=True)\n          (act_fn): SiLU()\n        )\n      )\n    )\n    (merger): Qwen2_5_VLPatchMerger(\n      (ln_q): Qwen2RMSNorm((1280,), eps=1e-06)\n      (mlp): Sequential(\n        (0): Linear4bit(in_features=5120, out_features=5120, bias=True)\n        (1): GELU(approximate='none')\n        (2): Linear4bit(in_features=5120, out_features=8192, bias=True)\n      )\n    )\n  )\n  (model): Qwen2_5_VLModel(\n    (embed_tokens): Embedding(152064, 8192)\n    (layers): ModuleList(\n      (0-79): 80 x Qwen2_5_VLDecoderLayer(\n        (self_attn): Qwen2_5_VLSdpaAttention(\n          (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=True)\n          (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=True)\n          (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=True)\n          (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n          (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n        )\n        (mlp): Qwen2MLP(\n          (gate_proj): Linear4bit(in_features=8192, out_features=29568, bias=False)\n          (up_proj): Linear4bit(in_features=8192, out_features=29568, bias=False)\n          (down_proj): Linear4bit(in_features=29568, out_features=8192, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): Qwen2RMSNorm((8192,), eps=1e-06)\n        (post_attention_layernorm): Qwen2RMSNorm((8192,), eps=1e-06)\n      )\n    )\n    (norm): Qwen2RMSNorm((8192,), eps=1e-06)\n    (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=8192, out_features=152064, bias=False)\n)\n\n\n\nimport os\nos.environ['CUDA_VISIBLE_DEVICES'] = '3'\n\nfrom transformers import BitsAndBytesConfig\nimport torch\nfrom transformers import PaliGemmaForConditionalGeneration\n\nbnb_config = BitsAndBytesConfig(\n                load_in_4bit=True,\n                # bnb_4bit_use_double_quant=True,\n                bnb_4bit_quant_type=\"nf4\",\n                bnb_4bit_compute_type=torch.bfloat16,)\n\nmodel = PaliGemmaForConditionalGeneration.from_pretrained(\"google/paligemma2-3b-pt-448\", torch_dtype=torch.bfloat16, device_map=\"auto\")\nmodel.get_memory_footprint()\n\n\n\n\n6066263008\n\n\n\nmodel\n\nPaliGemmaForConditionalGeneration(\n  (vision_tower): SiglipVisionModel(\n    (vision_model): SiglipVisionTransformer(\n      (embeddings): SiglipVisionEmbeddings(\n        (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n        (position_embedding): Embedding(1024, 1152)\n      )\n      (encoder): SiglipEncoder(\n        (layers): ModuleList(\n          (0-26): 27 x SiglipEncoderLayer(\n            (self_attn): SiglipSdpaAttention(\n              (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n              (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n              (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n              (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n            )\n            (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n            (mlp): SiglipMLP(\n              (activation_fn): PytorchGELUTanh()\n              (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n              (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n            )\n            (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n          )\n        )\n      )\n      (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n    )\n  )\n  (multi_modal_projector): PaliGemmaMultiModalProjector(\n    (linear): Linear(in_features=1152, out_features=2304, bias=True)\n  )\n  (language_model): Gemma2ForCausalLM(\n    (model): Gemma2Model(\n      (embed_tokens): Embedding(257216, 2304, padding_idx=0)\n      (layers): ModuleList(\n        (0-25): 26 x Gemma2DecoderLayer(\n          (self_attn): Gemma2Attention(\n            (q_proj): Linear(in_features=2304, out_features=2048, bias=False)\n            (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n            (v_proj): Linear(in_features=2304, out_features=1024, bias=False)\n            (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n          )\n          (mlp): Gemma2MLP(\n            (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n            (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n            (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n            (act_fn): PytorchGELUTanh()\n          )\n          (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n          (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n          (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n          (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n        )\n      )\n      (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n      (rotary_emb): Gemma2RotaryEmbedding()\n    )\n    (lm_head): Linear(in_features=2304, out_features=257216, bias=False)\n  )\n)\n\n\n\nimport os\nos.environ['CUDA_VISIBLE_DEVICES'] = '3'\n\nimport torch\nfrom transformers import AutoModelForCausalLM \n\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/Florence-2-large-ft\", torch_dtype=\"auto\", trust_remote_code=True)\nmodel.get_memory_footprint()\n\nFlorence2LanguageForConditionalGeneration has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n\n\n1645899954\n\n\n\nmodel.dtype\n\ntorch.float16\n\n\n\nmodel\n\nFlorence2ForConditionalGeneration(\n  (vision_tower): DaViT(\n    (convs): ModuleList(\n      (0): ConvEmbed(\n        (proj): Conv2d(3, 256, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n      )\n      (1): ConvEmbed(\n        (proj): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n      )\n      (2): ConvEmbed(\n        (proj): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (3): ConvEmbed(\n        (proj): Conv2d(1024, 2048, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n    (blocks): ModuleList(\n      (0): MySequential(\n        (0): MySequential(\n          (spatial_block): SpatialBlock(\n            (conv1): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n              )\n            )\n            (window_attn): PreNorm(\n              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n              (fn): WindowAttention(\n                (qkv): Linear(in_features=256, out_features=768, bias=True)\n                (proj): Linear(in_features=256, out_features=256, bias=True)\n                (softmax): Softmax(dim=-1)\n              )\n              (drop_path): Identity()\n            )\n            (conv2): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n              )\n            )\n            (ffn): PreNorm(\n              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n              (fn): Mlp(\n                (net): Sequential(\n                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n                  (act): GELU(approximate='none')\n                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n                )\n              )\n              (drop_path): Identity()\n            )\n          )\n          (channel_block): ChannelBlock(\n            (conv1): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n              )\n            )\n            (channel_attn): PreNorm(\n              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n              (fn): ChannelAttention(\n                (qkv): Linear(in_features=256, out_features=768, bias=True)\n                (proj): Linear(in_features=256, out_features=256, bias=True)\n              )\n              (drop_path): DropPath(drop_prob=0.004)\n            )\n            (conv2): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n              )\n            )\n            (ffn): PreNorm(\n              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n              (fn): Mlp(\n                (net): Sequential(\n                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n                  (act): GELU(approximate='none')\n                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n                )\n              )\n              (drop_path): DropPath(drop_prob=0.004)\n            )\n          )\n        )\n      )\n      (1): MySequential(\n        (0): MySequential(\n          (spatial_block): SpatialBlock(\n            (conv1): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n              )\n            )\n            (window_attn): PreNorm(\n              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (fn): WindowAttention(\n                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n                (proj): Linear(in_features=512, out_features=512, bias=True)\n                (softmax): Softmax(dim=-1)\n              )\n              (drop_path): DropPath(drop_prob=0.009)\n            )\n            (conv2): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n              )\n            )\n            (ffn): PreNorm(\n              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (fn): Mlp(\n                (net): Sequential(\n                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n                  (act): GELU(approximate='none')\n                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n                )\n              )\n              (drop_path): DropPath(drop_prob=0.009)\n            )\n          )\n          (channel_block): ChannelBlock(\n            (conv1): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n              )\n            )\n            (channel_attn): PreNorm(\n              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (fn): ChannelAttention(\n                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n                (proj): Linear(in_features=512, out_features=512, bias=True)\n              )\n              (drop_path): DropPath(drop_prob=0.013)\n            )\n            (conv2): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n              )\n            )\n            (ffn): PreNorm(\n              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (fn): Mlp(\n                (net): Sequential(\n                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n                  (act): GELU(approximate='none')\n                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n                )\n              )\n              (drop_path): DropPath(drop_prob=0.013)\n            )\n          )\n        )\n      )\n      (2): MySequential(\n        (0): MySequential(\n          (spatial_block): SpatialBlock(\n            (conv1): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (window_attn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): WindowAttention(\n                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n                (softmax): Softmax(dim=-1)\n              )\n              (drop_path): DropPath(drop_prob=0.017)\n            )\n            (conv2): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (ffn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): Mlp(\n                (net): Sequential(\n                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                  (act): GELU(approximate='none')\n                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n                )\n              )\n              (drop_path): DropPath(drop_prob=0.017)\n            )\n          )\n          (channel_block): ChannelBlock(\n            (conv1): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (channel_attn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): ChannelAttention(\n                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n              )\n              (drop_path): DropPath(drop_prob=0.022)\n            )\n            (conv2): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (ffn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): Mlp(\n                (net): Sequential(\n                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                  (act): GELU(approximate='none')\n                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n                )\n              )\n              (drop_path): DropPath(drop_prob=0.022)\n            )\n          )\n        )\n        (1): MySequential(\n          (spatial_block): SpatialBlock(\n            (conv1): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (window_attn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): WindowAttention(\n                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n                (softmax): Softmax(dim=-1)\n              )\n              (drop_path): DropPath(drop_prob=0.026)\n            )\n            (conv2): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (ffn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): Mlp(\n                (net): Sequential(\n                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                  (act): GELU(approximate='none')\n                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n                )\n              )\n              (drop_path): DropPath(drop_prob=0.026)\n            )\n          )\n          (channel_block): ChannelBlock(\n            (conv1): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (channel_attn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): ChannelAttention(\n                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n              )\n              (drop_path): DropPath(drop_prob=0.030)\n            )\n            (conv2): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (ffn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): Mlp(\n                (net): Sequential(\n                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                  (act): GELU(approximate='none')\n                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n                )\n              )\n              (drop_path): DropPath(drop_prob=0.030)\n            )\n          )\n        )\n        (2): MySequential(\n          (spatial_block): SpatialBlock(\n            (conv1): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (window_attn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): WindowAttention(\n                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n                (softmax): Softmax(dim=-1)\n              )\n              (drop_path): DropPath(drop_prob=0.035)\n            )\n            (conv2): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (ffn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): Mlp(\n                (net): Sequential(\n                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                  (act): GELU(approximate='none')\n                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n                )\n              )\n              (drop_path): DropPath(drop_prob=0.035)\n            )\n          )\n          (channel_block): ChannelBlock(\n            (conv1): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (channel_attn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): ChannelAttention(\n                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n              )\n              (drop_path): DropPath(drop_prob=0.039)\n            )\n            (conv2): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (ffn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): Mlp(\n                (net): Sequential(\n                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                  (act): GELU(approximate='none')\n                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n                )\n              )\n              (drop_path): DropPath(drop_prob=0.039)\n            )\n          )\n        )\n        (3): MySequential(\n          (spatial_block): SpatialBlock(\n            (conv1): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (window_attn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): WindowAttention(\n                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n                (softmax): Softmax(dim=-1)\n              )\n              (drop_path): DropPath(drop_prob=0.043)\n            )\n            (conv2): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (ffn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): Mlp(\n                (net): Sequential(\n                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                  (act): GELU(approximate='none')\n                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n                )\n              )\n              (drop_path): DropPath(drop_prob=0.043)\n            )\n          )\n          (channel_block): ChannelBlock(\n            (conv1): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (channel_attn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): ChannelAttention(\n                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n              )\n              (drop_path): DropPath(drop_prob=0.048)\n            )\n            (conv2): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (ffn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): Mlp(\n                (net): Sequential(\n                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                  (act): GELU(approximate='none')\n                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n                )\n              )\n              (drop_path): DropPath(drop_prob=0.048)\n            )\n          )\n        )\n        (4): MySequential(\n          (spatial_block): SpatialBlock(\n            (conv1): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (window_attn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): WindowAttention(\n                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n                (softmax): Softmax(dim=-1)\n              )\n              (drop_path): DropPath(drop_prob=0.052)\n            )\n            (conv2): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (ffn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): Mlp(\n                (net): Sequential(\n                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                  (act): GELU(approximate='none')\n                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n                )\n              )\n              (drop_path): DropPath(drop_prob=0.052)\n            )\n          )\n          (channel_block): ChannelBlock(\n            (conv1): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (channel_attn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): ChannelAttention(\n                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n              )\n              (drop_path): DropPath(drop_prob=0.057)\n            )\n            (conv2): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (ffn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): Mlp(\n                (net): Sequential(\n                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                  (act): GELU(approximate='none')\n                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n                )\n              )\n              (drop_path): DropPath(drop_prob=0.057)\n            )\n          )\n        )\n        (5): MySequential(\n          (spatial_block): SpatialBlock(\n            (conv1): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (window_attn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): WindowAttention(\n                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n                (softmax): Softmax(dim=-1)\n              )\n              (drop_path): DropPath(drop_prob=0.061)\n            )\n            (conv2): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (ffn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): Mlp(\n                (net): Sequential(\n                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                  (act): GELU(approximate='none')\n                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n                )\n              )\n              (drop_path): DropPath(drop_prob=0.061)\n            )\n          )\n          (channel_block): ChannelBlock(\n            (conv1): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (channel_attn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): ChannelAttention(\n                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n              )\n              (drop_path): DropPath(drop_prob=0.065)\n            )\n            (conv2): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (ffn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): Mlp(\n                (net): Sequential(\n                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                  (act): GELU(approximate='none')\n                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n                )\n              )\n              (drop_path): DropPath(drop_prob=0.065)\n            )\n          )\n        )\n        (6): MySequential(\n          (spatial_block): SpatialBlock(\n            (conv1): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (window_attn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): WindowAttention(\n                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n                (softmax): Softmax(dim=-1)\n              )\n              (drop_path): DropPath(drop_prob=0.070)\n            )\n            (conv2): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (ffn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): Mlp(\n                (net): Sequential(\n                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                  (act): GELU(approximate='none')\n                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n                )\n              )\n              (drop_path): DropPath(drop_prob=0.070)\n            )\n          )\n          (channel_block): ChannelBlock(\n            (conv1): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (channel_attn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): ChannelAttention(\n                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n              )\n              (drop_path): DropPath(drop_prob=0.074)\n            )\n            (conv2): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (ffn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): Mlp(\n                (net): Sequential(\n                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                  (act): GELU(approximate='none')\n                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n                )\n              )\n              (drop_path): DropPath(drop_prob=0.074)\n            )\n          )\n        )\n        (7): MySequential(\n          (spatial_block): SpatialBlock(\n            (conv1): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (window_attn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): WindowAttention(\n                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n                (softmax): Softmax(dim=-1)\n              )\n              (drop_path): DropPath(drop_prob=0.078)\n            )\n            (conv2): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (ffn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): Mlp(\n                (net): Sequential(\n                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                  (act): GELU(approximate='none')\n                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n                )\n              )\n              (drop_path): DropPath(drop_prob=0.078)\n            )\n          )\n          (channel_block): ChannelBlock(\n            (conv1): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (channel_attn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): ChannelAttention(\n                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n              )\n              (drop_path): DropPath(drop_prob=0.083)\n            )\n            (conv2): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (ffn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): Mlp(\n                (net): Sequential(\n                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                  (act): GELU(approximate='none')\n                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n                )\n              )\n              (drop_path): DropPath(drop_prob=0.083)\n            )\n          )\n        )\n        (8): MySequential(\n          (spatial_block): SpatialBlock(\n            (conv1): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (window_attn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): WindowAttention(\n                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n                (softmax): Softmax(dim=-1)\n              )\n              (drop_path): DropPath(drop_prob=0.087)\n            )\n            (conv2): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (ffn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): Mlp(\n                (net): Sequential(\n                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                  (act): GELU(approximate='none')\n                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n                )\n              )\n              (drop_path): DropPath(drop_prob=0.087)\n            )\n          )\n          (channel_block): ChannelBlock(\n            (conv1): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (channel_attn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): ChannelAttention(\n                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n              )\n              (drop_path): DropPath(drop_prob=0.091)\n            )\n            (conv2): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (ffn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): Mlp(\n                (net): Sequential(\n                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                  (act): GELU(approximate='none')\n                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n                )\n              )\n              (drop_path): DropPath(drop_prob=0.091)\n            )\n          )\n        )\n      )\n      (3): MySequential(\n        (0): MySequential(\n          (spatial_block): SpatialBlock(\n            (conv1): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n              )\n            )\n            (window_attn): PreNorm(\n              (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n              (fn): WindowAttention(\n                (qkv): Linear(in_features=2048, out_features=6144, bias=True)\n                (proj): Linear(in_features=2048, out_features=2048, bias=True)\n                (softmax): Softmax(dim=-1)\n              )\n              (drop_path): DropPath(drop_prob=0.096)\n            )\n            (conv2): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n              )\n            )\n            (ffn): PreNorm(\n              (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n              (fn): Mlp(\n                (net): Sequential(\n                  (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n                  (act): GELU(approximate='none')\n                  (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n                )\n              )\n              (drop_path): DropPath(drop_prob=0.096)\n            )\n          )\n          (channel_block): ChannelBlock(\n            (conv1): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n              )\n            )\n            (channel_attn): PreNorm(\n              (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n              (fn): ChannelAttention(\n                (qkv): Linear(in_features=2048, out_features=6144, bias=True)\n                (proj): Linear(in_features=2048, out_features=2048, bias=True)\n              )\n              (drop_path): DropPath(drop_prob=0.100)\n            )\n            (conv2): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n              )\n            )\n            (ffn): PreNorm(\n              (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n              (fn): Mlp(\n                (net): Sequential(\n                  (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n                  (act): GELU(approximate='none')\n                  (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n                )\n              )\n              (drop_path): DropPath(drop_prob=0.100)\n            )\n          )\n        )\n      )\n    )\n    (avgpool): AdaptiveAvgPool1d(output_size=1)\n  )\n  (image_proj_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n  (image_pos_embed): LearnedAbsolutePositionEmbedding2D(\n    (row_embeddings): Embedding(50, 1024)\n    (column_embeddings): Embedding(50, 1024)\n  )\n  (visual_temporal_embed): PositionalEmbeddingCosine1D()\n  (language_model): Florence2LanguageForConditionalGeneration(\n    (model): Florence2LanguageModel(\n      (shared): Embedding(51289, 1024, padding_idx=1)\n      (encoder): Florence2Encoder(\n        (embed_tokens): Florence2ScaledWordEmbedding(51289, 1024, padding_idx=1)\n        (embed_positions): Florence2LearnedPositionalEmbedding(1026, 1024)\n        (layers): ModuleList(\n          (0-11): 12 x Florence2EncoderLayer(\n            (self_attn): Florence2SdpaAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (activation_fn): GELUActivation()\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (decoder): Florence2Decoder(\n        (embed_tokens): Florence2ScaledWordEmbedding(51289, 1024, padding_idx=1)\n        (embed_positions): Florence2LearnedPositionalEmbedding(1026, 1024)\n        (layers): ModuleList(\n          (0-11): 12 x Florence2DecoderLayer(\n            (self_attn): Florence2SdpaAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (activation_fn): GELUActivation()\n            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (encoder_attn): Florence2SdpaAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n    (lm_head): Linear(in_features=1024, out_features=51289, bias=False)\n  )\n)"
  }
]