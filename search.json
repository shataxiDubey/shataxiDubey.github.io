[
  {
    "objectID": "blogs.html",
    "href": "blogs.html",
    "title": "Blogs",
    "section": "",
    "text": "Xavier initialisation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTransformers\n\n\n\nSelf attention\n\nmasked attention\n\nmulti head attention\n\n\n\nBuilding transformers from scratch\n\n\n\n\n\nJul 1, 2025\n\n\nShataxi Dubey\n\n\n\n\n\n\n\n\n\n\n\n\nReLU Activation Functions\n\n\n\nReLU\n\nLinearity\n\n\n\nNon Linearity using ReLU activation function\n\n\n\n\n\nJun 26, 2025\n\n\nShataxi Dubey\n\n\n\n\n\n\n\n\n\n\n\n\nVision Language Models\n\n\n\nvlm\n\nlora\n\nquantization\n\n\n\nVision Language Models and Lora adapters with quantization\n\n\n\n\n\nMay 23, 2025\n\n\nShataxi Dubey\n\n\n\n\n\n\n\n\n\n\n\n\nObject detection pipeline\n\n\n\nobject detection\n\n\n\nObject detection pipeline\n\n\n\n\n\nAug 1, 2024\n\n\nShataxi Dubey\n\n\n\n\n\n\n\n\n\n\n\n\nNon max suppression\n\n\n\nobject detection\n\nnms\n\n\n\nNon max suppression suppresses bounding box with non max confidence scores\n\n\n\n\n\nAug 1, 2024\n\n\nShataxi Dubey\n\n\n\n\n\n\n\n\n\n\n\n\nPrecision and Recall\n\n\n\nmetrics\n\n\n\nSignificance of precision and recall\n\n\n\n\n\nAug 1, 2024\n\n\nShataxi Dubey\n\n\n\n\n\n\n\n\n\n\n\n\nMean Average Precision\n\n\n\nobject detection\n\nMAP\n\n\n\nAn important metric in Object detection tasks\n\n\n\n\n\nMay 23, 2024\n\n\nShataxi Dubey\n\n\n\n\n\n\n\n\n\n\n\n\nEvolution in object detection\n\n\n\nobject detection\n\nSOTA\n\n\n\nstate of the art models\n\n\n\n\n\nMay 23, 2024\n\n\nShataxi Dubey\n\n\n\n\n\n\n\n\n\n\n\n\nPython Format Specifier\n\n\n\nprecision\n\nformat\n\n\n\nFormatting numbers\n\n\n\n\n\nMay 9, 2024\n\n\nShataxi Dubey\n\n\n\n\n\n\n\n\n\n\n\n\nImage Transformation\n\n\n\nimages\n\nopencv\n\n\n\nVarious ways to transform image\n\n\n\n\n\nDec 24, 2023\n\n\nShataxi Dubey\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/LSTM.html",
    "href": "posts/LSTM.html",
    "title": "Shataxi Dubey",
    "section": "",
    "text": "Through this notebook, we can easily understand how input gate, output gate, forget gate work in LSTM and how forward propagation and backward propagation happen through various gates in a recurrent neural network.\n\nimport torch\nimport numpy as np\nfrom tqdm import tqdm\n\n\ncell_state = torch.arange(start = 0, end = 5).reshape((1,5))\nhidden_state = torch.arange(start = 10, end = 15).reshape((1,5))\ninput_gate = torch.arange(start = 0, end = 10).reshape((5,2))\nforget_gate = torch.arange(start = 10, end = 20).reshape((5,2))\n\n\nconcatenated_input = torch.concatenate([hidden_state, cell_state], axis = 1)\nconcatenated_weights = torch.concatenate([input_gate, forget_gate], axis = 0)\n\n\nconcatenated_input@concatenated_weights\n\ntensor([[420, 490]])\n\n\n\ncell_state@forget_gate + hidden_state@input_gate\n\ntensor([[420, 490]])\n\n\n\ndef oneHotEncode(data, num_chars, chr_to_idx):\n    all_one_hots = []\n    for chr in data:\n        onehot = np.zeros((num_chars,1))\n        idx = chr_to_idx[chr]\n        onehot[idx] = 1\n        all_one_hots.append(onehot)\n    \n    return all_one_hots\n\n\nXavier initialisation\nUniform xavier initialisation\ndraw weight from a random uniform distribution [-x, x]\nx = sqrt(6/(input_size + output_size))\n\ndef initWeights(input_size, output_size):  # input size = hidden size, output size = hidden size + num_of_unique_chars \n    x = np.sqrt(6 / (input_size + output_size))\n    w = np.random.uniform(low = -x, high = x, size = (input_size, output_size)) \n    return w\n\n\ndef sigmoid(input):\n    input = np.clip(input, 1e-5, 1e5)\n    sig_out = 1 / (1 + np.exp(-input)) \n    return sig_out\n\ndef tanh(input):\n    # num = np.exp(input) - np.exp(-input)\n    # denom = np.exp(input) + np.exp(-input)\n\n    # return num/denom\n    return np.tanh(input)\n\ndef derivative_sigmoid(input):\n    return input*(1 - input)\n\ndef derivative_tanh(input):\n    return 1 - (input**2)\n\ndef softmax(input):\n    return np.exp(input)/ sum(np.exp(input))\n\n\n\nclass LSTM():\n    def __init__(self, input_size, hidden_size, output_size, learning_rate, num_epochs):\n\n        self.learning_rate = learning_rate\n        self.num_epochs = num_epochs\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n        # forget gate weights(it is multiplied with the cell state t-1)\n        self.wf = initWeights(hidden_size, input_size)\n        self.bf = initWeights(hidden_size, 1)\n\n        # input gate weights\n        self.wi = initWeights(hidden_size, input_size)\n        self.bi = initWeights(hidden_size, 1)\n\n        # output gate weights\n        self.wo = initWeights(hidden_size, input_size)\n        self.bo = initWeights(hidden_size, 1)\n\n        # candidate gate weights\n        self.wc = initWeights(hidden_size, input_size)\n        self.bc = initWeights(hidden_size, 1)\n\n        # final layer weights\n        self.wy = initWeights(output_size, hidden_size)\n        self.by = initWeights(output_size, 1)\n\n    \n    def reset(self):\n        self.hidden_state = {-1: np.zeros((self.hidden_size, 1))}\n        self.cell_state = {-1: np.zeros((self.hidden_size, 1))}\n        self.concatenated_input = {}\n        self.forget_gate = {}\n        self.input_gate = {}\n        self.output_gate = {}\n        self.intermediate_cell_state = {}\n\n    def forward(self, inputs): \n        self.reset()\n\n        output = []\n        for idx in range(len(inputs)):\n            self.concatenated_input[idx] = np.concatenate([self.hidden_state[idx - 1], inputs[idx]]) # shape of concatenated input: (hidden_size + char_size, 1)\n            self.forget_gate[idx] = sigmoid(self.wf@self.concatenated_input[idx] + self.bf) # shape of forget gate: (hidden_size, 1)\n            self.input_gate[idx] = sigmoid(self.wi@self.concatenated_input[idx] + self.bi) # shape of input gate: (hidden_size, 1)\n            self.output_gate[idx] = sigmoid(self.wo@self.concatenated_input[idx] + self.bo) # shape of output gate: (hidden_size, 1)\n            self.intermediate_cell_state[idx] = tanh(self.wc@self.concatenated_input[idx] + self.bc) # shape of intermediate cell state: (hidden_size, 1)\n            \n            self.cell_state[idx] = (np.multiply(self.intermediate_cell_state[idx], self.input_gate[idx]) + np.multiply(self.cell_state[idx - 1], self.forget_gate[idx])) # shape of cell state: (hidden size, 1)\n            self.hidden_state[idx] = np.multiply(tanh(self.cell_state[idx]), self.output_gate[idx]) # shape of hidden state: (hidden_size, 1)\n            \n            output += [self.wy @ self.hidden_state[idx] + self.by]  # The final output is computed from the hidden state\n            # shape of output: (char_size, 1)\n\n        return output\n    \n    def train(self, inputs, labels, chr_to_idx):\n\n        for _ in tqdm(range(self.num_epochs)):\n            outputs = self.forward(inputs)\n\n            errors = []\n            for idx in range(len(outputs)):\n                errors += [softmax(outputs[idx])] \n                errors[-1][chr_to_idx[labels[idx]]] -= 1 # Here we compute (y_hat - 1) which is the gradient of loss with respect to z where y_hat = softmax(z)\n\n            \n            self.backward(errors, outputs)\n    \n\n    def backward(self, errors, outputs):\n\n        d_wy, d_by = 0, 0\n        d_wo, d_bo = 0, 0\n        d_wc, d_bc = 0, 0\n        d_wi, d_bi = 0, 0\n        d_wf, d_bf = 0, 0\n\n        dh_next, dc_next = np.zeros((self.hidden_size, 1)), np.zeros((self.hidden_size, 1)) \n\n        for idx in range(len(outputs) - 1, 0, -1):\n            # gradient wrt final gate\n            d_wy += errors[idx] @ self.hidden_state[idx].T     # shape of error: (char_size, 1), shape of hidden_state: (hidden_size, 1), shape of wy: (char_size, hidden_size)\n            d_by += errors[idx]  # shape of d_by: (char_size, 1)\n\n            # gradient wrt hidden state\n            d_h = self.wy.T @ errors[idx] + dh_next  # shape of error: (char_size, 1), shape of wy (char_size, hidden_size) shape of d_h: (hidden_size, 1)\n\n\n            # gradient wrt output gate\n            d_o = tanh(self.cell_state[idx]) * derivative_sigmoid(self.output_gate[idx]) * d_h # shape of cell state: (hidden_size, 1), shape of output gate: (hidden_size, 1) shape of d_o: (hidden_size, 1)\n            d_wo += d_o @ self.concatenated_input[idx].T # shape of concatenated_input: (char_size + hidden_size, 1), shape of d_o: (hidden_size, 1), shape of w_o: (hidden_size, char_size + hidden_size), shape of d_wo: (hidden_size, char_size + hidden_size)\n            d_bo += d_o # shape of d_bo: (hidden_size, 1)\n\n            # gradient wrt cell state\n            d_cs = self.output_gate[idx] * derivative_tanh(tanh(self.cell_state[idx])) * d_h + dc_next # shape of output gate: (hidden_size, 1), shape of cell_state: (hidden_size, 1), shape of d_h: (hidden_size, 1), shape of d_cs: (hidden_size, 1)\n\n            # gradient wrt candidate gate\n            d_candidate = self.input_gate[idx] * derivative_tanh(self.intermediate_cell_state[idx]) * d_cs # shape of input gate: (hidden_size, 1), shape of intermediate_cell_state: (hidden_size, 1), shape of d_cs: (hidden_size, 1), shape of d_candidate: (hidden_size, 1)\n            d_wc += d_candidate @ self.concatenated_input[idx].T # shape of concatenated_input: (char_size + hidden_size, 1), shape of d_candidate: (hidden_size, 1), shape of d_wc: (hidden_size, hidden_size + char_size)\n            d_bc += d_candidate # shape of d_candidate: (hidden_size, 1), shape of d_bc: (hidden_size, 1)\n\n            # gradient wrt input gate\n            d_i = self.intermediate_cell_state[idx] * derivative_sigmoid(self.input_gate[idx]) * d_cs # shape of intermediate_cell_state: (hidden_size, 1), shape of input_gate: (hidden_size, 1), shape of d_cs: (hidden_size, 1), shape of d_i: (hidden_size, 1)\n            d_wi += d_i @ self.concatenated_input[idx].T # shape of concatenated_input: (hidden_size+ chr_size, 1), shape of d_i: (hidden_size, 1), shape of d_wi; (hidden_size, hidden_size+ chr_size)\n            d_bi += d_i # shape of d_i: (hidden_size, 1), shape of d_bi: (hidden_size, 1)\n\n            # gradient wrt forget gate\n            d_f = self.cell_state[idx-1] * derivative_sigmoid(self.forget_gate[idx]) * d_cs # shape of cell state: (hidden_size, 1), shape of forget gate: (hidden_size, 1), shape of d_cs: (hidden_size, 1)\n            d_wf += d_f @ self.concatenated_input[idx].T # shape of concatenated_input: (hidden_size+ chr_size, 1), shape of d_f: (hidden_size, 1), shape of d_wf: (hidden_size, hidden_size+ chr_size)\n            d_bf += d_f # shape of d_f: (hidden_size, 1), shape of d_bf: (hidden_size, 1)\n\n            # gradient wrt concatenated input error\n            d_z = self.wf.T @ d_f + self.wi.T @ d_i + self.wo.T @ d_o + self.wc.T @ d_cs # shape of d_z: (hidden_size + chr_size , 1)\n            \n            # gradient wrt hidden state at next time step\n            dh_next = d_z[: self.hidden_size, :]\n            # gradient wrt cell states at next time step\n            dc_next = self.forget_gate[idx]*d_cs\n            \n\n        for x in (d_wy, d_by, d_wf, d_bf, d_wc, d_bc, d_wi, d_bi, d_wo, d_bo):\n            x = np.clip(x, -1, 1)\n\n        self.wf = self.wf - self.learning_rate * d_wf\n        self.bf = self.bf - self.learning_rate * d_bf\n\n        self.wi = self.wi - self.learning_rate * d_wi\n        self.bi = self.bi - self.learning_rate * d_bi\n\n        self.wo = self.wo - self.learning_rate * d_wo\n        self.bo = self.bo - self.learning_rate * d_bo\n\n        self.wc = self.wc - self.learning_rate * d_wc\n        self.bc = self.bc - self.learning_rate * d_bc\n\n        self.wy = self.wy - self.learning_rate * d_wy\n        self.by = self.by - self.learning_rate * d_by\n\n\n    \n    def inference(self, input, idx_to_chr):\n        outputs = self.forward(input)\n        characters = ''\n        for output in outputs:\n            probs = softmax(output)\n            idx = np.argmax(probs)\n            # print(idx)\n            characters += (idx_to_chr[idx])\n\n        return characters\n\n\n##### Data #####\ndata = \"\"\"To be, or not to be, that is the question: Whether \\\n'tis nobler in the mind to suffer The slings and arrows of ou\\\ntrageous fortune, Or to take arms against a sea of troubles A\\\nnd by opposing end them. To die—to sleep, No more; and by a s\\\nleep to say we end The heart-ache and the thousand natural sh\\\nocks That flesh is heir to: 'tis a consummation Devoutly to b\\\ne wish'd. To die, to sleep; To sleep, perchance to dream—ay, \\\nthere's the rub: For in that sleep of death what dreams may c\\\nome, When we have shuffled off this mortal coil, Must give us\\\n pause—there's the respect That makes calamity of so long lif\\\ne. For who would bear the whips and scorns of time, Th'oppres\\\nsor's wrong, the proud man's contumely, The pangs of dispriz'\\\nd love, the law's delay, The insolence of office, and the spu\\\nrns That patient merit of th'unworthy takes, When he himself \\\nmight his quietus make\"\"\".lower()\n\n\nchars = set(data)\nnum_chars = len(chars)\n\nchr_to_idx = {c: i for i, c in enumerate(chars)}\nidx_to_chr = {i: c for i, c in enumerate(chars)}\n\ntrainX, trainY = data[:-1], data[1:]\n\nchar_size = len(chars)\nprint(f'Unique characters {char_size}, total training inputs {len(trainX)}')\nhidden_size = 10\n\nlearning_rate, num_epochs = 0.01, 1000\n\ntrainX = oneHotEncode(trainX, num_chars, chr_to_idx)\n\nlstm = LSTM(char_size + hidden_size, hidden_size, char_size, learning_rate, num_epochs)\n\nUnique characters 32, total training inputs 865\n\n\n\noutputs = lstm.forward(trainX)\nlen(outputs[0])\n\n32\n\n\n\nlen(outputs)\n\n865\n\n\n\nlstm.train(trainX, trainY, chr_to_idx)\n\n  3%|▎         | 28/1000 [00:04&lt;02:23,  6.78it/s]C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_13924\\2832247213.py:117: RuntimeWarning: overflow encountered in matmul\n  d_z = self.wf.T @ d_f + self.wi.T @ d_i + self.wo.T @ d_o + self.wc.T @ d_cs # shape of d_z: (hidden_size + chr_size , 1)\nC:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_13924\\2832247213.py:117: RuntimeWarning: overflow encountered in add\n  d_z = self.wf.T @ d_f + self.wi.T @ d_i + self.wo.T @ d_o + self.wc.T @ d_cs # shape of d_z: (hidden_size + chr_size , 1)\nC:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_13924\\2832247213.py:117: RuntimeWarning: invalid value encountered in add\n  d_z = self.wf.T @ d_f + self.wi.T @ d_i + self.wo.T @ d_o + self.wc.T @ d_cs # shape of d_z: (hidden_size + chr_size , 1)\nC:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_13924\\2832247213.py:94: RuntimeWarning: invalid value encountered in multiply\n  d_o = tanh(self.cell_state[idx]) * derivative_sigmoid(self.output_gate[idx]) * d_h # shape of cell state: (hidden_size, 1), shape of output gate: (hidden_size, 1) shape of d_o: (hidden_size, 1)\nC:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_13924\\2832247213.py:95: RuntimeWarning: invalid value encountered in matmul\n  d_wo += d_o @ self.concatenated_input[idx].T # shape of concatenated_input: (char_size + hidden_size, 1), shape of d_o: (hidden_size, 1), shape of w_o: (hidden_size, char_size + hidden_size), shape of d_wo: (hidden_size, char_size + hidden_size)\nC:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_13924\\2832247213.py:99: RuntimeWarning: invalid value encountered in multiply\n  d_cs = self.output_gate[idx] * derivative_tanh(tanh(self.cell_state[idx])) * d_h + dc_next # shape of output gate: (hidden_size, 1), shape of cell_state: (hidden_size, 1), shape of d_h: (hidden_size, 1), shape of d_cs: (hidden_size, 1)\nC:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_13924\\2832247213.py:102: RuntimeWarning: invalid value encountered in multiply\n  d_candidate = self.input_gate[idx] * derivative_tanh(self.intermediate_cell_state[idx]) * d_cs # shape of input gate: (hidden_size, 1), shape of intermediate_cell_state: (hidden_size, 1), shape of d_cs: (hidden_size, 1), shape of d_candidate: (hidden_size, 1)\nC:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_13924\\2832247213.py:107: RuntimeWarning: invalid value encountered in multiply\n  d_i = self.intermediate_cell_state[idx] * derivative_sigmoid(self.input_gate[idx]) * d_cs # shape of intermediate_cell_state: (hidden_size, 1), shape of input_gate: (hidden_size, 1), shape of d_cs: (hidden_size, 1), shape of d_i: (hidden_size, 1)\nC:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_13924\\2832247213.py:108: RuntimeWarning: invalid value encountered in matmul\n  d_wi += d_i @ self.concatenated_input[idx].T # shape of concatenated_input: (hidden_size+ chr_size, 1), shape of d_i: (hidden_size, 1), shape of d_wi; (hidden_size, hidden_size+ chr_size)\nC:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_13924\\2832247213.py:112: RuntimeWarning: invalid value encountered in multiply\n  d_f = self.cell_state[idx-1] * derivative_sigmoid(self.forget_gate[idx]) * d_cs # shape of cell state: (hidden_size, 1), shape of forget gate: (hidden_size, 1), shape of d_cs: (hidden_size, 1)\nC:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_13924\\2832247213.py:113: RuntimeWarning: invalid value encountered in matmul\n  d_wf += d_f @ self.concatenated_input[idx].T # shape of concatenated_input: (hidden_size+ chr_size, 1), shape of d_f: (hidden_size, 1), shape of d_wf: (hidden_size, hidden_size+ chr_size)\n100%|██████████| 1000/1000 [02:25&lt;00:00,  6.88it/s]\n\n\n\noutput = lstm.inference(trainX, idx_to_chr)\nprint(output)\nlen(output)\n\n:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n\n\n865\n\n\n\n\nWhat is the error in case of LSTM?\nAns: outputs is a vector of size char_size\nOn applying softmax to the output vector, we get a tensor that contains probabilities.\nWe calculate the cross-entropy loss: \\(-\\sum{y_ilog\\hat{y_i}}\\) —- Please check here does cross entropy loss include negation or not\nIf input x at timestep t belongs to class \\(l\\) then \\(y_l\\) = 1 and cross-entropy loss \\(L(\\theta) = -log(\\hat{y_l})\\)\n\\(\\frac{dL(\\theta)}{d\\hat{y_l}} = \\frac{-1}{\\hat{y_l}}\\)\n\\(\\hat{y} = softmax(z)\\)\n\\(\\hat{y} = (\\hat{y_1}, \\hat{y_2}, \\hat{y_3}, .......... \\hat{y_L})\\) where L is number of unique characters\n\\(\\frac{d\\hat{y_l}}{dz} = 1_{i = l}*softmax(z_l) - softmax(z_l)softmax(z_i) = 1_{i=l}* \\hat{y_l} - \\hat{y_l}*\\hat{y_i} = \\hat{y_l} (1_{i = l} - \\hat{y_i})\\)\n\\(\\frac{dL(\\theta)}{dz} = \\frac{-1}{\\hat{y_l}} * \\hat{y_l} (1_{i = l} - \\hat{y_i}) = (\\hat{y_i} - 1)\\) This vector is of size L\nReferences:\n\nhttps://github.com/CallMeTwitch/Neural-Network-Zoo/blob/main/LongShortTermMemoryNetwork.py"
  },
  {
    "objectID": "posts/Image-transformation.html",
    "href": "posts/Image-transformation.html",
    "title": "Image Transformation",
    "section": "",
    "text": "import numpy as np\nimport cv2\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/Image-transformation.html#reading-image",
    "href": "posts/Image-transformation.html#reading-image",
    "title": "Image Transformation",
    "section": "Reading Image",
    "text": "Reading Image\n\n# read the input image\nimg = cv2.imread(\"../images/trees.jpg\")\n# convert from BGR to RGB so we can plot using matplotlib\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n# disable x & y axis\nplt.axis('off')\n# show the image\nplt.imshow(img)\nplt.show()"
  },
  {
    "objectID": "posts/Image-transformation.html#translation-of-image",
    "href": "posts/Image-transformation.html#translation-of-image",
    "title": "Image Transformation",
    "section": "Translation of image",
    "text": "Translation of image\n\n# get the image shape\nprint(img.shape)\nrows, cols, dim = img.shape\n# transformation matrix for translation\nM = np.float32([[1, 0, 500],\n                [0, 1, 500],\n                [0, 0, 1]])\n# apply a perspective transformation to the image\ntranslated_img = cv2.warpPerspective(img, M, (cols, rows))\n# disable x & y axis\nplt.axis('off')\n# show the resulting image\nplt.imshow(translated_img)\nplt.show()\n\n(1606, 2222, 3)"
  },
  {
    "objectID": "posts/Image-transformation.html#scaling-of-image",
    "href": "posts/Image-transformation.html#scaling-of-image",
    "title": "Image Transformation",
    "section": "Scaling of Image",
    "text": "Scaling of Image\n\n# get the image shape\nprint(img.shape)\nrows, cols, dim = img.shape\n#transformation matrix for Scaling\nM = np.float32([[1.5, 0  , 0],\n                [0,   1.8, 0],\n                [0,   0,   1]])\n# apply a perspective transformation to the image\nscaled_img = cv2.warpPerspective(img,M,(cols,rows))\n# disable x & y axis\nplt.axis('off')\n# show the resulting image\nplt.imshow(scaled_img)\nplt.show()\n\n(1606, 2222, 3)"
  },
  {
    "objectID": "posts/Image-transformation.html#shearing-of-image",
    "href": "posts/Image-transformation.html#shearing-of-image",
    "title": "Image Transformation",
    "section": "Shearing of Image",
    "text": "Shearing of Image\n\n# get the image shape\nprint(img.shape)\nrows, cols, dim = img.shape\n# transformation matrix for Shearing\n# shearing applied to x-axis\nM = np.float32([[1, 0.5, 0],\n                [0, 1  , 0],\n                [0, 0  , 1]])\n# shearing applied to y-axis\n# M = np.float32([[1,   0, 0],\n#                 [0.5, 1, 0],\n#                 [0,   0, 1]])\n# apply a perspective transformation to the image                \nsheared_img = cv2.warpPerspective(img,M,(cols,rows))\n# disable x & y axis\nplt.axis('off')\n# show the resulting image\nplt.imshow(sheared_img)\nplt.show()\n\n(1606, 2222, 3)"
  },
  {
    "objectID": "posts/Image-transformation.html#reflection-of-image",
    "href": "posts/Image-transformation.html#reflection-of-image",
    "title": "Image Transformation",
    "section": "Reflection of Image",
    "text": "Reflection of Image\n\n# get the image shape\nrows, cols, dim = img.shape\n# transformation matrix for x-axis reflection \nM1 = np.float32([[1,  0, 0   ],\n                [0, -1, rows],\n                [0,  0, 1   ]])\n# transformation matrix for y-axis reflection\nM2 = np.float32([[-1, 0, cols],\n                [ 0, 1, 0   ],\n                [ 0, 0, 1   ]])\n# apply a perspective transformation to the image\nh_reflected_img = cv2.warpPerspective(img,M1,(cols,rows))\nv_reflected_img = cv2.warpPerspective(img,M2,(cols,rows))\n# disable x & y axis\nplt.axis('off')\n# show the resulting image\ncombined=np.hstack((h_reflected_img,v_reflected_img))\n\nplt.title('X-axis reflection and Y-axis reflection')\nplt.imshow(combined)\nplt.show()"
  },
  {
    "objectID": "posts/Image-transformation.html#rotation-of-image",
    "href": "posts/Image-transformation.html#rotation-of-image",
    "title": "Image Transformation",
    "section": "Rotation of Image",
    "text": "Rotation of Image\n\n# get the image shape\nrows, cols, dim = img.shape\n#angle from degree to radian\nangle = np.radians(10)\n#transformation matrix for Rotation\nM = np.float32([[np.cos(angle), -(np.sin(angle)), 0],\n                [np.sin(angle), np.cos(angle), 0],\n                [0, 0, 1]])\n# apply a perspective transformation to the image\nrotated_img = cv2.warpPerspective(img, M, (int(cols),int(rows)))\n# disable x & y axis\nplt.axis('off')\n# show the resulting image\nplt.imshow(rotated_img)\nplt.show()"
  },
  {
    "objectID": "posts/Image-transformation.html#cropping-of-image",
    "href": "posts/Image-transformation.html#cropping-of-image",
    "title": "Image Transformation",
    "section": "Cropping of Image",
    "text": "Cropping of Image\n\n# get 200 pixels from 100 to 300 on both x-axis & y-axis\n# change that if you will, just make sure you don't exceed cols & rows\ncropped_img = img[100:300, 100:300]\n# disable x & y axis\nplt.axis('off')\n# show the resulting image\nplt.imshow(cropped_img)\nplt.show()"
  },
  {
    "objectID": "posts/Image-transformation.html#center-crop-of-image",
    "href": "posts/Image-transformation.html#center-crop-of-image",
    "title": "Image Transformation",
    "section": "Center Crop of Image",
    "text": "Center Crop of Image\n\n# get the image width & height\nwidth, height = img.shape[1], img.shape[0]\n\n# get the image cropped width & height\ncrop_width , crop_height = 500, 600\n\nhalf_crop_width , half_crop_height = int(crop_width/2) , int(crop_height/2)\n\nmid_x, mid_y = int(width/2), int(height/2)\n\ncenter_cropped_img=img[mid_y-half_crop_height : mid_y+half_crop_height, mid_x-half_crop_width : mid_x+half_crop_width]\n\n# disable x & y axis\nplt.axis('off')\n# show the resulting image\nplt.imshow(center_cropped_img)\nplt.show()"
  },
  {
    "objectID": "posts/object_detection_pipeline.html",
    "href": "posts/object_detection_pipeline.html",
    "title": "Object detection pipeline",
    "section": "",
    "text": "Object detection Pipeline"
  },
  {
    "objectID": "posts/ReLU_activation.html",
    "href": "posts/ReLU_activation.html",
    "title": "ReLU Activation Functions",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\n\n\nbias = True\nlinear = nn.Linear(2, 4, bias)\nprint(f'Weights of the linear layer {linear.weight}')\nprint(f'Bias in the linear layer {linear.bias}')\nprint(f'dtype of weights {linear.weight.dtype}')\ninput = torch.arange(start=1, end = 3)\ninput = input.reshape(1,2)\ninput = input.to(torch.float32)\nprint(f'Input {input}')\noriginal_output = linear(input)\nprint(f'Output {original_output}')\nprint(f'Output shape {original_output.shape}')\n\nWeights of the linear layer Parameter containing:\ntensor([[ 0.5120, -0.4466],\n        [ 0.2942, -0.1407],\n        [ 0.2281, -0.0761],\n        [-0.5581,  0.6117]], requires_grad=True)\nBias in the linear layer Parameter containing:\ntensor([-0.3678, -0.3932,  0.4398,  0.2615], requires_grad=True)\ndtype of weights torch.float32\nInput tensor([[1., 2.]])\nOutput tensor([[-0.7489, -0.3803,  0.5158,  0.9267]], grad_fn=&lt;AddmmBackward0&gt;)\nOutput shape torch.Size([1, 4])\n\n\n\nlinear(input)\n\ntensor([[-0.7489, -0.3803,  0.5158,  0.9267]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\n\nif bias:\n  output = linear.weight[0]@input[0] + linear.bias[0]\nelse:\n  output = linear.weight[0]@input[0]\nprint(output)\n\ntensor(-0.7489, grad_fn=&lt;AddBackward0&gt;)\n\n\nlinear(\\(2*input\\)) = \\(2*linear(input)\\) when bias is False.\nlinear(\\(2*input\\)) != \\(2*linear(input)\\) when bias is True. The transformation is affine in this case.\n\nlinear(2*input)\n\ntensor([[-1.1300, -0.3675,  0.5917,  1.5919]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\n\n2*linear(input)\n\ntensor([[-1.4978, -0.7607,  1.0316,  1.8534]], grad_fn=&lt;MulBackward0&gt;)\n\n\n\nif bias:\n  output = linear.weight[0]@(2*input[0]) + linear.bias[0]\nelse:\n  output = linear.weight[0]@(2*input[0])\nprint(output)\n\ntensor(-1.1300, grad_fn=&lt;AddBackward0&gt;)\n\n\n\nlinear(input + 5)\n\ntensor([[-0.4215,  0.3874,  1.2760,  1.1945]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\n\nif bias:\n  output = linear(input) + linear(torch.tensor([5.0,5.0])) - linear.bias[0]\nelse:\n  output = linear(input) + linear(torch.tensor([5.0,5.0]))\nprint(output)\n\ntensor([[-0.4215,  0.3620,  2.0836,  1.8237]], grad_fn=&lt;SubBackward0&gt;)\n\n\n\nif bias:\n  output = linear.weight[0]@(input[0] + 5)+ linear.bias[0]\nelse:\n  output = linear.weight[0]@(input[0] + 5)\nprint(output)\n\ntensor(-0.4215, grad_fn=&lt;AddBackward0&gt;)\n\n\nLets see the non linearity added by ReLU activation function\n\noutput = original_output.squeeze(dim = 0)\noutput\n\ntensor([-0.7489, -0.3803,  0.5158,  0.9267], grad_fn=&lt;SqueezeBackward1&gt;)\n\n\n\nrelu = nn.ReLU()\nrelu(output)\n\ntensor([0.0000, 0.0000, 0.5158, 0.9267], grad_fn=&lt;ReluBackward0&gt;)\n\n\nReLU is not a linear function by the definition of Linear functions\n\n\\(ReLU(2*a) = 2*ReLU(a)\\)\n\n\nrelu(-2*output)\n\ntensor([1.4978, 0.7607, 0.0000, 0.0000], grad_fn=&lt;ReluBackward0&gt;)\n\n\n\n-2*relu(output)\n\ntensor([-0.0000, -0.0000, -1.0316, -1.8534], grad_fn=&lt;MulBackward0&gt;)\n\n\n\n\\(ReLU(a + b)\\) \\(!=\\) \\(ReLU(a) + ReLU(b)\\)\n\n\nrelu(output + output.min())\n\ntensor([0.0000, 0.0000, 0.0000, 0.1778], grad_fn=&lt;ReluBackward0&gt;)\n\n\n\nrelu(output) + relu(output.min())\n\ntensor([0.0000, 0.0000, 0.5158, 0.9267], grad_fn=&lt;AddBackward0&gt;)\n\n\nLets see what happens through multiple linear layers\n\nfrom tqdm import tqdm\ntrain_data = torch.tensor(torch.arange(1,100))\ntrain_data = train_data.to(torch.float32)\ntrain_data = train_data.reshape(99,1)\ntrain_out = train_data**2\nprint(train_data.size())\nprint(train_out.size())\nlinear = nn.Linear(1,4)\nhidden_linear = nn.Linear(4,4)\nout_linear = nn.Linear(4,1)\nmodel = nn.Sequential(\n    nn.Linear(1,8),\n    nn.Linear(8,8),\n    nn.Linear(8,1),\n)\n\nepochs = 100000\nloss_fn = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n\nfor epoch in tqdm(range(epochs)):\n  optimizer.zero_grad()\n  pred = model(train_data)\n  loss = loss_fn(pred, train_out)\n  loss.backward()\n  optimizer.step()\n  print(f'Epoch {epoch} loss {loss}')\n\nWithout ReLU activation function, there is no non-linearity in the network due to which the squared function is not followed by the neural network.\n\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots()\ninput = torch.tensor(torch.arange(1,101))\nprint(input)\ninput = input.to(torch.float32)\ninput = input.reshape(100,1)\nout = model(input)\nax.plot(input, out.detach(), color = 'orange', linewidth = 4, label = 'predicted')\nax.plot(input, input**2, color = 'green', label = 'ground truth')\nplt.legend()\n\ntensor([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,  14,\n         15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  28,\n         29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,\n         43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,  56,\n         57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,  70,\n         71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,  84,\n         85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,  98,\n         99, 100])\n\n\n/tmp/ipython-input-87-1621197856.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input = torch.tensor(torch.arange(1,101))\n\n\n\n\n\n\n\n\n\n\nfrom tqdm import tqdm\ntorch.random.manual_seed(42)\ninput = torch.tensor(torch.arange(1,100))\ninput = input.to(torch.float32)\ntrain_data = input.reshape(99,1)\ntrain_out = train_data**2\n\nrelu = nn.ReLU()\nmodel = nn.Sequential(\n    nn.Linear(1,8),\n    relu,\n    nn.Linear(8,8),\n    relu,\n    nn.Linear(8,1),\n\n)\n\nepochs = 100000\nloss_fn = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)\nfor epoch in range(epochs):\n  optimizer.zero_grad()\n  pred = model(train_data)\n  loss = loss_fn(pred, train_out)\n  loss.backward()\n  optimizer.step()\n  print(f'Epoch {epoch} loss {loss}')\n\n\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots()\ninput = torch.tensor(torch.arange(100,200))\nprint(input)\ninput = input.to(torch.float32)\ninput = input.reshape(100,1)\nout = model(input)\nax.plot(input, out.detach(), color = 'orange', linewidth = 4, label = 'predicted')\nax.plot(input, input**2, color = 'green', label = 'ground truth')\nplt.legend()\n\ntensor([100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113,\n        114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127,\n        128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n        184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197,\n        198, 199])\n\n\n/tmp/ipython-input-89-1213791851.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input = torch.tensor(torch.arange(100,200))\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots()\ninput = torch.tensor(torch.arange(1,101))\nprint(input)\ninput = input.to(torch.float32)\ninput = input.reshape(100,1)\nout = model(input)\nax.plot(input, out.detach(), color = 'orange', linewidth = 4, label = 'predicted')\nax.plot(input, input**2, color = 'green', label = 'ground truth')\nax.set_xlim(1,10)\nax.set_ylim(0,100)\nplt.legend()\n\ntensor([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,  14,\n         15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  28,\n         29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,\n         43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,  56,\n         57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,  70,\n         71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,  84,\n         85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,  98,\n         99, 100])\n\n\n/tmp/ipython-input-93-179746123.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input = torch.tensor(torch.arange(1,101))\n\n\n\n\n\n\n\n\n\nThis shows that using a series of ReLU activation functions introduces non-linearity into the neural network. A neural network without any activation functions produces only linear outputs, even on the training data. However, when ReLU is added, the network is able to learn the square function on the training data, and the learned function also approximates the actual function more closely on the test data."
  },
  {
    "objectID": "posts/python-format-specifier.html",
    "href": "posts/python-format-specifier.html",
    "title": "Python Format Specifier",
    "section": "",
    "text": "To format a number to sum precision point using format\n\n'{:.3f}'.format(3.145667)\n\n'3.146'\n\n\n\n'{:.3f}'.format(3.1)\n\n'3.100'"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Shataxi Dubey",
    "section": "",
    "text": "Hi! I am Shataxi Dubey an M.Tech Computer Science & Engineering student at Indian Institute of Technology Gandhinagar.\nMy interest area lies in Machine Learning and Computer Vision. Currently exploring the Earth through satellite imagery to unveil the hidden patterns and facts. I am working on my thesis Brick Kiln detection from Satellite Imagery using Vision Language Models under the guidance of Prof. Nipun Batra.\nHoping to add sustainable solutions to make world better!"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Shataxi Dubey",
    "section": "Education",
    "text": "Education\nB.Tech Computer Science and Engineering | University of Allahabad | Prayagraj, Uttar Pradesh | July 2018 - May 2022"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Shataxi Dubey",
    "section": "Experience",
    "text": "Experience\nAssociate Engineer | Indiamart Intermesh Ltd | May 2022 - July 2023"
  },
  {
    "objectID": "posts/vlm_sizes.html",
    "href": "posts/vlm_sizes.html",
    "title": "Vision Language Models",
    "section": "",
    "text": "import os\nos.environ['CUDA_VISIBLE_DEVICES'] = '3'\n\nfrom transformers import BitsAndBytesConfig\nimport torch\nfrom transformers import Qwen2_5_VLForConditionalGeneration\nbnb_config = BitsAndBytesConfig(\n                load_in_4bit=True,\n                # bnb_4bit_use_double_quant=True,\n                bnb_4bit_quant_type=\"nf4\",\n                bnb_4bit_compute_type=torch.bfloat16,)\n\nmodel = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n    \"Qwen/Qwen2.5-VL-72B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\", quantization_config=bnb_config\n)\n\nmodel.get_memory_footprint()\n\n\n\n\n40448783184\n\n\n\nmodel\n\nQwen2_5_VLForConditionalGeneration(\n  (visual): Qwen2_5_VisionTransformerPretrainedModel(\n    (patch_embed): Qwen2_5_VisionPatchEmbed(\n      (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n    )\n    (rotary_pos_emb): Qwen2_5_VisionRotaryEmbedding()\n    (blocks): ModuleList(\n      (0-31): 32 x Qwen2_5_VLVisionBlock(\n        (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n        (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n        (attn): Qwen2_5_VLVisionSdpaAttention(\n          (qkv): Linear4bit(in_features=1280, out_features=3840, bias=True)\n          (proj): Linear4bit(in_features=1280, out_features=1280, bias=True)\n        )\n        (mlp): Qwen2_5_VLMLP(\n          (gate_proj): Linear4bit(in_features=1280, out_features=3456, bias=True)\n          (up_proj): Linear4bit(in_features=1280, out_features=3456, bias=True)\n          (down_proj): Linear4bit(in_features=3456, out_features=1280, bias=True)\n          (act_fn): SiLU()\n        )\n      )\n    )\n    (merger): Qwen2_5_VLPatchMerger(\n      (ln_q): Qwen2RMSNorm((1280,), eps=1e-06)\n      (mlp): Sequential(\n        (0): Linear4bit(in_features=5120, out_features=5120, bias=True)\n        (1): GELU(approximate='none')\n        (2): Linear4bit(in_features=5120, out_features=8192, bias=True)\n      )\n    )\n  )\n  (model): Qwen2_5_VLModel(\n    (embed_tokens): Embedding(152064, 8192)\n    (layers): ModuleList(\n      (0-79): 80 x Qwen2_5_VLDecoderLayer(\n        (self_attn): Qwen2_5_VLSdpaAttention(\n          (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=True)\n          (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=True)\n          (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=True)\n          (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n          (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n        )\n        (mlp): Qwen2MLP(\n          (gate_proj): Linear4bit(in_features=8192, out_features=29568, bias=False)\n          (up_proj): Linear4bit(in_features=8192, out_features=29568, bias=False)\n          (down_proj): Linear4bit(in_features=29568, out_features=8192, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): Qwen2RMSNorm((8192,), eps=1e-06)\n        (post_attention_layernorm): Qwen2RMSNorm((8192,), eps=1e-06)\n      )\n    )\n    (norm): Qwen2RMSNorm((8192,), eps=1e-06)\n    (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=8192, out_features=152064, bias=False)\n)\n\n\n\nfrom peft import LoraConfig, get_peft_model\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\"q_proj\", \"v_proj\", \"o_proj\", \"k_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\npeft_model = get_peft_model(model, lora_config)\npeft_model.print_trainable_parameters()\nprint(peft_model.get_memory_footprint())\npeft_model\n\ntrainable params: 108,904,448 || all params: 73,519,681,792 || trainable%: 0.1481\n40884400976\n\n\nPeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): Qwen2_5_VLForConditionalGeneration(\n      (visual): Qwen2_5_VisionTransformerPretrainedModel(\n        (patch_embed): Qwen2_5_VisionPatchEmbed(\n          (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n        )\n        (rotary_pos_emb): Qwen2_5_VisionRotaryEmbedding()\n        (blocks): ModuleList(\n          (0-31): 32 x Qwen2_5_VLVisionBlock(\n            (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n            (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n            (attn): Qwen2_5_VLVisionSdpaAttention(\n              (qkv): Linear4bit(in_features=1280, out_features=3840, bias=True)\n              (proj): Linear4bit(in_features=1280, out_features=1280, bias=True)\n            )\n            (mlp): Qwen2_5_VLMLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1280, out_features=3456, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1280, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=3456, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1280, out_features=3456, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1280, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=3456, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3456, out_features=1280, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3456, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1280, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n          )\n        )\n        (merger): Qwen2_5_VLPatchMerger(\n          (ln_q): Qwen2RMSNorm((1280,), eps=1e-06)\n          (mlp): Sequential(\n            (0): Linear4bit(in_features=5120, out_features=5120, bias=True)\n            (1): GELU(approximate='none')\n            (2): Linear4bit(in_features=5120, out_features=8192, bias=True)\n          )\n        )\n      )\n      (model): Qwen2_5_VLModel(\n        (embed_tokens): Embedding(152064, 8192)\n        (layers): ModuleList(\n          (0-79): 80 x Qwen2_5_VLDecoderLayer(\n            (self_attn): Qwen2_5_VLSdpaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=8192, out_features=8192, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=8192, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=8192, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=8192, out_features=1024, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=8192, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=8192, out_features=1024, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=8192, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=8192, out_features=8192, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=8192, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=8192, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n            )\n            (mlp): Qwen2MLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=8192, out_features=29568, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=8192, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=29568, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=8192, out_features=29568, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=8192, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=29568, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=29568, out_features=8192, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=29568, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=8192, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): Qwen2RMSNorm((8192,), eps=1e-06)\n            (post_attention_layernorm): Qwen2RMSNorm((8192,), eps=1e-06)\n          )\n        )\n        (norm): Qwen2RMSNorm((8192,), eps=1e-06)\n        (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=8192, out_features=152064, bias=False)\n    )\n  )\n)\n\n\n\nfrom peft import LoraConfig, get_peft_model\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\"q_proj\", \"v_proj\",],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\npeft_model = get_peft_model(model, lora_config)\npeft_model.print_trainable_parameters()\nprint(peft_model.get_memory_footprint())\npeft_model\n\ntrainable params: 16,384,000 || all params: 73,427,161,344 || trainable%: 0.0223\n40514319184\n\n\nPeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): Qwen2_5_VLForConditionalGeneration(\n      (visual): Qwen2_5_VisionTransformerPretrainedModel(\n        (patch_embed): Qwen2_5_VisionPatchEmbed(\n          (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n        )\n        (rotary_pos_emb): Qwen2_5_VisionRotaryEmbedding()\n        (blocks): ModuleList(\n          (0-31): 32 x Qwen2_5_VLVisionBlock(\n            (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n            (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n            (attn): Qwen2_5_VLVisionSdpaAttention(\n              (qkv): Linear4bit(in_features=1280, out_features=3840, bias=True)\n              (proj): Linear4bit(in_features=1280, out_features=1280, bias=True)\n            )\n            (mlp): Qwen2_5_VLMLP(\n              (gate_proj): Linear4bit(in_features=1280, out_features=3456, bias=True)\n              (up_proj): Linear4bit(in_features=1280, out_features=3456, bias=True)\n              (down_proj): Linear4bit(in_features=3456, out_features=1280, bias=True)\n              (act_fn): SiLU()\n            )\n          )\n        )\n        (merger): Qwen2_5_VLPatchMerger(\n          (ln_q): Qwen2RMSNorm((1280,), eps=1e-06)\n          (mlp): Sequential(\n            (0): Linear4bit(in_features=5120, out_features=5120, bias=True)\n            (1): GELU(approximate='none')\n            (2): Linear4bit(in_features=5120, out_features=8192, bias=True)\n          )\n        )\n      )\n      (model): Qwen2_5_VLModel(\n        (embed_tokens): Embedding(152064, 8192)\n        (layers): ModuleList(\n          (0-79): 80 x Qwen2_5_VLDecoderLayer(\n            (self_attn): Qwen2_5_VLSdpaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=8192, out_features=8192, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=8192, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=8192, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=True)\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=8192, out_features=1024, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=8192, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n              (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n            )\n            (mlp): Qwen2MLP(\n              (gate_proj): Linear4bit(in_features=8192, out_features=29568, bias=False)\n              (up_proj): Linear4bit(in_features=8192, out_features=29568, bias=False)\n              (down_proj): Linear4bit(in_features=29568, out_features=8192, bias=False)\n              (act_fn): SiLU()\n            )\n            (input_layernorm): Qwen2RMSNorm((8192,), eps=1e-06)\n            (post_attention_layernorm): Qwen2RMSNorm((8192,), eps=1e-06)\n          )\n        )\n        (norm): Qwen2RMSNorm((8192,), eps=1e-06)\n        (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=8192, out_features=152064, bias=False)\n    )\n  )\n)\n\n\n\nimport os\nos.environ['CUDA_VISIBLE_DEVICES'] = '3'\n\nfrom transformers import BitsAndBytesConfig\nimport torch\nfrom transformers import PaliGemmaForConditionalGeneration\n\nbnb_config = BitsAndBytesConfig(\n                load_in_4bit=True,\n                # bnb_4bit_use_double_quant=True,\n                bnb_4bit_quant_type=\"nf4\",\n                bnb_4bit_compute_type=torch.bfloat16,)\n\nmodel = PaliGemmaForConditionalGeneration.from_pretrained(\"google/paligemma2-3b-pt-448\", torch_dtype=torch.bfloat16, device_map=\"auto\")\nmodel.get_memory_footprint()\n\n\n\n\n6066263008\n\n\n\nmodel\n\nPaliGemmaForConditionalGeneration(\n  (vision_tower): SiglipVisionModel(\n    (vision_model): SiglipVisionTransformer(\n      (embeddings): SiglipVisionEmbeddings(\n        (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n        (position_embedding): Embedding(1024, 1152)\n      )\n      (encoder): SiglipEncoder(\n        (layers): ModuleList(\n          (0-26): 27 x SiglipEncoderLayer(\n            (self_attn): SiglipSdpaAttention(\n              (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n              (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n              (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n              (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n            )\n            (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n            (mlp): SiglipMLP(\n              (activation_fn): PytorchGELUTanh()\n              (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n              (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n            )\n            (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n          )\n        )\n      )\n      (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n    )\n  )\n  (multi_modal_projector): PaliGemmaMultiModalProjector(\n    (linear): Linear(in_features=1152, out_features=2304, bias=True)\n  )\n  (language_model): Gemma2ForCausalLM(\n    (model): Gemma2Model(\n      (embed_tokens): Embedding(257216, 2304, padding_idx=0)\n      (layers): ModuleList(\n        (0-25): 26 x Gemma2DecoderLayer(\n          (self_attn): Gemma2Attention(\n            (q_proj): Linear(in_features=2304, out_features=2048, bias=False)\n            (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n            (v_proj): Linear(in_features=2304, out_features=1024, bias=False)\n            (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n          )\n          (mlp): Gemma2MLP(\n            (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n            (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n            (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n            (act_fn): PytorchGELUTanh()\n          )\n          (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n          (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n          (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n          (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n        )\n      )\n      (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n      (rotary_emb): Gemma2RotaryEmbedding()\n    )\n    (lm_head): Linear(in_features=2304, out_features=257216, bias=False)\n  )\n)\n\n\n\nfrom peft import get_peft_model, LoraConfig\nimport peft\n\nlora_config = LoraConfig(\n        r=8,\n        lora_alpha=8,\n        target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n        task_type=\"CAUSAL_LM\",\n    )\n\npeft_model = get_peft_model(model, lora_config)\npeft_model.print_trainable_parameters()\nprint(peft_model.get_memory_footprint())\npeft_model\n\ntrainable params: 11,876,352 || all params: 3,045,003,504 || trainable%: 0.3900\n6113768416\n\n\nPeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): PaliGemmaForConditionalGeneration(\n      (vision_tower): SiglipVisionModel(\n        (vision_model): SiglipVisionTransformer(\n          (embeddings): SiglipVisionEmbeddings(\n            (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n            (position_embedding): Embedding(1024, 1152)\n          )\n          (encoder): SiglipEncoder(\n            (layers): ModuleList(\n              (0-26): 27 x SiglipEncoderLayer(\n                (self_attn): SiglipSdpaAttention(\n                  (k_proj): lora.Linear(\n                    (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Identity()\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1152, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=1152, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (v_proj): lora.Linear(\n                    (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Identity()\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1152, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=1152, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (q_proj): lora.Linear(\n                    (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Identity()\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1152, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=1152, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n                )\n                (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n                (mlp): SiglipMLP(\n                  (activation_fn): PytorchGELUTanh()\n                  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n                  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n                )\n                (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n              )\n            )\n          )\n          (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n        )\n      )\n      (multi_modal_projector): PaliGemmaMultiModalProjector(\n        (linear): Linear(in_features=1152, out_features=2304, bias=True)\n      )\n      (language_model): Gemma2ForCausalLM(\n        (model): Gemma2Model(\n          (embed_tokens): Embedding(257216, 2304, padding_idx=0)\n          (layers): ModuleList(\n            (0-25): 26 x Gemma2DecoderLayer(\n              (self_attn): Gemma2Attention(\n                (q_proj): lora.Linear(\n                  (base_layer): Linear(in_features=2304, out_features=2048, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2304, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=2048, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (k_proj): lora.Linear(\n                  (base_layer): Linear(in_features=2304, out_features=1024, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2304, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=1024, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (v_proj): lora.Linear(\n                  (base_layer): Linear(in_features=2304, out_features=1024, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2304, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=1024, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (o_proj): lora.Linear(\n                  (base_layer): Linear(in_features=2048, out_features=2304, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2048, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=2304, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n              )\n              (mlp): Gemma2MLP(\n                (gate_proj): lora.Linear(\n                  (base_layer): Linear(in_features=2304, out_features=9216, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2304, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=9216, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (up_proj): lora.Linear(\n                  (base_layer): Linear(in_features=2304, out_features=9216, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2304, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=9216, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (down_proj): lora.Linear(\n                  (base_layer): Linear(in_features=9216, out_features=2304, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=9216, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=2304, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (act_fn): PytorchGELUTanh()\n              )\n              (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n              (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n              (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n              (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n            )\n          )\n          (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n          (rotary_emb): Gemma2RotaryEmbedding()\n        )\n        (lm_head): Linear(in_features=2304, out_features=257216, bias=False)\n      )\n    )\n  )\n)\n\n\n\nimport os\nos.environ['CUDA_VISIBLE_DEVICES'] = '3'\n\nimport torch\nfrom transformers import AutoModelForCausalLM \n\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/Florence-2-large-ft\", torch_dtype=\"auto\", trust_remote_code=True)\nmodel.get_memory_footprint()\n\nFlorence2LanguageForConditionalGeneration has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n\n\n1645899954\n\n\n\nmodel.dtype\n\ntorch.float16\n\n\n\nmodel\n\nFlorence2ForConditionalGeneration(\n  (vision_tower): DaViT(\n    (convs): ModuleList(\n      (0): ConvEmbed(\n        (proj): Conv2d(3, 256, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n      )\n      (1): ConvEmbed(\n        (proj): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n      )\n      (2): ConvEmbed(\n        (proj): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (3): ConvEmbed(\n        (proj): Conv2d(1024, 2048, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n    (blocks): ModuleList(\n      (0): MySequential(\n        (0): MySequential(\n          (spatial_block): SpatialBlock(\n            (conv1): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n              )\n            )\n            (window_attn): PreNorm(\n              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n              (fn): WindowAttention(\n                (qkv): Linear(in_features=256, out_features=768, bias=True)\n                (proj): Linear(in_features=256, out_features=256, bias=True)\n                (softmax): Softmax(dim=-1)\n              )\n              (drop_path): Identity()\n            )\n            (conv2): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n              )\n            )\n            (ffn): PreNorm(\n              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n              (fn): Mlp(\n                (net): Sequential(\n                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n                  (act): GELU(approximate='none')\n                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n                )\n              )\n              (drop_path): Identity()\n            )\n          )\n          (channel_block): ChannelBlock(\n            (conv1): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n              )\n            )\n            (channel_attn): PreNorm(\n              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n              (fn): ChannelAttention(\n                (qkv): Linear(in_features=256, out_features=768, bias=True)\n                (proj): Linear(in_features=256, out_features=256, bias=True)\n              )\n              (drop_path): DropPath(drop_prob=0.004)\n            )\n            (conv2): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n              )\n            )\n            (ffn): PreNorm(\n              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n              (fn): Mlp(\n                (net): Sequential(\n                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n                  (act): GELU(approximate='none')\n                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n                )\n              )\n              (drop_path): DropPath(drop_prob=0.004)\n            )\n          )\n        )\n      )\n      (1): MySequential(\n        (0): MySequential(\n          (spatial_block): SpatialBlock(\n            (conv1): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n              )\n            )\n            (window_attn): PreNorm(\n              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (fn): WindowAttention(\n                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n                (proj): Linear(in_features=512, out_features=512, bias=True)\n                (softmax): Softmax(dim=-1)\n              )\n              (drop_path): DropPath(drop_prob=0.009)\n            )\n            (conv2): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n              )\n            )\n            (ffn): PreNorm(\n              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (fn): Mlp(\n                (net): Sequential(\n                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n                  (act): GELU(approximate='none')\n                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n                )\n              )\n              (drop_path): DropPath(drop_prob=0.009)\n            )\n          )\n          (channel_block): ChannelBlock(\n            (conv1): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n              )\n            )\n            (channel_attn): PreNorm(\n              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (fn): ChannelAttention(\n                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n                (proj): Linear(in_features=512, out_features=512, bias=True)\n              )\n              (drop_path): DropPath(drop_prob=0.013)\n            )\n            (conv2): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n              )\n            )\n            (ffn): PreNorm(\n              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (fn): Mlp(\n                (net): Sequential(\n                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n                  (act): GELU(approximate='none')\n                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n                )\n              )\n              (drop_path): DropPath(drop_prob=0.013)\n            )\n          )\n        )\n      )\n      (2): MySequential(\n        (0): MySequential(\n          (spatial_block): SpatialBlock(\n            (conv1): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (window_attn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): WindowAttention(\n                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n                (softmax): Softmax(dim=-1)\n              )\n              (drop_path): DropPath(drop_prob=0.017)\n            )\n            (conv2): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (ffn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): Mlp(\n                (net): Sequential(\n                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                  (act): GELU(approximate='none')\n                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n                )\n              )\n              (drop_path): DropPath(drop_prob=0.017)\n            )\n          )\n          (channel_block): ChannelBlock(\n            (conv1): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (channel_attn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): ChannelAttention(\n                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n              )\n              (drop_path): DropPath(drop_prob=0.022)\n            )\n            (conv2): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (ffn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): Mlp(\n                (net): Sequential(\n                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                  (act): GELU(approximate='none')\n                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n                )\n              )\n              (drop_path): DropPath(drop_prob=0.022)\n            )\n          )\n        )\n        (1): MySequential(\n          (spatial_block): SpatialBlock(\n            (conv1): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (window_attn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): WindowAttention(\n                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n                (softmax): Softmax(dim=-1)\n              )\n              (drop_path): DropPath(drop_prob=0.026)\n            )\n            (conv2): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (ffn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): Mlp(\n                (net): Sequential(\n                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                  (act): GELU(approximate='none')\n                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n                )\n              )\n              (drop_path): DropPath(drop_prob=0.026)\n            )\n          )\n          (channel_block): ChannelBlock(\n            (conv1): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (channel_attn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): ChannelAttention(\n                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n              )\n              (drop_path): DropPath(drop_prob=0.030)\n            )\n            (conv2): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (ffn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): Mlp(\n                (net): Sequential(\n                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                  (act): GELU(approximate='none')\n                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n                )\n              )\n              (drop_path): DropPath(drop_prob=0.030)\n            )\n          )\n        )\n        (2): MySequential(\n          (spatial_block): SpatialBlock(\n            (conv1): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (window_attn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): WindowAttention(\n                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n                (softmax): Softmax(dim=-1)\n              )\n              (drop_path): DropPath(drop_prob=0.035)\n            )\n            (conv2): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (ffn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): Mlp(\n                (net): Sequential(\n                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                  (act): GELU(approximate='none')\n                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n                )\n              )\n              (drop_path): DropPath(drop_prob=0.035)\n            )\n          )\n          (channel_block): ChannelBlock(\n            (conv1): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (channel_attn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): ChannelAttention(\n                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n              )\n              (drop_path): DropPath(drop_prob=0.039)\n            )\n            (conv2): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (ffn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): Mlp(\n                (net): Sequential(\n                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                  (act): GELU(approximate='none')\n                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n                )\n              )\n              (drop_path): DropPath(drop_prob=0.039)\n            )\n          )\n        )\n        (3): MySequential(\n          (spatial_block): SpatialBlock(\n            (conv1): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (window_attn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): WindowAttention(\n                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n                (softmax): Softmax(dim=-1)\n              )\n              (drop_path): DropPath(drop_prob=0.043)\n            )\n            (conv2): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (ffn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): Mlp(\n                (net): Sequential(\n                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                  (act): GELU(approximate='none')\n                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n                )\n              )\n              (drop_path): DropPath(drop_prob=0.043)\n            )\n          )\n          (channel_block): ChannelBlock(\n            (conv1): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (channel_attn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): ChannelAttention(\n                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n              )\n              (drop_path): DropPath(drop_prob=0.048)\n            )\n            (conv2): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (ffn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): Mlp(\n                (net): Sequential(\n                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                  (act): GELU(approximate='none')\n                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n                )\n              )\n              (drop_path): DropPath(drop_prob=0.048)\n            )\n          )\n        )\n        (4): MySequential(\n          (spatial_block): SpatialBlock(\n            (conv1): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (window_attn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): WindowAttention(\n                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n                (softmax): Softmax(dim=-1)\n              )\n              (drop_path): DropPath(drop_prob=0.052)\n            )\n            (conv2): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (ffn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): Mlp(\n                (net): Sequential(\n                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                  (act): GELU(approximate='none')\n                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n                )\n              )\n              (drop_path): DropPath(drop_prob=0.052)\n            )\n          )\n          (channel_block): ChannelBlock(\n            (conv1): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (channel_attn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): ChannelAttention(\n                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n              )\n              (drop_path): DropPath(drop_prob=0.057)\n            )\n            (conv2): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (ffn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): Mlp(\n                (net): Sequential(\n                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                  (act): GELU(approximate='none')\n                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n                )\n              )\n              (drop_path): DropPath(drop_prob=0.057)\n            )\n          )\n        )\n        (5): MySequential(\n          (spatial_block): SpatialBlock(\n            (conv1): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (window_attn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): WindowAttention(\n                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n                (softmax): Softmax(dim=-1)\n              )\n              (drop_path): DropPath(drop_prob=0.061)\n            )\n            (conv2): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (ffn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): Mlp(\n                (net): Sequential(\n                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                  (act): GELU(approximate='none')\n                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n                )\n              )\n              (drop_path): DropPath(drop_prob=0.061)\n            )\n          )\n          (channel_block): ChannelBlock(\n            (conv1): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (channel_attn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): ChannelAttention(\n                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n              )\n              (drop_path): DropPath(drop_prob=0.065)\n            )\n            (conv2): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (ffn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): Mlp(\n                (net): Sequential(\n                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                  (act): GELU(approximate='none')\n                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n                )\n              )\n              (drop_path): DropPath(drop_prob=0.065)\n            )\n          )\n        )\n        (6): MySequential(\n          (spatial_block): SpatialBlock(\n            (conv1): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (window_attn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): WindowAttention(\n                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n                (softmax): Softmax(dim=-1)\n              )\n              (drop_path): DropPath(drop_prob=0.070)\n            )\n            (conv2): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (ffn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): Mlp(\n                (net): Sequential(\n                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                  (act): GELU(approximate='none')\n                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n                )\n              )\n              (drop_path): DropPath(drop_prob=0.070)\n            )\n          )\n          (channel_block): ChannelBlock(\n            (conv1): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (channel_attn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): ChannelAttention(\n                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n              )\n              (drop_path): DropPath(drop_prob=0.074)\n            )\n            (conv2): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (ffn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): Mlp(\n                (net): Sequential(\n                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                  (act): GELU(approximate='none')\n                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n                )\n              )\n              (drop_path): DropPath(drop_prob=0.074)\n            )\n          )\n        )\n        (7): MySequential(\n          (spatial_block): SpatialBlock(\n            (conv1): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (window_attn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): WindowAttention(\n                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n                (softmax): Softmax(dim=-1)\n              )\n              (drop_path): DropPath(drop_prob=0.078)\n            )\n            (conv2): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (ffn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): Mlp(\n                (net): Sequential(\n                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                  (act): GELU(approximate='none')\n                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n                )\n              )\n              (drop_path): DropPath(drop_prob=0.078)\n            )\n          )\n          (channel_block): ChannelBlock(\n            (conv1): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (channel_attn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): ChannelAttention(\n                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n              )\n              (drop_path): DropPath(drop_prob=0.083)\n            )\n            (conv2): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (ffn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): Mlp(\n                (net): Sequential(\n                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                  (act): GELU(approximate='none')\n                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n                )\n              )\n              (drop_path): DropPath(drop_prob=0.083)\n            )\n          )\n        )\n        (8): MySequential(\n          (spatial_block): SpatialBlock(\n            (conv1): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (window_attn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): WindowAttention(\n                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n                (softmax): Softmax(dim=-1)\n              )\n              (drop_path): DropPath(drop_prob=0.087)\n            )\n            (conv2): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (ffn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): Mlp(\n                (net): Sequential(\n                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                  (act): GELU(approximate='none')\n                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n                )\n              )\n              (drop_path): DropPath(drop_prob=0.087)\n            )\n          )\n          (channel_block): ChannelBlock(\n            (conv1): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (channel_attn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): ChannelAttention(\n                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n              )\n              (drop_path): DropPath(drop_prob=0.091)\n            )\n            (conv2): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n              )\n            )\n            (ffn): PreNorm(\n              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fn): Mlp(\n                (net): Sequential(\n                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                  (act): GELU(approximate='none')\n                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n                )\n              )\n              (drop_path): DropPath(drop_prob=0.091)\n            )\n          )\n        )\n      )\n      (3): MySequential(\n        (0): MySequential(\n          (spatial_block): SpatialBlock(\n            (conv1): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n              )\n            )\n            (window_attn): PreNorm(\n              (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n              (fn): WindowAttention(\n                (qkv): Linear(in_features=2048, out_features=6144, bias=True)\n                (proj): Linear(in_features=2048, out_features=2048, bias=True)\n                (softmax): Softmax(dim=-1)\n              )\n              (drop_path): DropPath(drop_prob=0.096)\n            )\n            (conv2): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n              )\n            )\n            (ffn): PreNorm(\n              (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n              (fn): Mlp(\n                (net): Sequential(\n                  (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n                  (act): GELU(approximate='none')\n                  (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n                )\n              )\n              (drop_path): DropPath(drop_prob=0.096)\n            )\n          )\n          (channel_block): ChannelBlock(\n            (conv1): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n              )\n            )\n            (channel_attn): PreNorm(\n              (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n              (fn): ChannelAttention(\n                (qkv): Linear(in_features=2048, out_features=6144, bias=True)\n                (proj): Linear(in_features=2048, out_features=2048, bias=True)\n              )\n              (drop_path): DropPath(drop_prob=0.100)\n            )\n            (conv2): PreNorm(\n              (fn): DepthWiseConv2d(\n                (dw): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n              )\n            )\n            (ffn): PreNorm(\n              (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n              (fn): Mlp(\n                (net): Sequential(\n                  (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n                  (act): GELU(approximate='none')\n                  (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n                )\n              )\n              (drop_path): DropPath(drop_prob=0.100)\n            )\n          )\n        )\n      )\n    )\n    (avgpool): AdaptiveAvgPool1d(output_size=1)\n  )\n  (image_proj_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n  (image_pos_embed): LearnedAbsolutePositionEmbedding2D(\n    (row_embeddings): Embedding(50, 1024)\n    (column_embeddings): Embedding(50, 1024)\n  )\n  (visual_temporal_embed): PositionalEmbeddingCosine1D()\n  (language_model): Florence2LanguageForConditionalGeneration(\n    (model): Florence2LanguageModel(\n      (shared): Embedding(51289, 1024, padding_idx=1)\n      (encoder): Florence2Encoder(\n        (embed_tokens): Florence2ScaledWordEmbedding(51289, 1024, padding_idx=1)\n        (embed_positions): Florence2LearnedPositionalEmbedding(1026, 1024)\n        (layers): ModuleList(\n          (0-11): 12 x Florence2EncoderLayer(\n            (self_attn): Florence2SdpaAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (activation_fn): GELUActivation()\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (decoder): Florence2Decoder(\n        (embed_tokens): Florence2ScaledWordEmbedding(51289, 1024, padding_idx=1)\n        (embed_positions): Florence2LearnedPositionalEmbedding(1026, 1024)\n        (layers): ModuleList(\n          (0-11): 12 x Florence2DecoderLayer(\n            (self_attn): Florence2SdpaAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (activation_fn): GELUActivation()\n            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (encoder_attn): Florence2SdpaAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n    (lm_head): Linear(in_features=1024, out_features=51289, bias=False)\n  )\n)\n\n\n\nfrom peft import LoraConfig, get_peft_model\nlora_config = LoraConfig(\n    r = 8,\n    lora_alpha = 8,\n    lora_dropout = 0.05,\n    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"linear\", \"Conv2d\", \"lm_head\", \"fc2\"],\n    task_type = \"CAUSAL_LM\"\n)\n\npeft_model = get_peft_model(model, lora_config)\npeft_model.print_trainable_parameters()\nprint(peft_model.get_memory_footprint())\npeft_model\n\ntrainable params: 4,133,576 || all params: 826,827,464 || trainable%: 0.4999\n1662434258\n\n\nPeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): Florence2ForConditionalGeneration(\n      (vision_tower): DaViT(\n        (convs): ModuleList(\n          (0): ConvEmbed(\n            (proj): Conv2d(3, 256, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          )\n          (1): ConvEmbed(\n            (proj): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          )\n          (2): ConvEmbed(\n            (proj): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          )\n          (3): ConvEmbed(\n            (proj): Conv2d(1024, 2048, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n        (blocks): ModuleList(\n          (0): MySequential(\n            (0): MySequential(\n              (spatial_block): SpatialBlock(\n                (conv1): PreNorm(\n                  (fn): DepthWiseConv2d(\n                    (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n                  )\n                )\n                (window_attn): PreNorm(\n                  (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n                  (fn): WindowAttention(\n                    (qkv): Linear(in_features=256, out_features=768, bias=True)\n                    (proj): Linear(in_features=256, out_features=256, bias=True)\n                    (softmax): Softmax(dim=-1)\n                  )\n                  (drop_path): Identity()\n                )\n                (conv2): PreNorm(\n                  (fn): DepthWiseConv2d(\n                    (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n                  )\n                )\n                (ffn): PreNorm(\n                  (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n                  (fn): Mlp(\n                    (net): Sequential(\n                      (fc1): Linear(in_features=256, out_features=1024, bias=True)\n                      (act): GELU(approximate='none')\n                      (fc2): lora.Linear(\n                        (base_layer): Linear(in_features=1024, out_features=256, bias=True)\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=1024, out_features=8, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=8, out_features=256, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                        (lora_magnitude_vector): ModuleDict()\n                      )\n                    )\n                  )\n                  (drop_path): Identity()\n                )\n              )\n              (channel_block): ChannelBlock(\n                (conv1): PreNorm(\n                  (fn): DepthWiseConv2d(\n                    (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n                  )\n                )\n                (channel_attn): PreNorm(\n                  (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n                  (fn): ChannelAttention(\n                    (qkv): Linear(in_features=256, out_features=768, bias=True)\n                    (proj): Linear(in_features=256, out_features=256, bias=True)\n                  )\n                  (drop_path): DropPath(drop_prob=0.004)\n                )\n                (conv2): PreNorm(\n                  (fn): DepthWiseConv2d(\n                    (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n                  )\n                )\n                (ffn): PreNorm(\n                  (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n                  (fn): Mlp(\n                    (net): Sequential(\n                      (fc1): Linear(in_features=256, out_features=1024, bias=True)\n                      (act): GELU(approximate='none')\n                      (fc2): lora.Linear(\n                        (base_layer): Linear(in_features=1024, out_features=256, bias=True)\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=1024, out_features=8, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=8, out_features=256, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                        (lora_magnitude_vector): ModuleDict()\n                      )\n                    )\n                  )\n                  (drop_path): DropPath(drop_prob=0.004)\n                )\n              )\n            )\n          )\n          (1): MySequential(\n            (0): MySequential(\n              (spatial_block): SpatialBlock(\n                (conv1): PreNorm(\n                  (fn): DepthWiseConv2d(\n                    (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n                  )\n                )\n                (window_attn): PreNorm(\n                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n                  (fn): WindowAttention(\n                    (qkv): Linear(in_features=512, out_features=1536, bias=True)\n                    (proj): Linear(in_features=512, out_features=512, bias=True)\n                    (softmax): Softmax(dim=-1)\n                  )\n                  (drop_path): DropPath(drop_prob=0.009)\n                )\n                (conv2): PreNorm(\n                  (fn): DepthWiseConv2d(\n                    (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n                  )\n                )\n                (ffn): PreNorm(\n                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n                  (fn): Mlp(\n                    (net): Sequential(\n                      (fc1): Linear(in_features=512, out_features=2048, bias=True)\n                      (act): GELU(approximate='none')\n                      (fc2): lora.Linear(\n                        (base_layer): Linear(in_features=2048, out_features=512, bias=True)\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=2048, out_features=8, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=8, out_features=512, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                        (lora_magnitude_vector): ModuleDict()\n                      )\n                    )\n                  )\n                  (drop_path): DropPath(drop_prob=0.009)\n                )\n              )\n              (channel_block): ChannelBlock(\n                (conv1): PreNorm(\n                  (fn): DepthWiseConv2d(\n                    (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n                  )\n                )\n                (channel_attn): PreNorm(\n                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n                  (fn): ChannelAttention(\n                    (qkv): Linear(in_features=512, out_features=1536, bias=True)\n                    (proj): Linear(in_features=512, out_features=512, bias=True)\n                  )\n                  (drop_path): DropPath(drop_prob=0.013)\n                )\n                (conv2): PreNorm(\n                  (fn): DepthWiseConv2d(\n                    (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n                  )\n                )\n                (ffn): PreNorm(\n                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n                  (fn): Mlp(\n                    (net): Sequential(\n                      (fc1): Linear(in_features=512, out_features=2048, bias=True)\n                      (act): GELU(approximate='none')\n                      (fc2): lora.Linear(\n                        (base_layer): Linear(in_features=2048, out_features=512, bias=True)\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=2048, out_features=8, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=8, out_features=512, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                        (lora_magnitude_vector): ModuleDict()\n                      )\n                    )\n                  )\n                  (drop_path): DropPath(drop_prob=0.013)\n                )\n              )\n            )\n          )\n          (2): MySequential(\n            (0): MySequential(\n              (spatial_block): SpatialBlock(\n                (conv1): PreNorm(\n                  (fn): DepthWiseConv2d(\n                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n                  )\n                )\n                (window_attn): PreNorm(\n                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n                  (fn): WindowAttention(\n                    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                    (proj): Linear(in_features=1024, out_features=1024, bias=True)\n                    (softmax): Softmax(dim=-1)\n                  )\n                  (drop_path): DropPath(drop_prob=0.017)\n                )\n                (conv2): PreNorm(\n                  (fn): DepthWiseConv2d(\n                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n                  )\n                )\n                (ffn): PreNorm(\n                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n                  (fn): Mlp(\n                    (net): Sequential(\n                      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                      (act): GELU(approximate='none')\n                      (fc2): lora.Linear(\n                        (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=4096, out_features=8, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=8, out_features=1024, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                        (lora_magnitude_vector): ModuleDict()\n                      )\n                    )\n                  )\n                  (drop_path): DropPath(drop_prob=0.017)\n                )\n              )\n              (channel_block): ChannelBlock(\n                (conv1): PreNorm(\n                  (fn): DepthWiseConv2d(\n                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n                  )\n                )\n                (channel_attn): PreNorm(\n                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n                  (fn): ChannelAttention(\n                    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                    (proj): Linear(in_features=1024, out_features=1024, bias=True)\n                  )\n                  (drop_path): DropPath(drop_prob=0.022)\n                )\n                (conv2): PreNorm(\n                  (fn): DepthWiseConv2d(\n                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n                  )\n                )\n                (ffn): PreNorm(\n                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n                  (fn): Mlp(\n                    (net): Sequential(\n                      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                      (act): GELU(approximate='none')\n                      (fc2): lora.Linear(\n                        (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=4096, out_features=8, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=8, out_features=1024, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                        (lora_magnitude_vector): ModuleDict()\n                      )\n                    )\n                  )\n                  (drop_path): DropPath(drop_prob=0.022)\n                )\n              )\n            )\n            (1): MySequential(\n              (spatial_block): SpatialBlock(\n                (conv1): PreNorm(\n                  (fn): DepthWiseConv2d(\n                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n                  )\n                )\n                (window_attn): PreNorm(\n                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n                  (fn): WindowAttention(\n                    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                    (proj): Linear(in_features=1024, out_features=1024, bias=True)\n                    (softmax): Softmax(dim=-1)\n                  )\n                  (drop_path): DropPath(drop_prob=0.026)\n                )\n                (conv2): PreNorm(\n                  (fn): DepthWiseConv2d(\n                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n                  )\n                )\n                (ffn): PreNorm(\n                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n                  (fn): Mlp(\n                    (net): Sequential(\n                      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                      (act): GELU(approximate='none')\n                      (fc2): lora.Linear(\n                        (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=4096, out_features=8, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=8, out_features=1024, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                        (lora_magnitude_vector): ModuleDict()\n                      )\n                    )\n                  )\n                  (drop_path): DropPath(drop_prob=0.026)\n                )\n              )\n              (channel_block): ChannelBlock(\n                (conv1): PreNorm(\n                  (fn): DepthWiseConv2d(\n                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n                  )\n                )\n                (channel_attn): PreNorm(\n                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n                  (fn): ChannelAttention(\n                    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                    (proj): Linear(in_features=1024, out_features=1024, bias=True)\n                  )\n                  (drop_path): DropPath(drop_prob=0.030)\n                )\n                (conv2): PreNorm(\n                  (fn): DepthWiseConv2d(\n                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n                  )\n                )\n                (ffn): PreNorm(\n                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n                  (fn): Mlp(\n                    (net): Sequential(\n                      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                      (act): GELU(approximate='none')\n                      (fc2): lora.Linear(\n                        (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=4096, out_features=8, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=8, out_features=1024, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                        (lora_magnitude_vector): ModuleDict()\n                      )\n                    )\n                  )\n                  (drop_path): DropPath(drop_prob=0.030)\n                )\n              )\n            )\n            (2): MySequential(\n              (spatial_block): SpatialBlock(\n                (conv1): PreNorm(\n                  (fn): DepthWiseConv2d(\n                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n                  )\n                )\n                (window_attn): PreNorm(\n                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n                  (fn): WindowAttention(\n                    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                    (proj): Linear(in_features=1024, out_features=1024, bias=True)\n                    (softmax): Softmax(dim=-1)\n                  )\n                  (drop_path): DropPath(drop_prob=0.035)\n                )\n                (conv2): PreNorm(\n                  (fn): DepthWiseConv2d(\n                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n                  )\n                )\n                (ffn): PreNorm(\n                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n                  (fn): Mlp(\n                    (net): Sequential(\n                      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                      (act): GELU(approximate='none')\n                      (fc2): lora.Linear(\n                        (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=4096, out_features=8, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=8, out_features=1024, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                        (lora_magnitude_vector): ModuleDict()\n                      )\n                    )\n                  )\n                  (drop_path): DropPath(drop_prob=0.035)\n                )\n              )\n              (channel_block): ChannelBlock(\n                (conv1): PreNorm(\n                  (fn): DepthWiseConv2d(\n                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n                  )\n                )\n                (channel_attn): PreNorm(\n                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n                  (fn): ChannelAttention(\n                    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                    (proj): Linear(in_features=1024, out_features=1024, bias=True)\n                  )\n                  (drop_path): DropPath(drop_prob=0.039)\n                )\n                (conv2): PreNorm(\n                  (fn): DepthWiseConv2d(\n                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n                  )\n                )\n                (ffn): PreNorm(\n                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n                  (fn): Mlp(\n                    (net): Sequential(\n                      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                      (act): GELU(approximate='none')\n                      (fc2): lora.Linear(\n                        (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=4096, out_features=8, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=8, out_features=1024, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                        (lora_magnitude_vector): ModuleDict()\n                      )\n                    )\n                  )\n                  (drop_path): DropPath(drop_prob=0.039)\n                )\n              )\n            )\n            (3): MySequential(\n              (spatial_block): SpatialBlock(\n                (conv1): PreNorm(\n                  (fn): DepthWiseConv2d(\n                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n                  )\n                )\n                (window_attn): PreNorm(\n                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n                  (fn): WindowAttention(\n                    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                    (proj): Linear(in_features=1024, out_features=1024, bias=True)\n                    (softmax): Softmax(dim=-1)\n                  )\n                  (drop_path): DropPath(drop_prob=0.043)\n                )\n                (conv2): PreNorm(\n                  (fn): DepthWiseConv2d(\n                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n                  )\n                )\n                (ffn): PreNorm(\n                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n                  (fn): Mlp(\n                    (net): Sequential(\n                      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                      (act): GELU(approximate='none')\n                      (fc2): lora.Linear(\n                        (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=4096, out_features=8, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=8, out_features=1024, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                        (lora_magnitude_vector): ModuleDict()\n                      )\n                    )\n                  )\n                  (drop_path): DropPath(drop_prob=0.043)\n                )\n              )\n              (channel_block): ChannelBlock(\n                (conv1): PreNorm(\n                  (fn): DepthWiseConv2d(\n                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n                  )\n                )\n                (channel_attn): PreNorm(\n                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n                  (fn): ChannelAttention(\n                    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                    (proj): Linear(in_features=1024, out_features=1024, bias=True)\n                  )\n                  (drop_path): DropPath(drop_prob=0.048)\n                )\n                (conv2): PreNorm(\n                  (fn): DepthWiseConv2d(\n                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n                  )\n                )\n                (ffn): PreNorm(\n                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n                  (fn): Mlp(\n                    (net): Sequential(\n                      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                      (act): GELU(approximate='none')\n                      (fc2): lora.Linear(\n                        (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=4096, out_features=8, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=8, out_features=1024, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                        (lora_magnitude_vector): ModuleDict()\n                      )\n                    )\n                  )\n                  (drop_path): DropPath(drop_prob=0.048)\n                )\n              )\n            )\n            (4): MySequential(\n              (spatial_block): SpatialBlock(\n                (conv1): PreNorm(\n                  (fn): DepthWiseConv2d(\n                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n                  )\n                )\n                (window_attn): PreNorm(\n                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n                  (fn): WindowAttention(\n                    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                    (proj): Linear(in_features=1024, out_features=1024, bias=True)\n                    (softmax): Softmax(dim=-1)\n                  )\n                  (drop_path): DropPath(drop_prob=0.052)\n                )\n                (conv2): PreNorm(\n                  (fn): DepthWiseConv2d(\n                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n                  )\n                )\n                (ffn): PreNorm(\n                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n                  (fn): Mlp(\n                    (net): Sequential(\n                      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                      (act): GELU(approximate='none')\n                      (fc2): lora.Linear(\n                        (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=4096, out_features=8, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=8, out_features=1024, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                        (lora_magnitude_vector): ModuleDict()\n                      )\n                    )\n                  )\n                  (drop_path): DropPath(drop_prob=0.052)\n                )\n              )\n              (channel_block): ChannelBlock(\n                (conv1): PreNorm(\n                  (fn): DepthWiseConv2d(\n                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n                  )\n                )\n                (channel_attn): PreNorm(\n                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n                  (fn): ChannelAttention(\n                    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                    (proj): Linear(in_features=1024, out_features=1024, bias=True)\n                  )\n                  (drop_path): DropPath(drop_prob=0.057)\n                )\n                (conv2): PreNorm(\n                  (fn): DepthWiseConv2d(\n                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n                  )\n                )\n                (ffn): PreNorm(\n                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n                  (fn): Mlp(\n                    (net): Sequential(\n                      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                      (act): GELU(approximate='none')\n                      (fc2): lora.Linear(\n                        (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=4096, out_features=8, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=8, out_features=1024, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                        (lora_magnitude_vector): ModuleDict()\n                      )\n                    )\n                  )\n                  (drop_path): DropPath(drop_prob=0.057)\n                )\n              )\n            )\n            (5): MySequential(\n              (spatial_block): SpatialBlock(\n                (conv1): PreNorm(\n                  (fn): DepthWiseConv2d(\n                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n                  )\n                )\n                (window_attn): PreNorm(\n                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n                  (fn): WindowAttention(\n                    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                    (proj): Linear(in_features=1024, out_features=1024, bias=True)\n                    (softmax): Softmax(dim=-1)\n                  )\n                  (drop_path): DropPath(drop_prob=0.061)\n                )\n                (conv2): PreNorm(\n                  (fn): DepthWiseConv2d(\n                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n                  )\n                )\n                (ffn): PreNorm(\n                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n                  (fn): Mlp(\n                    (net): Sequential(\n                      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                      (act): GELU(approximate='none')\n                      (fc2): lora.Linear(\n                        (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=4096, out_features=8, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=8, out_features=1024, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                        (lora_magnitude_vector): ModuleDict()\n                      )\n                    )\n                  )\n                  (drop_path): DropPath(drop_prob=0.061)\n                )\n              )\n              (channel_block): ChannelBlock(\n                (conv1): PreNorm(\n                  (fn): DepthWiseConv2d(\n                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n                  )\n                )\n                (channel_attn): PreNorm(\n                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n                  (fn): ChannelAttention(\n                    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                    (proj): Linear(in_features=1024, out_features=1024, bias=True)\n                  )\n                  (drop_path): DropPath(drop_prob=0.065)\n                )\n                (conv2): PreNorm(\n                  (fn): DepthWiseConv2d(\n                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n                  )\n                )\n                (ffn): PreNorm(\n                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n                  (fn): Mlp(\n                    (net): Sequential(\n                      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                      (act): GELU(approximate='none')\n                      (fc2): lora.Linear(\n                        (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=4096, out_features=8, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=8, out_features=1024, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                        (lora_magnitude_vector): ModuleDict()\n                      )\n                    )\n                  )\n                  (drop_path): DropPath(drop_prob=0.065)\n                )\n              )\n            )\n            (6): MySequential(\n              (spatial_block): SpatialBlock(\n                (conv1): PreNorm(\n                  (fn): DepthWiseConv2d(\n                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n                  )\n                )\n                (window_attn): PreNorm(\n                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n                  (fn): WindowAttention(\n                    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                    (proj): Linear(in_features=1024, out_features=1024, bias=True)\n                    (softmax): Softmax(dim=-1)\n                  )\n                  (drop_path): DropPath(drop_prob=0.070)\n                )\n                (conv2): PreNorm(\n                  (fn): DepthWiseConv2d(\n                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n                  )\n                )\n                (ffn): PreNorm(\n                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n                  (fn): Mlp(\n                    (net): Sequential(\n                      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                      (act): GELU(approximate='none')\n                      (fc2): lora.Linear(\n                        (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=4096, out_features=8, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=8, out_features=1024, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                        (lora_magnitude_vector): ModuleDict()\n                      )\n                    )\n                  )\n                  (drop_path): DropPath(drop_prob=0.070)\n                )\n              )\n              (channel_block): ChannelBlock(\n                (conv1): PreNorm(\n                  (fn): DepthWiseConv2d(\n                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n                  )\n                )\n                (channel_attn): PreNorm(\n                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n                  (fn): ChannelAttention(\n                    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                    (proj): Linear(in_features=1024, out_features=1024, bias=True)\n                  )\n                  (drop_path): DropPath(drop_prob=0.074)\n                )\n                (conv2): PreNorm(\n                  (fn): DepthWiseConv2d(\n                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n                  )\n                )\n                (ffn): PreNorm(\n                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n                  (fn): Mlp(\n                    (net): Sequential(\n                      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                      (act): GELU(approximate='none')\n                      (fc2): lora.Linear(\n                        (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=4096, out_features=8, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=8, out_features=1024, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                        (lora_magnitude_vector): ModuleDict()\n                      )\n                    )\n                  )\n                  (drop_path): DropPath(drop_prob=0.074)\n                )\n              )\n            )\n            (7): MySequential(\n              (spatial_block): SpatialBlock(\n                (conv1): PreNorm(\n                  (fn): DepthWiseConv2d(\n                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n                  )\n                )\n                (window_attn): PreNorm(\n                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n                  (fn): WindowAttention(\n                    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                    (proj): Linear(in_features=1024, out_features=1024, bias=True)\n                    (softmax): Softmax(dim=-1)\n                  )\n                  (drop_path): DropPath(drop_prob=0.078)\n                )\n                (conv2): PreNorm(\n                  (fn): DepthWiseConv2d(\n                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n                  )\n                )\n                (ffn): PreNorm(\n                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n                  (fn): Mlp(\n                    (net): Sequential(\n                      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                      (act): GELU(approximate='none')\n                      (fc2): lora.Linear(\n                        (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=4096, out_features=8, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=8, out_features=1024, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                        (lora_magnitude_vector): ModuleDict()\n                      )\n                    )\n                  )\n                  (drop_path): DropPath(drop_prob=0.078)\n                )\n              )\n              (channel_block): ChannelBlock(\n                (conv1): PreNorm(\n                  (fn): DepthWiseConv2d(\n                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n                  )\n                )\n                (channel_attn): PreNorm(\n                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n                  (fn): ChannelAttention(\n                    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                    (proj): Linear(in_features=1024, out_features=1024, bias=True)\n                  )\n                  (drop_path): DropPath(drop_prob=0.083)\n                )\n                (conv2): PreNorm(\n                  (fn): DepthWiseConv2d(\n                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n                  )\n                )\n                (ffn): PreNorm(\n                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n                  (fn): Mlp(\n                    (net): Sequential(\n                      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                      (act): GELU(approximate='none')\n                      (fc2): lora.Linear(\n                        (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=4096, out_features=8, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=8, out_features=1024, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                        (lora_magnitude_vector): ModuleDict()\n                      )\n                    )\n                  )\n                  (drop_path): DropPath(drop_prob=0.083)\n                )\n              )\n            )\n            (8): MySequential(\n              (spatial_block): SpatialBlock(\n                (conv1): PreNorm(\n                  (fn): DepthWiseConv2d(\n                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n                  )\n                )\n                (window_attn): PreNorm(\n                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n                  (fn): WindowAttention(\n                    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                    (proj): Linear(in_features=1024, out_features=1024, bias=True)\n                    (softmax): Softmax(dim=-1)\n                  )\n                  (drop_path): DropPath(drop_prob=0.087)\n                )\n                (conv2): PreNorm(\n                  (fn): DepthWiseConv2d(\n                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n                  )\n                )\n                (ffn): PreNorm(\n                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n                  (fn): Mlp(\n                    (net): Sequential(\n                      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                      (act): GELU(approximate='none')\n                      (fc2): lora.Linear(\n                        (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=4096, out_features=8, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=8, out_features=1024, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                        (lora_magnitude_vector): ModuleDict()\n                      )\n                    )\n                  )\n                  (drop_path): DropPath(drop_prob=0.087)\n                )\n              )\n              (channel_block): ChannelBlock(\n                (conv1): PreNorm(\n                  (fn): DepthWiseConv2d(\n                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n                  )\n                )\n                (channel_attn): PreNorm(\n                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n                  (fn): ChannelAttention(\n                    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n                    (proj): Linear(in_features=1024, out_features=1024, bias=True)\n                  )\n                  (drop_path): DropPath(drop_prob=0.091)\n                )\n                (conv2): PreNorm(\n                  (fn): DepthWiseConv2d(\n                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n                  )\n                )\n                (ffn): PreNorm(\n                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n                  (fn): Mlp(\n                    (net): Sequential(\n                      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                      (act): GELU(approximate='none')\n                      (fc2): lora.Linear(\n                        (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=4096, out_features=8, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=8, out_features=1024, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                        (lora_magnitude_vector): ModuleDict()\n                      )\n                    )\n                  )\n                  (drop_path): DropPath(drop_prob=0.091)\n                )\n              )\n            )\n          )\n          (3): MySequential(\n            (0): MySequential(\n              (spatial_block): SpatialBlock(\n                (conv1): PreNorm(\n                  (fn): DepthWiseConv2d(\n                    (dw): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n                  )\n                )\n                (window_attn): PreNorm(\n                  (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n                  (fn): WindowAttention(\n                    (qkv): Linear(in_features=2048, out_features=6144, bias=True)\n                    (proj): Linear(in_features=2048, out_features=2048, bias=True)\n                    (softmax): Softmax(dim=-1)\n                  )\n                  (drop_path): DropPath(drop_prob=0.096)\n                )\n                (conv2): PreNorm(\n                  (fn): DepthWiseConv2d(\n                    (dw): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n                  )\n                )\n                (ffn): PreNorm(\n                  (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n                  (fn): Mlp(\n                    (net): Sequential(\n                      (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n                      (act): GELU(approximate='none')\n                      (fc2): lora.Linear(\n                        (base_layer): Linear(in_features=8192, out_features=2048, bias=True)\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=8192, out_features=8, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=8, out_features=2048, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                        (lora_magnitude_vector): ModuleDict()\n                      )\n                    )\n                  )\n                  (drop_path): DropPath(drop_prob=0.096)\n                )\n              )\n              (channel_block): ChannelBlock(\n                (conv1): PreNorm(\n                  (fn): DepthWiseConv2d(\n                    (dw): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n                  )\n                )\n                (channel_attn): PreNorm(\n                  (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n                  (fn): ChannelAttention(\n                    (qkv): Linear(in_features=2048, out_features=6144, bias=True)\n                    (proj): Linear(in_features=2048, out_features=2048, bias=True)\n                  )\n                  (drop_path): DropPath(drop_prob=0.100)\n                )\n                (conv2): PreNorm(\n                  (fn): DepthWiseConv2d(\n                    (dw): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n                  )\n                )\n                (ffn): PreNorm(\n                  (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n                  (fn): Mlp(\n                    (net): Sequential(\n                      (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n                      (act): GELU(approximate='none')\n                      (fc2): lora.Linear(\n                        (base_layer): Linear(in_features=8192, out_features=2048, bias=True)\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=8192, out_features=8, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=8, out_features=2048, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                        (lora_magnitude_vector): ModuleDict()\n                      )\n                    )\n                  )\n                  (drop_path): DropPath(drop_prob=0.100)\n                )\n              )\n            )\n          )\n        )\n        (avgpool): AdaptiveAvgPool1d(output_size=1)\n      )\n      (image_proj_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      (image_pos_embed): LearnedAbsolutePositionEmbedding2D(\n        (row_embeddings): Embedding(50, 1024)\n        (column_embeddings): Embedding(50, 1024)\n      )\n      (visual_temporal_embed): PositionalEmbeddingCosine1D()\n      (language_model): Florence2LanguageForConditionalGeneration(\n        (model): Florence2LanguageModel(\n          (shared): Embedding(51289, 1024, padding_idx=1)\n          (encoder): Florence2Encoder(\n            (embed_tokens): Florence2ScaledWordEmbedding(51289, 1024, padding_idx=1)\n            (embed_positions): Florence2LearnedPositionalEmbedding(1026, 1024)\n            (layers): ModuleList(\n              (0-11): 12 x Florence2EncoderLayer(\n                (self_attn): Florence2SdpaAttention(\n                  (k_proj): lora.Linear(\n                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (v_proj): lora.Linear(\n                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (q_proj): lora.Linear(\n                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n                )\n                (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n                (activation_fn): GELUActivation()\n                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                (fc2): lora.Linear(\n                  (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=4096, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=1024, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              )\n            )\n            (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (decoder): Florence2Decoder(\n            (embed_tokens): Florence2ScaledWordEmbedding(51289, 1024, padding_idx=1)\n            (embed_positions): Florence2LearnedPositionalEmbedding(1026, 1024)\n            (layers): ModuleList(\n              (0-11): 12 x Florence2DecoderLayer(\n                (self_attn): Florence2SdpaAttention(\n                  (k_proj): lora.Linear(\n                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (v_proj): lora.Linear(\n                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (q_proj): lora.Linear(\n                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n                )\n                (activation_fn): GELUActivation()\n                (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n                (encoder_attn): Florence2SdpaAttention(\n                  (k_proj): lora.Linear(\n                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (v_proj): lora.Linear(\n                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (q_proj): lora.Linear(\n                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n                )\n                (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                (fc2): lora.Linear(\n                  (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=4096, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=1024, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              )\n            )\n            (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n        (lm_head): lora.Linear(\n          (base_layer): Linear(in_features=1024, out_features=51289, bias=False)\n          (lora_dropout): ModuleDict(\n            (default): Dropout(p=0.05, inplace=False)\n          )\n          (lora_A): ModuleDict(\n            (default): Linear(in_features=1024, out_features=8, bias=False)\n          )\n          (lora_B): ModuleDict(\n            (default): Linear(in_features=8, out_features=51289, bias=False)\n          )\n          (lora_embedding_A): ParameterDict()\n          (lora_embedding_B): ParameterDict()\n          (lora_magnitude_vector): ModuleDict()\n        )\n      )\n    )\n  )\n)"
  },
  {
    "objectID": "posts/mean_average_precision.html",
    "href": "posts/mean_average_precision.html",
    "title": "Mean Average Precision",
    "section": "",
    "text": "https://youtu.be/FppOzcDvaDI?si=Dwv7FEAoaS9CLlGH\nIt is a graph between precision and recall for different confidence score with a paricular IOU. The precision recall curve will be different for different IOUs.\nThe area under the Precision-Recall curve is the average precision. Higher the area under curve, higher the average precision.\nAverage Precision(AP) is calculated for each class.\nMean Average Precision is calculated by taking average of the AP of two classes.\nSuppose we have two classes : Dogs and Cats\nNow, we will calculate the Precision-Recall curve for dog class. Suppose there are two images of dog in which\nthe number of ground truth boxes are : 4 the number of predicted boxes are : 2\nFor first image :\nthe number of ground truth boxes are : 1 the number of predicted boxes are : 2\nFor Second image :\nthe number of ground truth boxes are : 3 the number of predicted boxes are : 2\nSo we will start from first image with its first predicted bounding box.\nWe will check:\n\nis the predicted box has IOU greater than threshold –&gt; If yes then the predicted box is true positive\nIf No then predicted box is false positive.\n\nSuppose the first predicted box has IOU greater than threshold then\nPrecision till now : 1 divided by total predicted boxes till now which is equal to 1 , so precision is 1\nRecall till now: 1 divided by total number of ground truth boxes which is equal to 4, so recall is 0.25\nNow come to second predicted box of first image.\nSuppose the second predicted box has IOU less than threshold then\nPrecision till now : 1 divided by total predicted boxes till now which is equal to 2 , so precision is 0.5\nRecall till now: 1 divided by total number of ground truth boxes which is equal to 4, so recall is 0.25\nNext, we will go to second image.\nSuppose the first predicted box has IOU greater than threshold then\nPrecision till now : 2 divided by total predicted boxes till now which is equal to 3 , so precision is 0.66 (Why true positives are 2 because we have total correctly predicted boxes till now equal to 2)\nRecall till now: 2 divided by total number of ground truth boxes which is equal to 4, so recall is 0.5\nNow come to second predicted box of second image.\nSuppose the second predicted box has IOU less than threshold then\nPrecision till now : 2 divided by total predicted boxes till now which is equal to 4 , so precision is 0.5\nRecall till now: 2 divided by total number of ground truth boxes which is equal to 4, so recall is 0.5\nNow for plotting the P-R curve, we need all precision values: 1, 0.5, 0.66, 0.5 and all recall values: 0.25, 0.25, 0.5, 0.5\nThis will plot the curve for a particular IOU threshold. Now calculate the area under the curve, which will give the average precision at a particular threshold for Dog class.\nSimilarly, we can do for Cat class and can get the average precision.\nTo get the ean average precision, we take the average of the Average precision of the two classes."
  },
  {
    "objectID": "posts/transformers.html",
    "href": "posts/transformers.html",
    "title": "Transformers",
    "section": "",
    "text": "Here I have implemented the transformer architecture from scratch with a focus on forward pass. This will help understand how the inputs flow through each module. Note that backpropagation code is not implemented.\n\nimport torch\nimport torch.nn as nn\n\n\n\n\n\n\nHere we are building multi head attention according to transformer paper \n\nclass SelfAttention(nn.Module):\n    def __init__(self, embed_size, heads): # why are we dividing the embed_size among heads\n        super(SelfAttention, self).__init__()\n        self.embed_size = embed_size\n        self.heads = heads\n        self.head_dim = embed_size // heads\n\n        self.W_k = nn.Linear(self.embed_size, self.embed_size) # every time the input to the query, key and value matrix will be a vector of dimension embed_size and it will brought down to head_dim\n        self.W_q = nn.Linear(self.embed_size, self.embed_size)\n        self.W_v = nn.Linear(self.embed_size, self.embed_size)\n\n        self.fc = nn.Linear(self.heads*self.head_dim, self.embed_size)\n\n        # The query, key and value have tokens and each token is of size embed_size.\n        # Suppose, there are 100 tokens in a query, each token is 512-dimensional.\n        # The embed_size is divided among heads. Suppose the number of heads is 8.\n        # The 512-dimensional token is divided among heads. Each head has 64-dimensional query\n\n        # One query has 100 tokens\n        # One token is 512-dimensional. Embed size is 512-dimensional.\n        # Each head has 64-dimensional token.\n        # (N, 100, 512) is the input taken by Linear layer W_q and then the output is reshaped into (N, 100, 8, 64)\n\n    def forward(self, query, key, value, mask):\n        N = query.shape[0]\n        query_len, key_len, value_len = query.shape[1], key.shape[1], value.shape[1] # here we find out how many tokens are present in query, key and value\n        \n        query = self.W_q(query)\n        key = self.W_k(key)\n        value = self.W_v(value)\n\n        # reshape the query, key and value such that each head has query dimension, key dimension and value dimension equal to head_dimension.\n        query = query.reshape(N, query_len, self.heads, self.head_dim)\n        key = key.reshape(N, key_len, self.heads, self.head_dim)\n        value = value.reshape(N, value_len, self.heads, self.head_dim)\n\n        # calculate attention scores QK.T \n        # Remember: While using einsum, you only need to specify the ranks of input matrices and the output matrix, the internal computation is handled accordingly.\n        attention_scores = torch.einsum('nqhd, nkhd-&gt; nhqk', [query, key])\n\n        # the input sentences are of same length even if some sentences have fewer words.\n        # Suppose Sentence 1 has 5 words and Sentence 2 has 11 words. \n        # Padding is done in sentence 1 with 6 tokens so that sentence 1 and sentence 2 has same length\n        # In reality, the padded 6 tokens in sentence 1 are meaningless so we require a mask to tell which tokens are real and which are not.\n        # Hence for sentence 1 the mask will be [1,1,1,1,1,0,0,0,0,0,0]\n        # For sentence 2, the mask will be [1,1,1,1,1,1,1,1,1,1,1]\n        # Final mask = [[1,1,1,1,1,0,0,0,0,0,0],\n        #               [1,1,1,1,1,1,1,1,1,1,1]]\n        # As the non-real tokens add no value to sentence, computing attention scores on these non-real tokens is of no use.\n        # Hence we replace the attention scores calculated from these non-real tokens to large negative values.\n        # On applying softmax to these large negative values, the final attention weight will be zero. \n        # This satisfies our goal to not have any attention from non-real tokens.\n\n        if mask is not None:\n            attention_scores = attention_scores.masked_fill(mask == 0, -1e-20)\n\n\n        # attention weight = softmax(QK.T/sqrt(dk)) here dk is the dimension of the query which is equal to head dim \n        attention_weights = torch.softmax((attention_scores)/(self.head_dim ** 0.5), dim = 3)  \n        # dimension of attention_values : (N, heads, query_len, key_len)\n\n        # multiply attention values with value vector then reshaping to concatenate all 64-dimensional value vectors of all heads into 512-d value vector\n        out = torch.einsum('nhqk, nkhd -&gt; nqhd', [attention_weights, value]).reshape(N, query_len, self.heads*self.head_dim)\n\n        # pass the 512-dimensional value vector through the linear layer\n        out = self.fc(out)\n\n        return out\n\nNow, lets build the transformer block\n\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, embed_size, heads, dropout):\n        super(TransformerBlock, self).__init__()\n        self.attention = SelfAttention(embed_size, heads)\n        self.norm1 = nn.LayerNorm(embed_size)\n        self.norm2 = nn.LayerNorm(embed_size)\n\n        self.fc = nn.Sequential(\n            nn.Linear(embed_size, embed_size), # here forward expansion is also added in some tutorials\n            nn.ReLU(),\n            nn.Linear(embed_size, embed_size)\n        )\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, query, key, value, mask):\n        attention = self.attention(query, key, value, mask)\n        fc_in =  self.dropout(self.norm1(attention + query))\n        fc_out = self.fc(fc_in)\n        out = self.norm2(fc_out + fc_in)\n\n        return out\n\nAfter building the transformer block, we build the encoder block that contains transformer block and the positional encodings and input embeddings.\n\n\nclass Encoder(nn.Module):\n    def __init__(self, num_embeddings, max_len, embed_size, heads, num_layers, dropout):\n        super(Encoder, self).__init__()\n        self.embedding_layer = nn.Embedding(num_embeddings, embed_size)\n        self.positional_encoding = nn.Embedding(max_len, embed_size)\n        self.transformer_layers = nn.ModuleList(\n            [TransformerBlock(embed_size, heads, dropout) for _ in range(num_layers)]\n        )\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, input, mask):\n        N, seq_len = input.shape\n        input_position = torch.arange(0, seq_len).expand(N, seq_len) # for every input, we store the position of the input words \n                                                                     # in the input sentence\n        \n        input_embedding = self.embedding_layer(input)\n        positional_embedding = self.positional_encoding(input_position)\n\n        combined_input = input_embedding + positional_embedding\n        out = self.dropout(combined_input)\n\n        for transformerBlock in self.transformer_layers:\n            out = transformerBlock(out, out, out, mask)\n        \n        return out\n\nEmbedding layer takes two important parameters: - the vocab size - the dimension of each token in the vocabulary.\n How embedding layer encodes vector representation of the tokens. \nSuppose the vocabulary includes 5 words [‘I’, ‘am’, ‘go’ , ‘to’, ‘school’]\nand we want the vector representation of each word of 10 dimension. The Embedding Layer is initialized as follows:\nembedding_layer = nn.Embedding(num_embeddings = 5, embedding_dim = 10)\nNow, if we want the vector representation of the sentence ‘I go to school’, we represent the sentence with token indices [1,3,4,5]\nembedding_layer([1,3,4,5]) will give the 10 dimensional vector representation of each of the tokens in the sentence ‘I go to school’.\n How embedding layer encodes vector representation of the position of the tokens in the sentence.\nIn the above sentence ‘I go to school’, token ‘I’ appears at position 0 in the sentence, token ‘go’ appears at position 1 in the sentence.\nInitally, the position of the tokens is [0,1,2,3].\nThe embedding layer takes [0,1,2,3] as input and then the weights learnt tell at what position token ‘I’ should appear in any new sentence.\nThe model learns that certain words tend to appear in certain positions, not because we tell it where they should appear, but because it observes this from training data using the combination of token + positional embeddings.\n\n\nLets build the decoder block\n\n\nclass DecoderBlock(nn.Module):\n    def __init__(self, embed_size, heads, dropout):\n        super(DecoderBlock, self).__init__()\n        self.attention = SelfAttention(embed_size, heads)\n        self.transformer_block = TransformerBlock(embed_size, heads, dropout)\n        self.norm = nn.LayerNorm(embed_size)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, input, key, value, source_mask, target_mask):\n        x = self.attention(input, input, input, target_mask) \n        query = self.dropout(self.norm(x + input))\n        out = self.transformer_block(query, key, value, source_mask) # Here the query is coming from the decoder attention module\n\n        return out\n\n\n\nclass Decoder(nn.Module):\n    def __init__(self, target_vocab_size, max_len, embed_size, heads,  num_layers, dropout):\n        super(Decoder, self).__init__()\n        self.embedding_layer = nn.Embedding(num_embeddings=target_vocab_size, embedding_dim= embed_size)\n        self.positional_embedding = nn.Embedding(num_embeddings= max_len, embedding_dim= embed_size)\n        self.decoder_layers = nn.ModuleList([\n            DecoderBlock(embed_size, heads, dropout) for _ in range(num_layers)\n        ])\n        self.fc = nn.Linear(embed_size, target_vocab_size)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, input, enc_out, source_mask, target_mask):\n        N, seq_len = input.shape\n        positions = torch.arange(0, seq_len).expand((N, seq_len))\n\n        x = self.dropout(self.embedding_layer(input) + self.positional_embedding(positions))\n\n        for decoder_layer in self.decoder_layers:\n            x = decoder_layer(x, enc_out, enc_out, source_mask, target_mask) \n\n        out = self.fc(x)  # why no softmax operation is done after Linear layer?\n\n        return out\n\n\n\n\n\nIntegrate all the blocks to get the transformer\n\n\n\nMasking is very important in case of Self Attention in decoder. It ensures that future tokens are not seen.\nDecoder generates the tokens one at a timestep (autoregressive token generation).\nDuring training, the target sentence is available at all timesteps. Here masking ensures tokens of timestep &gt; t are not seen at timestep t.\nDuring infernce, masking is not required because tokens of timestep t+1 are not poduced by the time token at timestep t is generated.\nSuppose our task is language translation from English to Hindi.\nSource sentence: I am going to school Target sentence: Mai school jaa raha hu.\nToken in target sentence: [‘Mai’, ‘school’,‘jaa’, ‘raha’, ‘hu’]\nAll the token in the target sentence is the input to the decoder but it will be right shifted.\nDecoder input: [‘’, ‘Mai’, ‘school’,‘jaa’, ‘raha’] Token indices: [0, 5, 1, 2, 3]\nQuery, Key and Value are computed from the decoder input\nAttention scores are computed between Query and the Key\nattn_score = torch.randn((5,5))\n\nattn_score\n\ntensor([[1., 0., 0., 0., 0.],\n        [1., 1., 0., 0., 0.],\n        [1., 1., 1., 0., 0.],\n        [1., 1., 1., 1., 0.],\n        [1., 1., 1., 1., 1.]])\n\nmask = torch.tril(torch.ones((5,5)))\n\nmask\n\ntensor([[1., 0., 0., 0., 0.],\n        [1., 1., 0., 0., 0.],\n        [1., 1., 1., 0., 0.],\n        [1., 1., 1., 1., 0.],\n        [1., 1., 1., 1., 1.]])\n\nattn_score = attn_score.masked_fill(mask == 0, -1e20)\n\nattn_weight = torch.tril(attn_score) #Here softmax is applied\n\nattn_weight \n\ntensor([[-0.3627,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.3311, -0.5411,  0.0000,  0.0000,  0.0000],\n        [-0.6262, -0.9632, -0.4826,  0.0000,  0.0000],\n        [ 1.2154, -0.2964,  0.2686,  0.8283,  0.0000],\n        [-0.7835,  0.8071,  0.9509,  0.3160,  0.4949]])\nEach row in attn_weight represents one timestep.\n\nclass Transformer(nn.Module):\n    def __init__(self, src_pad_idx, source_vocab_size, target_vocab_size, max_len, embed_size, heads, num_layers, dropout):\n        super(Transformer, self).__init__()\n        self.src_pad_idx = src_pad_idx\n\n        self.encoder = Encoder(source_vocab_size, max_len, embed_size, heads, num_layers, dropout)\n        self.decoder = Decoder(target_vocab_size, max_len, embed_size, heads,  num_layers, dropout)\n\n    def create_source_mask(self, source_input):\n        mask = (source_input != self.src_pad_idx)\n        mask = mask.unsqueeze(1).unsqueeze(2) # (num_of_sentences, num_of_attention_heads, num_of tokens_in_query, num_of_tokens_in_key)\n\n    def create_target_mask(self, target_input):\n        N, seq_len = target_input.shape\n        mask = torch.tril(torch.ones((N, 1, seq_len, seq_len))) # (num_of_sentences, num_of_attention_heads, num_of_token_in_target_sentence, num_of_token_in_target_sentence)\n        return mask\n\n    def forward(self, source_input, target_input):\n        source_mask = self.create_source_mask(source_input)\n        target_mask = self.create_target_mask(target_input)\n        enc_out = self.encoder(source_input, source_mask)\n        out = self.decoder(target_input, enc_out, source_mask, target_mask)\n    \n        return out\n\n\nsrc_pad_idx = 0\nsource_vocab_size = 8\ntarget_vocab_size = 8\nmax_len = 10\nembed_size = 512\nheads = 8\nnum_layers = 6\ndropout = 0.2\n\nsource_input = torch.tensor([[1,2,3,4,5,6,7,2,0,0],\n                             [2,4,5,6,7,1,5,3,4,0]])\n\ntarget_input = torch.tensor([[1,2,3,4,5,6,7,1,0,0],\n                             [2,4,5,6,7,1,2,3,4,0]])\n\nmodel = Transformer(src_pad_idx, source_vocab_size, target_vocab_size, max_len, embed_size, heads, num_layers, dropout)\nout = model(source_input, target_input[:, :-1])\n\n\n\nout.shape\n\ntorch.Size([2, 9, 8])\n\n\n\nout\n\ntensor([[[-2.4522e-01,  7.5161e-01,  1.5044e-01, -4.4457e-01, -3.6331e-01,\n          -1.3185e-01, -4.4139e-01,  1.1206e+00],\n         [-2.5892e-01,  1.8479e-01, -2.6550e-01,  1.3232e-02,  1.1513e+00,\n           6.2616e-01, -4.4417e-01, -4.3183e-02],\n         [-1.2515e-01, -5.5137e-01, -2.0334e-01, -7.0015e-01,  2.9892e-01,\n          -6.1758e-01, -3.9991e-01, -4.0937e-01],\n         [-9.3089e-01,  5.5678e-02,  4.7316e-01,  2.9314e-01, -3.2622e-01,\n           2.1052e-01, -5.2436e-02,  1.0685e+00],\n         [ 4.6724e-01, -1.6304e-01, -8.9153e-01,  1.1432e-03, -2.7380e-02,\n           7.6934e-01, -2.4154e-01,  6.7970e-01],\n         [-1.0680e-01,  7.6220e-01,  2.3358e-01, -8.6498e-01,  3.1073e-01,\n          -6.9051e-01, -2.1195e-01, -4.0020e-02],\n         [ 1.1692e-01, -8.2362e-01, -5.6247e-01, -1.5735e+00, -8.2848e-01,\n          -6.7880e-02,  5.4737e-02,  3.4298e-01],\n         [-7.3295e-01,  2.4218e-01,  5.4857e-01, -5.2408e-01,  1.7051e-01,\n          -5.0285e-01,  3.9243e-02,  1.5502e-01],\n         [ 3.7826e-01,  2.6755e-02,  5.0969e-01,  9.6464e-01, -1.7561e-01,\n          -4.5683e-03,  1.6245e-02, -2.0068e-01]],\n\n        [[-4.4446e-01,  2.6719e-01, -7.4989e-02,  5.0081e-02,  4.7049e-01,\n          -1.9041e+00,  5.1257e-01,  3.2910e-01],\n         [-1.5257e-01, -4.6883e-01,  8.6818e-01,  2.2030e-02,  2.9119e-01,\n           9.4907e-01,  5.7688e-01, -1.4505e-01],\n         [-6.3640e-01, -7.2573e-01, -6.0644e-01, -2.1226e-01,  1.5071e-02,\n           4.0294e-01,  3.7317e-01, -7.1153e-01],\n         [-1.6581e-02,  1.0416e+00,  9.1748e-01,  1.4845e-01,  1.3690e+00,\n           3.0138e-01,  3.2478e-01,  2.8285e-01],\n         [ 8.3124e-01,  2.7311e-01,  1.6623e-01, -4.6480e-01,  7.8047e-02,\n          -4.7173e-01,  2.1744e-01,  1.8619e-01],\n         [-1.4566e-01,  3.1397e-01, -1.5519e-01, -1.1649e+00,  5.8320e-01,\n          -6.6345e-01, -3.9948e-02, -9.0548e-01],\n         [ 1.7868e-01,  4.2507e-01,  2.1227e-01, -2.4670e-02,  1.2108e+00,\n           1.0988e-02,  1.4864e-01,  2.1023e-01],\n         [ 4.6141e-01,  4.7899e-01,  8.3111e-01, -6.7718e-01, -1.2969e-01,\n          -6.5497e-01,  1.2411e+00, -8.8316e-01],\n         [-7.8651e-01,  4.2094e-01,  2.6151e-02, -3.9575e-03,  6.4821e-01,\n           2.4018e-01, -1.1315e-01,  1.1400e-01]]], grad_fn=&lt;ViewBackward0&gt;)\n\n\n\n\nEinsum usage\n\nA = torch.arange(1, 17)\nB = torch.arange(100,116)\nA = A.reshape((4,4))\nB = B.reshape((4,4))\ntorch.einsum('ij,kj-&gt;ik', A,B)\n\ntensor([[1080, 1090, 1100, 1110],\n        [2776, 2802, 2828, 2854],\n        [4472, 4514, 4556, 4598],\n        [6168, 6226, 6284, 6342]])\n\n\n\ntorch.einsum('ij,jk', A,B)\n\ntensor([[1080, 1090, 1100, 1110],\n        [2776, 2802, 2828, 2854],\n        [4472, 4514, 4556, 4598],\n        [6168, 6226, 6284, 6342]])\n\n\n\ntorch.einsum('ij,jk-&gt;ik', A,B)\n\ntensor([[1080, 1090, 1100, 1110],\n        [2776, 2802, 2828, 2854],\n        [4472, 4514, 4556, 4598],\n        [6168, 6226, 6284, 6342]])\n\n\n\nA@B\n\ntensor([[1080, 1090, 1100, 1110],\n        [2776, 2802, 2828, 2854],\n        [4472, 4514, 4556, 4598],\n        [6168, 6226, 6284, 6342]])\n\n\n\ntorch.einsum('ij,kj-&gt;ikj', A, B)\n\ntensor([[[ 100,  202,  306,  412],\n         [ 104,  210,  318,  428],\n         [ 108,  218,  330,  444],\n         [ 112,  226,  342,  460]],\n\n        [[ 500,  606,  714,  824],\n         [ 520,  630,  742,  856],\n         [ 540,  654,  770,  888],\n         [ 560,  678,  798,  920]],\n\n        [[ 900, 1010, 1122, 1236],\n         [ 936, 1050, 1166, 1284],\n         [ 972, 1090, 1210, 1332],\n         [1008, 1130, 1254, 1380]],\n\n        [[1300, 1414, 1530, 1648],\n         [1352, 1470, 1590, 1712],\n         [1404, 1526, 1650, 1776],\n         [1456, 1582, 1710, 1840]]])\n\n\n\ntorch.einsum('ij,kl-&gt;ijkl', A, B)\n\ntensor([[[[ 100,  101,  102,  103],\n          [ 104,  105,  106,  107],\n          [ 108,  109,  110,  111],\n          [ 112,  113,  114,  115]],\n\n         [[ 200,  202,  204,  206],\n          [ 208,  210,  212,  214],\n          [ 216,  218,  220,  222],\n          [ 224,  226,  228,  230]],\n\n         [[ 300,  303,  306,  309],\n          [ 312,  315,  318,  321],\n          [ 324,  327,  330,  333],\n          [ 336,  339,  342,  345]],\n\n         [[ 400,  404,  408,  412],\n          [ 416,  420,  424,  428],\n          [ 432,  436,  440,  444],\n          [ 448,  452,  456,  460]]],\n\n\n        [[[ 500,  505,  510,  515],\n          [ 520,  525,  530,  535],\n          [ 540,  545,  550,  555],\n          [ 560,  565,  570,  575]],\n\n         [[ 600,  606,  612,  618],\n          [ 624,  630,  636,  642],\n          [ 648,  654,  660,  666],\n          [ 672,  678,  684,  690]],\n\n         [[ 700,  707,  714,  721],\n          [ 728,  735,  742,  749],\n          [ 756,  763,  770,  777],\n          [ 784,  791,  798,  805]],\n\n         [[ 800,  808,  816,  824],\n          [ 832,  840,  848,  856],\n          [ 864,  872,  880,  888],\n          [ 896,  904,  912,  920]]],\n\n\n        [[[ 900,  909,  918,  927],\n          [ 936,  945,  954,  963],\n          [ 972,  981,  990,  999],\n          [1008, 1017, 1026, 1035]],\n\n         [[1000, 1010, 1020, 1030],\n          [1040, 1050, 1060, 1070],\n          [1080, 1090, 1100, 1110],\n          [1120, 1130, 1140, 1150]],\n\n         [[1100, 1111, 1122, 1133],\n          [1144, 1155, 1166, 1177],\n          [1188, 1199, 1210, 1221],\n          [1232, 1243, 1254, 1265]],\n\n         [[1200, 1212, 1224, 1236],\n          [1248, 1260, 1272, 1284],\n          [1296, 1308, 1320, 1332],\n          [1344, 1356, 1368, 1380]]],\n\n\n        [[[1300, 1313, 1326, 1339],\n          [1352, 1365, 1378, 1391],\n          [1404, 1417, 1430, 1443],\n          [1456, 1469, 1482, 1495]],\n\n         [[1400, 1414, 1428, 1442],\n          [1456, 1470, 1484, 1498],\n          [1512, 1526, 1540, 1554],\n          [1568, 1582, 1596, 1610]],\n\n         [[1500, 1515, 1530, 1545],\n          [1560, 1575, 1590, 1605],\n          [1620, 1635, 1650, 1665],\n          [1680, 1695, 1710, 1725]],\n\n         [[1600, 1616, 1632, 1648],\n          [1664, 1680, 1696, 1712],\n          [1728, 1744, 1760, 1776],\n          [1792, 1808, 1824, 1840]]]])"
  },
  {
    "objectID": "posts/non_max_suppression.html",
    "href": "posts/non_max_suppression.html",
    "title": "Non max suppression",
    "section": "",
    "text": "Remove all the bounding boxes whose confidence is less than confidence threshld\nFor a particular category, find the bounding box with highest confidence score\nCheck its IOU with the other bounding boxes of same category\nIf the IOU &gt; IOU threshold, this means that the box is bounding the same object in such case, we remove or suppress these boxes."
  },
  {
    "objectID": "posts/precision_recall.html",
    "href": "posts/precision_recall.html",
    "title": "Precision and Recall",
    "section": "",
    "text": "When a model has high recall but low precision, then the model classifies most of the positive samples correctly but it has many false positives (i.e. classifies many Negative samples as Positive). When a model has high precision but low recall, then the model is accurate when it classifies a sample as Positive but it may classify only some of the positive samples.\nNote that as the recall increases, the precision decreases. The reason is that when the number of positive samples increases (high recall), the accuracy of classifying each sample correctly decreases (low precision). This is expected, as the model is more likely to fail when there are many samples.\n\nimport numpy as np\n\ny_true = [\"positive\", \"negative\", \"negative\", \"positive\", \"positive\", \"positive\", \"negative\", \"positive\", \"negative\", \"positive\", \"positive\", \"positive\", \"positive\", \"negative\", \"negative\", \"negative\"]\n\npred_scores = [0.7, 0.3, 0.5, 0.6, 0.55, 0.9, 0.4, 0.2, 0.4, 0.3, 0.7, 0.5, 0.8, 0.2, 0.3, 0.35]\n\nthresholds = np.arange(start=0.2, stop=0.7, step=0.05)\n\n\nimport sklearn.metrics\n\ndef precision_recall_curve(y_true, pred_scores, thresholds):\n    precisions = []\n    recalls = []\n    \n    for threshold in thresholds:\n        y_pred = [\"positive\" if score &gt;= threshold else \"negative\" for score in pred_scores]\n\n        precision = sklearn.metrics.precision_score(y_true=y_true, y_pred=y_pred, pos_label=\"positive\")\n        recall = sklearn.metrics.recall_score(y_true=y_true, y_pred=y_pred, pos_label=\"positive\")\n        \n        precisions.append(precision)\n        recalls.append(recall)\n\n    return precisions, recalls\n\n\n# when threshold(0.2) is low, all predicted samples become positive and it will definetly include actual positives \n# so recall will be high but precision will be low because total predicted positives are more than actual positives\ny_pred = [\"positive\" if score &gt;= 0.2 else \"negative\" for score in pred_scores]\nprecision = sklearn.metrics.precision_score(y_true=y_true, y_pred=y_pred, pos_label=\"positive\")\nrecall = sklearn.metrics.recall_score(y_true=y_true, y_pred=y_pred, pos_label=\"positive\")\nprint(f'Precision {precision}, Recall {recall}')\n\nPrecision 0.5625, Recall 1.0\n\n\n\n# when threshold(0.9) is high, positive samples will be actual positive samples so they become true positive which makes precision high\n# that is the model is more than 0.9 sure that the sample is positive, so it will be an actual positive but still you are not covering all \n# positives because of keeping high threshold\ny_pred = [\"positive\" if score &gt;= 0.9 else \"negative\" for score in pred_scores]\nprecision = sklearn.metrics.precision_score(y_true=y_true, y_pred=y_pred, pos_label=\"positive\")\nrecall = sklearn.metrics.recall_score(y_true=y_true, y_pred=y_pred, pos_label=\"positive\")\nprint(f'Precision {precision}, Recall {recall}')\n\nPrecision 1.0, Recall 0.1111111111111111\n\n\n\nprecisions, recalls = precision_recall_curve(y_true, pred_scores, thresholds)\nprint(f'Precision list {precisions}')\nprint(f'Recall list {recalls}')\n\nPrecision list [0.5625, 0.5714285714285714, 0.5714285714285714, 0.6363636363636364, 0.7, 0.875, 0.875, 1.0, 1.0, 1.0]\nRecall list [1.0, 0.8888888888888888, 0.8888888888888888, 0.7777777777777778, 0.7777777777777778, 0.7777777777777778, 0.7777777777777778, 0.6666666666666666, 0.5555555555555556, 0.4444444444444444]\n\n\n\nimport matplotlib.pyplot as plt\nplt.plot(recalls, precisions, color=\"red\")\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.title(\"Precision-Recall Curve obtained by varying score threshold \\n score threshold is increasing from right to left\")\nplt.show()\n\n\n\n\n\n\n\n\nSimilarly, if we vary IOU threshold then we will get another precision recall curve\n\ndef compute_ap(recall, precision):\n    #from ultralytics\n    # Append sentinel values to beginning and end\n    mrec = np.concatenate(([0.0], recall, [1.0]))\n    mpre = np.concatenate(([1.0], precision, [0.0]))\n    print(f'mpre {mpre}')\n    print(f'np.flip {np.flip(mpre)}')\n    print(f'np.accumulate.maximum {np.maximum.accumulate(np.flip(mpre))}')\n    # Compute the precision envelope\n    mpre = np.flip(np.maximum.accumulate(np.flip(mpre)))\n\n    # Integrate area under curve\n    method = \"interp\"  # methods: 'continuous', 'interp'\n    if method == \"interp\":\n        x = np.linspace(0, 1, 101)  # 101-point interp (COCO)\n        ap = np.trapz(np.interp(x, mrec, mpre), x)  # integrate\n    else:  # 'continuous'\n        i = np.where(mrec[1:] != mrec[:-1])[0]  # points where x-axis (recall) changes\n        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])  # area under curve\n\n    return ap, mpre, mrec\n\n\ndef smooth(y, f=0.05):\n    \"\"\"Box filter of fraction f.\"\"\"\n    nf = round(len(y) * f * 2) // 2 + 1  # number of filter elements (must be odd)\n    p = np.ones(nf // 2)  # ones padding\n    yp = np.concatenate((p * y[0], y, p * y[-1]), 0)  # y padded\n    return np.convolve(yp, np.ones(nf) / nf, mode=\"valid\")  # y-smoothed\n\n\ndef ap_per_class(\n    tp, conf, pred_cls, target_cls, plot=False,eps=1e-16):\n    \n    # Sort by objectness\n    i = np.argsort(-conf)\n    tp, conf, pred_cls = tp[i], conf[i], pred_cls[i] # decreasing order of  confidences\n    tp = tp.reshape((tp.shape[0],1))\n    # Find unique classes\n    unique_classes, nt = np.unique(target_cls, return_counts=True)\n    nc = unique_classes.shape[0]  # number of classes, number of detections\n\n    # Create Precision-Recall curve and compute AP for each class\n    x, prec_values = np.linspace(0, 1, 1000), []\n\n    # Average precision, precision and recall curves\n    ap, p_curve, r_curve = np.zeros((nc, tp.shape[1])), np.zeros((nc, 1000)), np.zeros((nc, 1000))\n    for ci, c in enumerate(unique_classes):\n        i = pred_cls == c\n        n_l = nt[ci]  # number of labels or ground truth\n        n_p = i.sum()  # number of predictions\n        if n_p == 0 or n_l == 0:\n            continue\n\n        # Accumulate FPs and TPs\n        fpc = (1 - tp[i]).cumsum(0) \n        tpc = tp[i].cumsum(0)\n        \n        # Recall\n        recall = tpc / (n_l + eps)  # recall curve\n        print(f'recall {recall[:,0]}')\n        # print(f'-conf[i] {-conf[i]}')\n        r_curve[ci] = np.interp(-x, -conf[i], recall[:, 0], left=0)  # negative x, xp because xp decreases\n        # print(f'r_curve[ci] {r_curve[ci]}')\n        # Precision\n        precision = tpc / (tpc + fpc)  # precision curve\n        print(f'precision {precision[:,0]}')\n        # print(f'-conf[i] {-conf[i]}')\n        p_curve[ci] = np.interp(-x, -conf[i], precision[:, 0], left=1)  # p at pr_score\n        # print(f'p_curve[ci] {p_curve[ci]}')\n        # AP from recall-precision curve\n        for j in range(tp.shape[1]):\n            ap[ci, j], mpre, mrec = compute_ap(recall[:, j], precision[:, j])\n            print(f'AP {ap[ci,j]}')\n            if plot and j == 0:\n                prec_values.append(np.interp(x, mrec, mpre))  \n\n    prec_values = np.array(prec_values)  # (nc, 1000)\n\n    # Compute F1 (harmonic mean of precision and recall)\n    f1_curve = 2 * p_curve * r_curve / (p_curve + r_curve + eps)\n\n    i = smooth(f1_curve.mean(0), 0.1).argmax()  # max F1 index\n    p, r, f1 = p_curve[:, i], r_curve[:, i], f1_curve[:, i]  # max-F1 precision, recall, F1 values\n    tp = (r * nt).round()  # true positives\n    fp = (tp / (p + eps) - tp).round()  # false positives\n    # return tp, fp, p, r, ap, f1, unique_classes.astype(int), p_curve, r_curve, f1_curve, x, prec_values\n    return 1\n\nTrue positives are decided using IOU\nclass 1 p = 1, r = 0.33 p = 0.5, r = 0.33 p = 0.66, r = 0.66\nclass 2 p = 1, r = 0.5 p = 0.5, r = 0.5\n\ntp = np.array([1,0,1,1,0])\nconf = np.array([1,1,1,1,1])\npred_cls = np.array(['1','1','1','2','2'])\ntarget_cls = np.array(['1','1','1','2','2'])\nap_per_class(tp, conf, pred_cls, target_cls, plot=False,eps=1e-16)\n\nrecall [0.33333333 0.33333333 0.66666667]\nprecision [1.         0.5        0.66666667]\nmpre [1.         1.         0.5        0.66666667 0.        ]\nnp.flip [0.         0.66666667 0.5        1.         1.        ]\nnp.accumulate.maximum [0.         0.66666667 0.66666667 1.         1.        ]\nAP 0.6672\nrecall [0.5 0.5]\nprecision [1.  0.5]\nmpre [1.  1.  0.5 0. ]\nnp.flip [0.  0.5 1.  1. ]\nnp.accumulate.maximum [0.  0.5 1.  1. ]\nAP 0.6224999999999999\n\n\n1"
  },
  {
    "objectID": "posts/SOTA_object_detection.html",
    "href": "posts/SOTA_object_detection.html",
    "title": "Evolution in object detection",
    "section": "",
    "text": "two stage detector –&gt; one stage detector –&gt; anchor free"
  }
]