{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37db1c16",
   "metadata": {},
   "source": [
    "---\n",
    "title: Vision Language Models\n",
    "jupyter: \"vlm\"\n",
    "description: \"Vision Language Models and Lora adapters with quantization\"\n",
    "author: \"Shataxi Dubey\"\n",
    "date: \"2025-05-23\"\n",
    "categories: [vlm, lora, quantization]\n",
    "format:\n",
    "    html:\n",
    "        toc: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bafa106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f865f327a414dfc9fe5bb64431f9401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "40448783184"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                # bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_type=torch.bfloat16,)\n",
    "\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-72B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\", quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "model.get_memory_footprint()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9ed4e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2_5_VLForConditionalGeneration(\n",
       "  (visual): Qwen2_5_VisionTransformerPretrainedModel(\n",
       "    (patch_embed): Qwen2_5_VisionPatchEmbed(\n",
       "      (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
       "    )\n",
       "    (rotary_pos_emb): Qwen2_5_VisionRotaryEmbedding()\n",
       "    (blocks): ModuleList(\n",
       "      (0-31): 32 x Qwen2_5_VLVisionBlock(\n",
       "        (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "        (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "        (attn): Qwen2_5_VLVisionSdpaAttention(\n",
       "          (qkv): Linear4bit(in_features=1280, out_features=3840, bias=True)\n",
       "          (proj): Linear4bit(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (mlp): Qwen2_5_VLMLP(\n",
       "          (gate_proj): Linear4bit(in_features=1280, out_features=3456, bias=True)\n",
       "          (up_proj): Linear4bit(in_features=1280, out_features=3456, bias=True)\n",
       "          (down_proj): Linear4bit(in_features=3456, out_features=1280, bias=True)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (merger): Qwen2_5_VLPatchMerger(\n",
       "      (ln_q): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear4bit(in_features=5120, out_features=8192, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (model): Qwen2_5_VLModel(\n",
       "    (embed_tokens): Embedding(152064, 8192)\n",
       "    (layers): ModuleList(\n",
       "      (0-79): 80 x Qwen2_5_VLDecoderLayer(\n",
       "        (self_attn): Qwen2_5_VLSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=8192, out_features=29568, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=8192, out_features=29568, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=29568, out_features=8192, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((8192,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((8192,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((8192,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=8192, out_features=152064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cfd71aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 108,904,448 || all params: 73,519,681,792 || trainable%: 0.1481\n",
      "40884400976\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen2_5_VLForConditionalGeneration(\n",
       "      (visual): Qwen2_5_VisionTransformerPretrainedModel(\n",
       "        (patch_embed): Qwen2_5_VisionPatchEmbed(\n",
       "          (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
       "        )\n",
       "        (rotary_pos_emb): Qwen2_5_VisionRotaryEmbedding()\n",
       "        (blocks): ModuleList(\n",
       "          (0-31): 32 x Qwen2_5_VLVisionBlock(\n",
       "            (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "            (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "            (attn): Qwen2_5_VLVisionSdpaAttention(\n",
       "              (qkv): Linear4bit(in_features=1280, out_features=3840, bias=True)\n",
       "              (proj): Linear4bit(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (mlp): Qwen2_5_VLMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=1280, out_features=3456, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3456, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=1280, out_features=3456, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3456, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3456, out_features=1280, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3456, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (merger): Qwen2_5_VLPatchMerger(\n",
       "          (ln_q): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "          (mlp): Sequential(\n",
       "            (0): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear4bit(in_features=5120, out_features=8192, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (model): Qwen2_5_VLModel(\n",
       "        (embed_tokens): Embedding(152064, 8192)\n",
       "        (layers): ModuleList(\n",
       "          (0-79): 80 x Qwen2_5_VLDecoderLayer(\n",
       "            (self_attn): Qwen2_5_VLSdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8192, out_features=8192, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8192, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8192, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8192, out_features=29568, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=29568, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8192, out_features=29568, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=29568, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=29568, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=29568, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((8192,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((8192,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen2RMSNorm((8192,), eps=1e-06)\n",
       "        (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=8192, out_features=152064, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"o_proj\", \"k_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "peft_model.print_trainable_parameters()\n",
    "print(peft_model.get_memory_footprint())\n",
    "peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb91a879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 16,384,000 || all params: 73,427,161,344 || trainable%: 0.0223\n",
      "40514319184\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen2_5_VLForConditionalGeneration(\n",
       "      (visual): Qwen2_5_VisionTransformerPretrainedModel(\n",
       "        (patch_embed): Qwen2_5_VisionPatchEmbed(\n",
       "          (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
       "        )\n",
       "        (rotary_pos_emb): Qwen2_5_VisionRotaryEmbedding()\n",
       "        (blocks): ModuleList(\n",
       "          (0-31): 32 x Qwen2_5_VLVisionBlock(\n",
       "            (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "            (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "            (attn): Qwen2_5_VLVisionSdpaAttention(\n",
       "              (qkv): Linear4bit(in_features=1280, out_features=3840, bias=True)\n",
       "              (proj): Linear4bit(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (mlp): Qwen2_5_VLMLP(\n",
       "              (gate_proj): Linear4bit(in_features=1280, out_features=3456, bias=True)\n",
       "              (up_proj): Linear4bit(in_features=1280, out_features=3456, bias=True)\n",
       "              (down_proj): Linear4bit(in_features=3456, out_features=1280, bias=True)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (merger): Qwen2_5_VLPatchMerger(\n",
       "          (ln_q): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "          (mlp): Sequential(\n",
       "            (0): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear4bit(in_features=5120, out_features=8192, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (model): Qwen2_5_VLModel(\n",
       "        (embed_tokens): Embedding(152064, 8192)\n",
       "        (layers): ModuleList(\n",
       "          (0-79): 80 x Qwen2_5_VLDecoderLayer(\n",
       "            (self_attn): Qwen2_5_VLSdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8192, out_features=8192, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=True)\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8192, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
       "              (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): Linear4bit(in_features=8192, out_features=29568, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=8192, out_features=29568, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=29568, out_features=8192, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((8192,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((8192,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen2RMSNorm((8192,), eps=1e-06)\n",
       "        (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=8192, out_features=152064, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\",],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "peft_model.print_trainable_parameters()\n",
    "print(peft_model.get_memory_footprint())\n",
    "peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e603134c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca0a3631c9dd454382e29f37daebe8a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "6066263008"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "from transformers import PaliGemmaForConditionalGeneration\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                # bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_type=torch.bfloat16,)\n",
    "\n",
    "model = PaliGemmaForConditionalGeneration.from_pretrained(\"google/paligemma2-3b-pt-448\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "model.get_memory_footprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44a95334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PaliGemmaForConditionalGeneration(\n",
       "  (vision_tower): SiglipVisionModel(\n",
       "    (vision_model): SiglipVisionTransformer(\n",
       "      (embeddings): SiglipVisionEmbeddings(\n",
       "        (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
       "        (position_embedding): Embedding(1024, 1152)\n",
       "      )\n",
       "      (encoder): SiglipEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-26): 27 x SiglipEncoderLayer(\n",
       "            (self_attn): SiglipSdpaAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): PytorchGELUTanh()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (multi_modal_projector): PaliGemmaMultiModalProjector(\n",
       "    (linear): Linear(in_features=1152, out_features=2304, bias=True)\n",
       "  )\n",
       "  (language_model): Gemma2ForCausalLM(\n",
       "    (model): Gemma2Model(\n",
       "      (embed_tokens): Embedding(257216, 2304, padding_idx=0)\n",
       "      (layers): ModuleList(\n",
       "        (0-25): 26 x Gemma2DecoderLayer(\n",
       "          (self_attn): Gemma2Attention(\n",
       "            (q_proj): Linear(in_features=2304, out_features=2048, bias=False)\n",
       "            (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n",
       "          )\n",
       "          (mlp): Gemma2MLP(\n",
       "            (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "            (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "            (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n",
       "            (act_fn): PytorchGELUTanh()\n",
       "          )\n",
       "          (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "          (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "          (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "          (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        )\n",
       "      )\n",
       "      (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      (rotary_emb): Gemma2RotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2304, out_features=257216, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37ea9a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 11,876,352 || all params: 3,045,003,504 || trainable%: 0.3900\n",
      "6113768416\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): PaliGemmaForConditionalGeneration(\n",
       "      (vision_tower): SiglipVisionModel(\n",
       "        (vision_model): SiglipVisionTransformer(\n",
       "          (embeddings): SiglipVisionEmbeddings(\n",
       "            (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
       "            (position_embedding): Embedding(1024, 1152)\n",
       "          )\n",
       "          (encoder): SiglipEncoder(\n",
       "            (layers): ModuleList(\n",
       "              (0-26): 27 x SiglipEncoderLayer(\n",
       "                (self_attn): SiglipSdpaAttention(\n",
       "                  (k_proj): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Identity()\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1152, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1152, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (v_proj): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Identity()\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1152, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1152, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (q_proj): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Identity()\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1152, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1152, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "                (mlp): SiglipMLP(\n",
       "                  (activation_fn): PytorchGELUTanh()\n",
       "                  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "                  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (multi_modal_projector): PaliGemmaMultiModalProjector(\n",
       "        (linear): Linear(in_features=1152, out_features=2304, bias=True)\n",
       "      )\n",
       "      (language_model): Gemma2ForCausalLM(\n",
       "        (model): Gemma2Model(\n",
       "          (embed_tokens): Embedding(257216, 2304, padding_idx=0)\n",
       "          (layers): ModuleList(\n",
       "            (0-25): 26 x Gemma2DecoderLayer(\n",
       "              (self_attn): Gemma2Attention(\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2304, out_features=2048, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2304, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2304, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2304, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2048, out_features=2304, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=2304, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "              (mlp): Gemma2MLP(\n",
       "                (gate_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2304, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=9216, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (up_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2304, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=9216, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (down_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=9216, out_features=2304, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=9216, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=2304, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (act_fn): PytorchGELUTanh()\n",
       "              )\n",
       "              (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "              (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "              (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "              (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "            )\n",
       "          )\n",
       "          (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "          (rotary_emb): Gemma2RotaryEmbedding()\n",
       "        )\n",
       "        (lm_head): Linear(in_features=2304, out_features=257216, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import get_peft_model, LoraConfig\n",
    "import peft\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=8,\n",
    "        target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "peft_model.print_trainable_parameters()\n",
    "print(peft_model.get_memory_footprint())\n",
    "peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eccdb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Florence2LanguageForConditionalGeneration has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1645899954"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM \n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/Florence-2-large-ft\", torch_dtype=\"auto\", trust_remote_code=True)\n",
    "model.get_memory_footprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "307f69f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float16"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "814a8d4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Florence2ForConditionalGeneration(\n",
       "  (vision_tower): DaViT(\n",
       "    (convs): ModuleList(\n",
       "      (0): ConvEmbed(\n",
       "        (proj): Conv2d(3, 256, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n",
       "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): ConvEmbed(\n",
       "        (proj): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): ConvEmbed(\n",
       "        (proj): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): ConvEmbed(\n",
       "        (proj): Conv2d(1024, 2048, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0): MySequential(\n",
       "        (0): MySequential(\n",
       "          (spatial_block): SpatialBlock(\n",
       "            (conv1): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "              )\n",
       "            )\n",
       "            (window_attn): PreNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): WindowAttention(\n",
       "                (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "            )\n",
       "            (conv2): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "              )\n",
       "            )\n",
       "            (ffn): PreNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Mlp(\n",
       "                (net): Sequential(\n",
       "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "            )\n",
       "          )\n",
       "          (channel_block): ChannelBlock(\n",
       "            (conv1): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "              )\n",
       "            )\n",
       "            (channel_attn): PreNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): ChannelAttention(\n",
       "                (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.004)\n",
       "            )\n",
       "            (conv2): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "              )\n",
       "            )\n",
       "            (ffn): PreNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Mlp(\n",
       "                (net): Sequential(\n",
       "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.004)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): MySequential(\n",
       "        (0): MySequential(\n",
       "          (spatial_block): SpatialBlock(\n",
       "            (conv1): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "              )\n",
       "            )\n",
       "            (window_attn): PreNorm(\n",
       "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.009)\n",
       "            )\n",
       "            (conv2): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "              )\n",
       "            )\n",
       "            (ffn): PreNorm(\n",
       "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Mlp(\n",
       "                (net): Sequential(\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.009)\n",
       "            )\n",
       "          )\n",
       "          (channel_block): ChannelBlock(\n",
       "            (conv1): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "              )\n",
       "            )\n",
       "            (channel_attn): PreNorm(\n",
       "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): ChannelAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.013)\n",
       "            )\n",
       "            (conv2): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "              )\n",
       "            )\n",
       "            (ffn): PreNorm(\n",
       "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Mlp(\n",
       "                (net): Sequential(\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.013)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): MySequential(\n",
       "        (0): MySequential(\n",
       "          (spatial_block): SpatialBlock(\n",
       "            (conv1): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "              )\n",
       "            )\n",
       "            (window_attn): PreNorm(\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): WindowAttention(\n",
       "                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.017)\n",
       "            )\n",
       "            (conv2): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "              )\n",
       "            )\n",
       "            (ffn): PreNorm(\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Mlp(\n",
       "                (net): Sequential(\n",
       "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.017)\n",
       "            )\n",
       "          )\n",
       "          (channel_block): ChannelBlock(\n",
       "            (conv1): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "              )\n",
       "            )\n",
       "            (channel_attn): PreNorm(\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): ChannelAttention(\n",
       "                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.022)\n",
       "            )\n",
       "            (conv2): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "              )\n",
       "            )\n",
       "            (ffn): PreNorm(\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Mlp(\n",
       "                (net): Sequential(\n",
       "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.022)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): MySequential(\n",
       "          (spatial_block): SpatialBlock(\n",
       "            (conv1): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "              )\n",
       "            )\n",
       "            (window_attn): PreNorm(\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): WindowAttention(\n",
       "                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.026)\n",
       "            )\n",
       "            (conv2): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "              )\n",
       "            )\n",
       "            (ffn): PreNorm(\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Mlp(\n",
       "                (net): Sequential(\n",
       "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.026)\n",
       "            )\n",
       "          )\n",
       "          (channel_block): ChannelBlock(\n",
       "            (conv1): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "              )\n",
       "            )\n",
       "            (channel_attn): PreNorm(\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): ChannelAttention(\n",
       "                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.030)\n",
       "            )\n",
       "            (conv2): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "              )\n",
       "            )\n",
       "            (ffn): PreNorm(\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Mlp(\n",
       "                (net): Sequential(\n",
       "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.030)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): MySequential(\n",
       "          (spatial_block): SpatialBlock(\n",
       "            (conv1): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "              )\n",
       "            )\n",
       "            (window_attn): PreNorm(\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): WindowAttention(\n",
       "                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.035)\n",
       "            )\n",
       "            (conv2): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "              )\n",
       "            )\n",
       "            (ffn): PreNorm(\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Mlp(\n",
       "                (net): Sequential(\n",
       "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.035)\n",
       "            )\n",
       "          )\n",
       "          (channel_block): ChannelBlock(\n",
       "            (conv1): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "              )\n",
       "            )\n",
       "            (channel_attn): PreNorm(\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): ChannelAttention(\n",
       "                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.039)\n",
       "            )\n",
       "            (conv2): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "              )\n",
       "            )\n",
       "            (ffn): PreNorm(\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Mlp(\n",
       "                (net): Sequential(\n",
       "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.039)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): MySequential(\n",
       "          (spatial_block): SpatialBlock(\n",
       "            (conv1): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "              )\n",
       "            )\n",
       "            (window_attn): PreNorm(\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): WindowAttention(\n",
       "                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.043)\n",
       "            )\n",
       "            (conv2): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "              )\n",
       "            )\n",
       "            (ffn): PreNorm(\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Mlp(\n",
       "                (net): Sequential(\n",
       "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.043)\n",
       "            )\n",
       "          )\n",
       "          (channel_block): ChannelBlock(\n",
       "            (conv1): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "              )\n",
       "            )\n",
       "            (channel_attn): PreNorm(\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): ChannelAttention(\n",
       "                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.048)\n",
       "            )\n",
       "            (conv2): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "              )\n",
       "            )\n",
       "            (ffn): PreNorm(\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Mlp(\n",
       "                (net): Sequential(\n",
       "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.048)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): MySequential(\n",
       "          (spatial_block): SpatialBlock(\n",
       "            (conv1): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "              )\n",
       "            )\n",
       "            (window_attn): PreNorm(\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): WindowAttention(\n",
       "                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.052)\n",
       "            )\n",
       "            (conv2): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "              )\n",
       "            )\n",
       "            (ffn): PreNorm(\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Mlp(\n",
       "                (net): Sequential(\n",
       "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.052)\n",
       "            )\n",
       "          )\n",
       "          (channel_block): ChannelBlock(\n",
       "            (conv1): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "              )\n",
       "            )\n",
       "            (channel_attn): PreNorm(\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): ChannelAttention(\n",
       "                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.057)\n",
       "            )\n",
       "            (conv2): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "              )\n",
       "            )\n",
       "            (ffn): PreNorm(\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Mlp(\n",
       "                (net): Sequential(\n",
       "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.057)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): MySequential(\n",
       "          (spatial_block): SpatialBlock(\n",
       "            (conv1): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "              )\n",
       "            )\n",
       "            (window_attn): PreNorm(\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): WindowAttention(\n",
       "                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.061)\n",
       "            )\n",
       "            (conv2): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "              )\n",
       "            )\n",
       "            (ffn): PreNorm(\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Mlp(\n",
       "                (net): Sequential(\n",
       "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.061)\n",
       "            )\n",
       "          )\n",
       "          (channel_block): ChannelBlock(\n",
       "            (conv1): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "              )\n",
       "            )\n",
       "            (channel_attn): PreNorm(\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): ChannelAttention(\n",
       "                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.065)\n",
       "            )\n",
       "            (conv2): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "              )\n",
       "            )\n",
       "            (ffn): PreNorm(\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Mlp(\n",
       "                (net): Sequential(\n",
       "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.065)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): MySequential(\n",
       "          (spatial_block): SpatialBlock(\n",
       "            (conv1): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "              )\n",
       "            )\n",
       "            (window_attn): PreNorm(\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): WindowAttention(\n",
       "                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.070)\n",
       "            )\n",
       "            (conv2): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "              )\n",
       "            )\n",
       "            (ffn): PreNorm(\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Mlp(\n",
       "                (net): Sequential(\n",
       "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.070)\n",
       "            )\n",
       "          )\n",
       "          (channel_block): ChannelBlock(\n",
       "            (conv1): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "              )\n",
       "            )\n",
       "            (channel_attn): PreNorm(\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): ChannelAttention(\n",
       "                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.074)\n",
       "            )\n",
       "            (conv2): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "              )\n",
       "            )\n",
       "            (ffn): PreNorm(\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Mlp(\n",
       "                (net): Sequential(\n",
       "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.074)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): MySequential(\n",
       "          (spatial_block): SpatialBlock(\n",
       "            (conv1): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "              )\n",
       "            )\n",
       "            (window_attn): PreNorm(\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): WindowAttention(\n",
       "                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.078)\n",
       "            )\n",
       "            (conv2): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "              )\n",
       "            )\n",
       "            (ffn): PreNorm(\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Mlp(\n",
       "                (net): Sequential(\n",
       "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.078)\n",
       "            )\n",
       "          )\n",
       "          (channel_block): ChannelBlock(\n",
       "            (conv1): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "              )\n",
       "            )\n",
       "            (channel_attn): PreNorm(\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): ChannelAttention(\n",
       "                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.083)\n",
       "            )\n",
       "            (conv2): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "              )\n",
       "            )\n",
       "            (ffn): PreNorm(\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Mlp(\n",
       "                (net): Sequential(\n",
       "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.083)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (8): MySequential(\n",
       "          (spatial_block): SpatialBlock(\n",
       "            (conv1): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "              )\n",
       "            )\n",
       "            (window_attn): PreNorm(\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): WindowAttention(\n",
       "                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.087)\n",
       "            )\n",
       "            (conv2): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "              )\n",
       "            )\n",
       "            (ffn): PreNorm(\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Mlp(\n",
       "                (net): Sequential(\n",
       "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.087)\n",
       "            )\n",
       "          )\n",
       "          (channel_block): ChannelBlock(\n",
       "            (conv1): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "              )\n",
       "            )\n",
       "            (channel_attn): PreNorm(\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): ChannelAttention(\n",
       "                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.091)\n",
       "            )\n",
       "            (conv2): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "              )\n",
       "            )\n",
       "            (ffn): PreNorm(\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Mlp(\n",
       "                (net): Sequential(\n",
       "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.091)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): MySequential(\n",
       "        (0): MySequential(\n",
       "          (spatial_block): SpatialBlock(\n",
       "            (conv1): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
       "              )\n",
       "            )\n",
       "            (window_attn): PreNorm(\n",
       "              (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): WindowAttention(\n",
       "                (qkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "                (proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.096)\n",
       "            )\n",
       "            (conv2): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
       "              )\n",
       "            )\n",
       "            (ffn): PreNorm(\n",
       "              (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Mlp(\n",
       "                (net): Sequential(\n",
       "                  (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.096)\n",
       "            )\n",
       "          )\n",
       "          (channel_block): ChannelBlock(\n",
       "            (conv1): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
       "              )\n",
       "            )\n",
       "            (channel_attn): PreNorm(\n",
       "              (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): ChannelAttention(\n",
       "                (qkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "                (proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.100)\n",
       "            )\n",
       "            (conv2): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
       "              )\n",
       "            )\n",
       "            (ffn): PreNorm(\n",
       "              (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Mlp(\n",
       "                (net): Sequential(\n",
       "                  (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.100)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       "  )\n",
       "  (image_proj_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (image_pos_embed): LearnedAbsolutePositionEmbedding2D(\n",
       "    (row_embeddings): Embedding(50, 1024)\n",
       "    (column_embeddings): Embedding(50, 1024)\n",
       "  )\n",
       "  (visual_temporal_embed): PositionalEmbeddingCosine1D()\n",
       "  (language_model): Florence2LanguageForConditionalGeneration(\n",
       "    (model): Florence2LanguageModel(\n",
       "      (shared): Embedding(51289, 1024, padding_idx=1)\n",
       "      (encoder): Florence2Encoder(\n",
       "        (embed_tokens): Florence2ScaledWordEmbedding(51289, 1024, padding_idx=1)\n",
       "        (embed_positions): Florence2LearnedPositionalEmbedding(1026, 1024)\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x Florence2EncoderLayer(\n",
       "            (self_attn): Florence2SdpaAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Florence2Decoder(\n",
       "        (embed_tokens): Florence2ScaledWordEmbedding(51289, 1024, padding_idx=1)\n",
       "        (embed_positions): Florence2LearnedPositionalEmbedding(1026, 1024)\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x Florence2DecoderLayer(\n",
       "            (self_attn): Florence2SdpaAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): Florence2SdpaAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (lm_head): Linear(in_features=1024, out_features=51289, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "290e02a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,133,576 || all params: 826,827,464 || trainable%: 0.4999\n",
      "1662434258\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Florence2ForConditionalGeneration(\n",
       "      (vision_tower): DaViT(\n",
       "        (convs): ModuleList(\n",
       "          (0): ConvEmbed(\n",
       "            (proj): Conv2d(3, 256, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n",
       "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): ConvEmbed(\n",
       "            (proj): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): ConvEmbed(\n",
       "            (proj): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): ConvEmbed(\n",
       "            (proj): Conv2d(1024, 2048, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (blocks): ModuleList(\n",
       "          (0): MySequential(\n",
       "            (0): MySequential(\n",
       "              (spatial_block): SpatialBlock(\n",
       "                (conv1): PreNorm(\n",
       "                  (fn): DepthWiseConv2d(\n",
       "                    (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "                  )\n",
       "                )\n",
       "                (window_attn): PreNorm(\n",
       "                  (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): WindowAttention(\n",
       "                    (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                    (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                    (softmax): Softmax(dim=-1)\n",
       "                  )\n",
       "                  (drop_path): Identity()\n",
       "                )\n",
       "                (conv2): PreNorm(\n",
       "                  (fn): DepthWiseConv2d(\n",
       "                    (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "                  )\n",
       "                )\n",
       "                (ffn): PreNorm(\n",
       "                  (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Mlp(\n",
       "                    (net): Sequential(\n",
       "                      (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                      (act): GELU(approximate='none')\n",
       "                      (fc2): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Dropout(p=0.05, inplace=False)\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): Identity()\n",
       "                )\n",
       "              )\n",
       "              (channel_block): ChannelBlock(\n",
       "                (conv1): PreNorm(\n",
       "                  (fn): DepthWiseConv2d(\n",
       "                    (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "                  )\n",
       "                )\n",
       "                (channel_attn): PreNorm(\n",
       "                  (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): ChannelAttention(\n",
       "                    (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                    (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  )\n",
       "                  (drop_path): DropPath(drop_prob=0.004)\n",
       "                )\n",
       "                (conv2): PreNorm(\n",
       "                  (fn): DepthWiseConv2d(\n",
       "                    (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "                  )\n",
       "                )\n",
       "                (ffn): PreNorm(\n",
       "                  (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Mlp(\n",
       "                    (net): Sequential(\n",
       "                      (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                      (act): GELU(approximate='none')\n",
       "                      (fc2): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Dropout(p=0.05, inplace=False)\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): DropPath(drop_prob=0.004)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): MySequential(\n",
       "            (0): MySequential(\n",
       "              (spatial_block): SpatialBlock(\n",
       "                (conv1): PreNorm(\n",
       "                  (fn): DepthWiseConv2d(\n",
       "                    (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "                  )\n",
       "                )\n",
       "                (window_attn): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): WindowAttention(\n",
       "                    (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                    (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (softmax): Softmax(dim=-1)\n",
       "                  )\n",
       "                  (drop_path): DropPath(drop_prob=0.009)\n",
       "                )\n",
       "                (conv2): PreNorm(\n",
       "                  (fn): DepthWiseConv2d(\n",
       "                    (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "                  )\n",
       "                )\n",
       "                (ffn): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Mlp(\n",
       "                    (net): Sequential(\n",
       "                      (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU(approximate='none')\n",
       "                      (fc2): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Dropout(p=0.05, inplace=False)\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): DropPath(drop_prob=0.009)\n",
       "                )\n",
       "              )\n",
       "              (channel_block): ChannelBlock(\n",
       "                (conv1): PreNorm(\n",
       "                  (fn): DepthWiseConv2d(\n",
       "                    (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "                  )\n",
       "                )\n",
       "                (channel_attn): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): ChannelAttention(\n",
       "                    (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                    (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  )\n",
       "                  (drop_path): DropPath(drop_prob=0.013)\n",
       "                )\n",
       "                (conv2): PreNorm(\n",
       "                  (fn): DepthWiseConv2d(\n",
       "                    (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "                  )\n",
       "                )\n",
       "                (ffn): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Mlp(\n",
       "                    (net): Sequential(\n",
       "                      (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU(approximate='none')\n",
       "                      (fc2): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Dropout(p=0.05, inplace=False)\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): DropPath(drop_prob=0.013)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): MySequential(\n",
       "            (0): MySequential(\n",
       "              (spatial_block): SpatialBlock(\n",
       "                (conv1): PreNorm(\n",
       "                  (fn): DepthWiseConv2d(\n",
       "                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                  )\n",
       "                )\n",
       "                (window_attn): PreNorm(\n",
       "                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): WindowAttention(\n",
       "                    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                    (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                    (softmax): Softmax(dim=-1)\n",
       "                  )\n",
       "                  (drop_path): DropPath(drop_prob=0.017)\n",
       "                )\n",
       "                (conv2): PreNorm(\n",
       "                  (fn): DepthWiseConv2d(\n",
       "                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                  )\n",
       "                )\n",
       "                (ffn): PreNorm(\n",
       "                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Mlp(\n",
       "                    (net): Sequential(\n",
       "                      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                      (act): GELU(approximate='none')\n",
       "                      (fc2): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Dropout(p=0.05, inplace=False)\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): DropPath(drop_prob=0.017)\n",
       "                )\n",
       "              )\n",
       "              (channel_block): ChannelBlock(\n",
       "                (conv1): PreNorm(\n",
       "                  (fn): DepthWiseConv2d(\n",
       "                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                  )\n",
       "                )\n",
       "                (channel_attn): PreNorm(\n",
       "                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): ChannelAttention(\n",
       "                    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                    (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  )\n",
       "                  (drop_path): DropPath(drop_prob=0.022)\n",
       "                )\n",
       "                (conv2): PreNorm(\n",
       "                  (fn): DepthWiseConv2d(\n",
       "                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                  )\n",
       "                )\n",
       "                (ffn): PreNorm(\n",
       "                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Mlp(\n",
       "                    (net): Sequential(\n",
       "                      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                      (act): GELU(approximate='none')\n",
       "                      (fc2): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Dropout(p=0.05, inplace=False)\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): DropPath(drop_prob=0.022)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): MySequential(\n",
       "              (spatial_block): SpatialBlock(\n",
       "                (conv1): PreNorm(\n",
       "                  (fn): DepthWiseConv2d(\n",
       "                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                  )\n",
       "                )\n",
       "                (window_attn): PreNorm(\n",
       "                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): WindowAttention(\n",
       "                    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                    (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                    (softmax): Softmax(dim=-1)\n",
       "                  )\n",
       "                  (drop_path): DropPath(drop_prob=0.026)\n",
       "                )\n",
       "                (conv2): PreNorm(\n",
       "                  (fn): DepthWiseConv2d(\n",
       "                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                  )\n",
       "                )\n",
       "                (ffn): PreNorm(\n",
       "                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Mlp(\n",
       "                    (net): Sequential(\n",
       "                      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                      (act): GELU(approximate='none')\n",
       "                      (fc2): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Dropout(p=0.05, inplace=False)\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): DropPath(drop_prob=0.026)\n",
       "                )\n",
       "              )\n",
       "              (channel_block): ChannelBlock(\n",
       "                (conv1): PreNorm(\n",
       "                  (fn): DepthWiseConv2d(\n",
       "                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                  )\n",
       "                )\n",
       "                (channel_attn): PreNorm(\n",
       "                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): ChannelAttention(\n",
       "                    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                    (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  )\n",
       "                  (drop_path): DropPath(drop_prob=0.030)\n",
       "                )\n",
       "                (conv2): PreNorm(\n",
       "                  (fn): DepthWiseConv2d(\n",
       "                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                  )\n",
       "                )\n",
       "                (ffn): PreNorm(\n",
       "                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Mlp(\n",
       "                    (net): Sequential(\n",
       "                      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                      (act): GELU(approximate='none')\n",
       "                      (fc2): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Dropout(p=0.05, inplace=False)\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): DropPath(drop_prob=0.030)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): MySequential(\n",
       "              (spatial_block): SpatialBlock(\n",
       "                (conv1): PreNorm(\n",
       "                  (fn): DepthWiseConv2d(\n",
       "                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                  )\n",
       "                )\n",
       "                (window_attn): PreNorm(\n",
       "                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): WindowAttention(\n",
       "                    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                    (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                    (softmax): Softmax(dim=-1)\n",
       "                  )\n",
       "                  (drop_path): DropPath(drop_prob=0.035)\n",
       "                )\n",
       "                (conv2): PreNorm(\n",
       "                  (fn): DepthWiseConv2d(\n",
       "                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                  )\n",
       "                )\n",
       "                (ffn): PreNorm(\n",
       "                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Mlp(\n",
       "                    (net): Sequential(\n",
       "                      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                      (act): GELU(approximate='none')\n",
       "                      (fc2): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Dropout(p=0.05, inplace=False)\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): DropPath(drop_prob=0.035)\n",
       "                )\n",
       "              )\n",
       "              (channel_block): ChannelBlock(\n",
       "                (conv1): PreNorm(\n",
       "                  (fn): DepthWiseConv2d(\n",
       "                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                  )\n",
       "                )\n",
       "                (channel_attn): PreNorm(\n",
       "                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): ChannelAttention(\n",
       "                    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                    (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  )\n",
       "                  (drop_path): DropPath(drop_prob=0.039)\n",
       "                )\n",
       "                (conv2): PreNorm(\n",
       "                  (fn): DepthWiseConv2d(\n",
       "                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                  )\n",
       "                )\n",
       "                (ffn): PreNorm(\n",
       "                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Mlp(\n",
       "                    (net): Sequential(\n",
       "                      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                      (act): GELU(approximate='none')\n",
       "                      (fc2): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Dropout(p=0.05, inplace=False)\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): DropPath(drop_prob=0.039)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (3): MySequential(\n",
       "              (spatial_block): SpatialBlock(\n",
       "                (conv1): PreNorm(\n",
       "                  (fn): DepthWiseConv2d(\n",
       "                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                  )\n",
       "                )\n",
       "                (window_attn): PreNorm(\n",
       "                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): WindowAttention(\n",
       "                    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                    (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                    (softmax): Softmax(dim=-1)\n",
       "                  )\n",
       "                  (drop_path): DropPath(drop_prob=0.043)\n",
       "                )\n",
       "                (conv2): PreNorm(\n",
       "                  (fn): DepthWiseConv2d(\n",
       "                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                  )\n",
       "                )\n",
       "                (ffn): PreNorm(\n",
       "                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Mlp(\n",
       "                    (net): Sequential(\n",
       "                      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                      (act): GELU(approximate='none')\n",
       "                      (fc2): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Dropout(p=0.05, inplace=False)\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): DropPath(drop_prob=0.043)\n",
       "                )\n",
       "              )\n",
       "              (channel_block): ChannelBlock(\n",
       "                (conv1): PreNorm(\n",
       "                  (fn): DepthWiseConv2d(\n",
       "                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                  )\n",
       "                )\n",
       "                (channel_attn): PreNorm(\n",
       "                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): ChannelAttention(\n",
       "                    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                    (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  )\n",
       "                  (drop_path): DropPath(drop_prob=0.048)\n",
       "                )\n",
       "                (conv2): PreNorm(\n",
       "                  (fn): DepthWiseConv2d(\n",
       "                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                  )\n",
       "                )\n",
       "                (ffn): PreNorm(\n",
       "                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Mlp(\n",
       "                    (net): Sequential(\n",
       "                      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                      (act): GELU(approximate='none')\n",
       "                      (fc2): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Dropout(p=0.05, inplace=False)\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): DropPath(drop_prob=0.048)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (4): MySequential(\n",
       "              (spatial_block): SpatialBlock(\n",
       "                (conv1): PreNorm(\n",
       "                  (fn): DepthWiseConv2d(\n",
       "                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                  )\n",
       "                )\n",
       "                (window_attn): PreNorm(\n",
       "                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): WindowAttention(\n",
       "                    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                    (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                    (softmax): Softmax(dim=-1)\n",
       "                  )\n",
       "                  (drop_path): DropPath(drop_prob=0.052)\n",
       "                )\n",
       "                (conv2): PreNorm(\n",
       "                  (fn): DepthWiseConv2d(\n",
       "                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                  )\n",
       "                )\n",
       "                (ffn): PreNorm(\n",
       "                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Mlp(\n",
       "                    (net): Sequential(\n",
       "                      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                      (act): GELU(approximate='none')\n",
       "                      (fc2): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Dropout(p=0.05, inplace=False)\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): DropPath(drop_prob=0.052)\n",
       "                )\n",
       "              )\n",
       "              (channel_block): ChannelBlock(\n",
       "                (conv1): PreNorm(\n",
       "                  (fn): DepthWiseConv2d(\n",
       "                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                  )\n",
       "                )\n",
       "                (channel_attn): PreNorm(\n",
       "                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): ChannelAttention(\n",
       "                    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                    (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  )\n",
       "                  (drop_path): DropPath(drop_prob=0.057)\n",
       "                )\n",
       "                (conv2): PreNorm(\n",
       "                  (fn): DepthWiseConv2d(\n",
       "                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                  )\n",
       "                )\n",
       "                (ffn): PreNorm(\n",
       "                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Mlp(\n",
       "                    (net): Sequential(\n",
       "                      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                      (act): GELU(approximate='none')\n",
       "                      (fc2): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Dropout(p=0.05, inplace=False)\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): DropPath(drop_prob=0.057)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (5): MySequential(\n",
       "              (spatial_block): SpatialBlock(\n",
       "                (conv1): PreNorm(\n",
       "                  (fn): DepthWiseConv2d(\n",
       "                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                  )\n",
       "                )\n",
       "                (window_attn): PreNorm(\n",
       "                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): WindowAttention(\n",
       "                    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                    (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                    (softmax): Softmax(dim=-1)\n",
       "                  )\n",
       "                  (drop_path): DropPath(drop_prob=0.061)\n",
       "                )\n",
       "                (conv2): PreNorm(\n",
       "                  (fn): DepthWiseConv2d(\n",
       "                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                  )\n",
       "                )\n",
       "                (ffn): PreNorm(\n",
       "                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Mlp(\n",
       "                    (net): Sequential(\n",
       "                      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                      (act): GELU(approximate='none')\n",
       "                      (fc2): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Dropout(p=0.05, inplace=False)\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): DropPath(drop_prob=0.061)\n",
       "                )\n",
       "              )\n",
       "              (channel_block): ChannelBlock(\n",
       "                (conv1): PreNorm(\n",
       "                  (fn): DepthWiseConv2d(\n",
       "                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                  )\n",
       "                )\n",
       "                (channel_attn): PreNorm(\n",
       "                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): ChannelAttention(\n",
       "                    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                    (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  )\n",
       "                  (drop_path): DropPath(drop_prob=0.065)\n",
       "                )\n",
       "                (conv2): PreNorm(\n",
       "                  (fn): DepthWiseConv2d(\n",
       "                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                  )\n",
       "                )\n",
       "                (ffn): PreNorm(\n",
       "                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Mlp(\n",
       "                    (net): Sequential(\n",
       "                      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                      (act): GELU(approximate='none')\n",
       "                      (fc2): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Dropout(p=0.05, inplace=False)\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): DropPath(drop_prob=0.065)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (6): MySequential(\n",
       "              (spatial_block): SpatialBlock(\n",
       "                (conv1): PreNorm(\n",
       "                  (fn): DepthWiseConv2d(\n",
       "                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                  )\n",
       "                )\n",
       "                (window_attn): PreNorm(\n",
       "                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): WindowAttention(\n",
       "                    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                    (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                    (softmax): Softmax(dim=-1)\n",
       "                  )\n",
       "                  (drop_path): DropPath(drop_prob=0.070)\n",
       "                )\n",
       "                (conv2): PreNorm(\n",
       "                  (fn): DepthWiseConv2d(\n",
       "                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                  )\n",
       "                )\n",
       "                (ffn): PreNorm(\n",
       "                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Mlp(\n",
       "                    (net): Sequential(\n",
       "                      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                      (act): GELU(approximate='none')\n",
       "                      (fc2): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Dropout(p=0.05, inplace=False)\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): DropPath(drop_prob=0.070)\n",
       "                )\n",
       "              )\n",
       "              (channel_block): ChannelBlock(\n",
       "                (conv1): PreNorm(\n",
       "                  (fn): DepthWiseConv2d(\n",
       "                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                  )\n",
       "                )\n",
       "                (channel_attn): PreNorm(\n",
       "                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): ChannelAttention(\n",
       "                    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                    (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  )\n",
       "                  (drop_path): DropPath(drop_prob=0.074)\n",
       "                )\n",
       "                (conv2): PreNorm(\n",
       "                  (fn): DepthWiseConv2d(\n",
       "                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                  )\n",
       "                )\n",
       "                (ffn): PreNorm(\n",
       "                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Mlp(\n",
       "                    (net): Sequential(\n",
       "                      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                      (act): GELU(approximate='none')\n",
       "                      (fc2): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Dropout(p=0.05, inplace=False)\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): DropPath(drop_prob=0.074)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (7): MySequential(\n",
       "              (spatial_block): SpatialBlock(\n",
       "                (conv1): PreNorm(\n",
       "                  (fn): DepthWiseConv2d(\n",
       "                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                  )\n",
       "                )\n",
       "                (window_attn): PreNorm(\n",
       "                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): WindowAttention(\n",
       "                    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                    (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                    (softmax): Softmax(dim=-1)\n",
       "                  )\n",
       "                  (drop_path): DropPath(drop_prob=0.078)\n",
       "                )\n",
       "                (conv2): PreNorm(\n",
       "                  (fn): DepthWiseConv2d(\n",
       "                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                  )\n",
       "                )\n",
       "                (ffn): PreNorm(\n",
       "                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Mlp(\n",
       "                    (net): Sequential(\n",
       "                      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                      (act): GELU(approximate='none')\n",
       "                      (fc2): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Dropout(p=0.05, inplace=False)\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): DropPath(drop_prob=0.078)\n",
       "                )\n",
       "              )\n",
       "              (channel_block): ChannelBlock(\n",
       "                (conv1): PreNorm(\n",
       "                  (fn): DepthWiseConv2d(\n",
       "                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                  )\n",
       "                )\n",
       "                (channel_attn): PreNorm(\n",
       "                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): ChannelAttention(\n",
       "                    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                    (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  )\n",
       "                  (drop_path): DropPath(drop_prob=0.083)\n",
       "                )\n",
       "                (conv2): PreNorm(\n",
       "                  (fn): DepthWiseConv2d(\n",
       "                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                  )\n",
       "                )\n",
       "                (ffn): PreNorm(\n",
       "                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Mlp(\n",
       "                    (net): Sequential(\n",
       "                      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                      (act): GELU(approximate='none')\n",
       "                      (fc2): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Dropout(p=0.05, inplace=False)\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): DropPath(drop_prob=0.083)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (8): MySequential(\n",
       "              (spatial_block): SpatialBlock(\n",
       "                (conv1): PreNorm(\n",
       "                  (fn): DepthWiseConv2d(\n",
       "                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                  )\n",
       "                )\n",
       "                (window_attn): PreNorm(\n",
       "                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): WindowAttention(\n",
       "                    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                    (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                    (softmax): Softmax(dim=-1)\n",
       "                  )\n",
       "                  (drop_path): DropPath(drop_prob=0.087)\n",
       "                )\n",
       "                (conv2): PreNorm(\n",
       "                  (fn): DepthWiseConv2d(\n",
       "                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                  )\n",
       "                )\n",
       "                (ffn): PreNorm(\n",
       "                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Mlp(\n",
       "                    (net): Sequential(\n",
       "                      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                      (act): GELU(approximate='none')\n",
       "                      (fc2): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Dropout(p=0.05, inplace=False)\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): DropPath(drop_prob=0.087)\n",
       "                )\n",
       "              )\n",
       "              (channel_block): ChannelBlock(\n",
       "                (conv1): PreNorm(\n",
       "                  (fn): DepthWiseConv2d(\n",
       "                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                  )\n",
       "                )\n",
       "                (channel_attn): PreNorm(\n",
       "                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): ChannelAttention(\n",
       "                    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                    (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  )\n",
       "                  (drop_path): DropPath(drop_prob=0.091)\n",
       "                )\n",
       "                (conv2): PreNorm(\n",
       "                  (fn): DepthWiseConv2d(\n",
       "                    (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                  )\n",
       "                )\n",
       "                (ffn): PreNorm(\n",
       "                  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Mlp(\n",
       "                    (net): Sequential(\n",
       "                      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                      (act): GELU(approximate='none')\n",
       "                      (fc2): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Dropout(p=0.05, inplace=False)\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): DropPath(drop_prob=0.091)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): MySequential(\n",
       "            (0): MySequential(\n",
       "              (spatial_block): SpatialBlock(\n",
       "                (conv1): PreNorm(\n",
       "                  (fn): DepthWiseConv2d(\n",
       "                    (dw): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
       "                  )\n",
       "                )\n",
       "                (window_attn): PreNorm(\n",
       "                  (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): WindowAttention(\n",
       "                    (qkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "                    (proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "                    (softmax): Softmax(dim=-1)\n",
       "                  )\n",
       "                  (drop_path): DropPath(drop_prob=0.096)\n",
       "                )\n",
       "                (conv2): PreNorm(\n",
       "                  (fn): DepthWiseConv2d(\n",
       "                    (dw): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
       "                  )\n",
       "                )\n",
       "                (ffn): PreNorm(\n",
       "                  (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Mlp(\n",
       "                    (net): Sequential(\n",
       "                      (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "                      (act): GELU(approximate='none')\n",
       "                      (fc2): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Dropout(p=0.05, inplace=False)\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=8192, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): DropPath(drop_prob=0.096)\n",
       "                )\n",
       "              )\n",
       "              (channel_block): ChannelBlock(\n",
       "                (conv1): PreNorm(\n",
       "                  (fn): DepthWiseConv2d(\n",
       "                    (dw): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
       "                  )\n",
       "                )\n",
       "                (channel_attn): PreNorm(\n",
       "                  (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): ChannelAttention(\n",
       "                    (qkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "                    (proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "                  )\n",
       "                  (drop_path): DropPath(drop_prob=0.100)\n",
       "                )\n",
       "                (conv2): PreNorm(\n",
       "                  (fn): DepthWiseConv2d(\n",
       "                    (dw): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
       "                  )\n",
       "                )\n",
       "                (ffn): PreNorm(\n",
       "                  (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Mlp(\n",
       "                    (net): Sequential(\n",
       "                      (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "                      (act): GELU(approximate='none')\n",
       "                      (fc2): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Dropout(p=0.05, inplace=False)\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=8192, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): DropPath(drop_prob=0.100)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       "      )\n",
       "      (image_proj_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (image_pos_embed): LearnedAbsolutePositionEmbedding2D(\n",
       "        (row_embeddings): Embedding(50, 1024)\n",
       "        (column_embeddings): Embedding(50, 1024)\n",
       "      )\n",
       "      (visual_temporal_embed): PositionalEmbeddingCosine1D()\n",
       "      (language_model): Florence2LanguageForConditionalGeneration(\n",
       "        (model): Florence2LanguageModel(\n",
       "          (shared): Embedding(51289, 1024, padding_idx=1)\n",
       "          (encoder): Florence2Encoder(\n",
       "            (embed_tokens): Florence2ScaledWordEmbedding(51289, 1024, padding_idx=1)\n",
       "            (embed_positions): Florence2LearnedPositionalEmbedding(1026, 1024)\n",
       "            (layers): ModuleList(\n",
       "              (0-11): 12 x Florence2EncoderLayer(\n",
       "                (self_attn): Florence2SdpaAttention(\n",
       "                  (k_proj): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (v_proj): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (q_proj): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                )\n",
       "                (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (activation_fn): GELUActivation()\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (fc2): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (decoder): Florence2Decoder(\n",
       "            (embed_tokens): Florence2ScaledWordEmbedding(51289, 1024, padding_idx=1)\n",
       "            (embed_positions): Florence2LearnedPositionalEmbedding(1026, 1024)\n",
       "            (layers): ModuleList(\n",
       "              (0-11): 12 x Florence2DecoderLayer(\n",
       "                (self_attn): Florence2SdpaAttention(\n",
       "                  (k_proj): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (v_proj): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (q_proj): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                )\n",
       "                (activation_fn): GELUActivation()\n",
       "                (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (encoder_attn): Florence2SdpaAttention(\n",
       "                  (k_proj): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (v_proj): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (q_proj): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                )\n",
       "                (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (fc2): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (lm_head): lora.Linear(\n",
       "          (base_layer): Linear(in_features=1024, out_features=51289, bias=False)\n",
       "          (lora_dropout): ModuleDict(\n",
       "            (default): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (lora_A): ModuleDict(\n",
       "            (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "          )\n",
       "          (lora_B): ModuleDict(\n",
       "            (default): Linear(in_features=8, out_features=51289, bias=False)\n",
       "          )\n",
       "          (lora_embedding_A): ParameterDict()\n",
       "          (lora_embedding_B): ParameterDict()\n",
       "          (lora_magnitude_vector): ModuleDict()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "lora_config = LoraConfig(\n",
    "    r = 8,\n",
    "    lora_alpha = 8,\n",
    "    lora_dropout = 0.05,\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"linear\", \"Conv2d\", \"lm_head\", \"fc2\"],\n",
    "    task_type = \"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "peft_model.print_trainable_parameters()\n",
    "print(peft_model.get_memory_footprint())\n",
    "peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b46439",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shataxi_space",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
