{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00bc179e",
   "metadata": {},
   "source": [
    "---\n",
    "title: Long Short Term Memory\n",
    "jupyter: \"lstm\"\n",
    "description: \"A recurrent neural network with gates\"\n",
    "author: \"Shataxi Dubey\"\n",
    "date: \"2025-07-15\"\n",
    "categories: [lstm, gates]\n",
    "format:\n",
    "    html:\n",
    "        toc: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f6f2dc",
   "metadata": {},
   "source": [
    "Through this notebook, we can easily understand how input gate, output gate, forget gate work in LSTM and how forward propagation and backward propagation happen through various gates in a recurrent neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "870be971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52043131",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_state = torch.arange(start = 0, end = 5).reshape((1,5))\n",
    "hidden_state = torch.arange(start = 10, end = 15).reshape((1,5))\n",
    "input_gate = torch.arange(start = 0, end = 10).reshape((5,2))\n",
    "forget_gate = torch.arange(start = 10, end = 20).reshape((5,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8cd9c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_input = torch.concatenate([hidden_state, cell_state], axis = 1)\n",
    "concatenated_weights = torch.concatenate([input_gate, forget_gate], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c48017f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[420, 490]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated_input@concatenated_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6560d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[420, 490]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell_state@forget_gate + hidden_state@input_gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10d4d54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHotEncode(data, num_chars, chr_to_idx):\n",
    "    all_one_hots = []\n",
    "    for chr in data:\n",
    "        onehot = np.zeros((num_chars,1))\n",
    "        idx = chr_to_idx[chr]\n",
    "        onehot[idx] = 1\n",
    "        all_one_hots.append(onehot)\n",
    "    \n",
    "    return all_one_hots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c7d62b",
   "metadata": {},
   "source": [
    "#### Xavier initialisation\n",
    "\n",
    "\n",
    "Uniform xavier initialisation\n",
    "\n",
    "draw weight from a random uniform distribution [-x, x]\n",
    "\n",
    "x = sqrt(6/(input_size + output_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29160799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initWeights(input_size, output_size):  # input size = hidden size, output size = hidden size + num_of_unique_chars \n",
    "    x = np.sqrt(6 / (input_size + output_size))\n",
    "    w = np.random.uniform(low = -x, high = x, size = (input_size, output_size)) \n",
    "    return w\n",
    "\n",
    "\n",
    "def sigmoid(input):\n",
    "    input = np.clip(input, 1e-5, 1e5)\n",
    "    sig_out = 1 / (1 + np.exp(-input)) \n",
    "    return sig_out\n",
    "\n",
    "def tanh(input):\n",
    "    return np.tanh(input)\n",
    "\n",
    "def derivative_sigmoid(input):\n",
    "    return input*(1 - input)\n",
    "\n",
    "def derivative_tanh(input):\n",
    "    return 1 - (input**2)\n",
    "\n",
    "def softmax(input):\n",
    "    return np.exp(input)/ sum(np.exp(input))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2273b905",
   "metadata": {},
   "source": [
    "![](images/lstm_gates.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe5b9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM():\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate, num_epochs):\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # forget gate weights(it is multiplied with the cell state t-1)\n",
    "        self.wf = initWeights(hidden_size, input_size)\n",
    "        self.bf = initWeights(hidden_size, 1)\n",
    "\n",
    "        # input gate weights\n",
    "        self.wi = initWeights(hidden_size, input_size)\n",
    "        self.bi = initWeights(hidden_size, 1)\n",
    "\n",
    "        # output gate weights\n",
    "        self.wo = initWeights(hidden_size, input_size)\n",
    "        self.bo = initWeights(hidden_size, 1)\n",
    "\n",
    "        # candidate gate weights\n",
    "        self.wc = initWeights(hidden_size, input_size)\n",
    "        self.bc = initWeights(hidden_size, 1)\n",
    "\n",
    "        # final layer weights\n",
    "        self.wy = initWeights(output_size, hidden_size)\n",
    "        self.by = initWeights(output_size, 1)\n",
    "\n",
    "    \n",
    "    def reset(self):\n",
    "        self.hidden_state = {-1: np.zeros((self.hidden_size, 1))}\n",
    "        self.cell_state = {-1: np.zeros((self.hidden_size, 1))}\n",
    "        self.concatenated_input = {}\n",
    "        self.forget_gate = {}\n",
    "        self.input_gate = {}\n",
    "        self.output_gate = {}\n",
    "        self.intermediate_cell_state = {}\n",
    "\n",
    "    def forward(self, inputs): \n",
    "        self.reset()\n",
    "\n",
    "        output = []\n",
    "        for idx in range(len(inputs)):\n",
    "            self.concatenated_input[idx] = np.concatenate([self.hidden_state[idx - 1], inputs[idx]]) # shape of concatenated input: (hidden_size + char_size, 1)\n",
    "            self.forget_gate[idx] = sigmoid(self.wf@self.concatenated_input[idx] + self.bf) # shape of forget gate: (hidden_size, 1)\n",
    "            self.input_gate[idx] = sigmoid(self.wi@self.concatenated_input[idx] + self.bi) # shape of input gate: (hidden_size, 1)\n",
    "            self.output_gate[idx] = sigmoid(self.wo@self.concatenated_input[idx] + self.bo) # shape of output gate: (hidden_size, 1)\n",
    "            self.intermediate_cell_state[idx] = tanh(self.wc@self.concatenated_input[idx] + self.bc) # shape of intermediate cell state: (hidden_size, 1)\n",
    "            \n",
    "            self.cell_state[idx] = (np.multiply(self.intermediate_cell_state[idx], self.input_gate[idx]) + np.multiply(self.cell_state[idx - 1], self.forget_gate[idx])) # shape of cell state: (hidden size, 1)\n",
    "            self.hidden_state[idx] = np.multiply(tanh(self.cell_state[idx]), self.output_gate[idx]) # shape of hidden state: (hidden_size, 1)\n",
    "            \n",
    "            output += [self.wy @ self.hidden_state[idx] + self.by]  # The final output is computed from the hidden state\n",
    "            # shape of output: (char_size, 1)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def train(self, inputs, labels, chr_to_idx):\n",
    "\n",
    "        for _ in tqdm(range(self.num_epochs)):\n",
    "            outputs = self.forward(inputs)\n",
    "\n",
    "            errors = []\n",
    "            for idx in range(len(outputs)):\n",
    "                errors += [softmax(outputs[idx])] \n",
    "                errors[-1][chr_to_idx[labels[idx]]] -= 1 # Here we compute (y_hat - 1) which is the gradient of loss with respect to z where y_hat = softmax(z)\n",
    "\n",
    "            \n",
    "            self.backward(errors, outputs)\n",
    "    \n",
    "\n",
    "    def backward(self, errors, outputs):\n",
    "\n",
    "        d_wy, d_by = 0, 0\n",
    "        d_wo, d_bo = 0, 0\n",
    "        d_wc, d_bc = 0, 0\n",
    "        d_wi, d_bi = 0, 0\n",
    "        d_wf, d_bf = 0, 0\n",
    "\n",
    "        dh_next, dc_next = np.zeros((self.hidden_size, 1)), np.zeros((self.hidden_size, 1)) \n",
    "\n",
    "        for idx in range(len(outputs) - 1, 0, -1):\n",
    "            # gradient wrt final gate\n",
    "            d_wy += errors[idx] @ self.hidden_state[idx].T     # shape of error: (char_size, 1), shape of hidden_state: (hidden_size, 1), shape of wy: (char_size, hidden_size)\n",
    "            d_by += errors[idx]  # shape of d_by: (char_size, 1)\n",
    "\n",
    "            # gradient wrt hidden state\n",
    "            d_h = self.wy.T @ errors[idx] + dh_next  # shape of error: (char_size, 1), shape of wy (char_size, hidden_size) shape of d_h: (hidden_size, 1)\n",
    "\n",
    "\n",
    "            # gradient wrt output gate\n",
    "            d_o = tanh(self.cell_state[idx]) * derivative_sigmoid(self.output_gate[idx]) * d_h # shape of cell state: (hidden_size, 1), shape of output gate: (hidden_size, 1) shape of d_o: (hidden_size, 1)\n",
    "            d_wo += d_o @ self.concatenated_input[idx].T # shape of concatenated_input: (char_size + hidden_size, 1), shape of d_o: (hidden_size, 1), shape of w_o: (hidden_size, char_size + hidden_size), shape of d_wo: (hidden_size, char_size + hidden_size)\n",
    "            d_bo += d_o # shape of d_bo: (hidden_size, 1)\n",
    "\n",
    "            # gradient wrt cell state\n",
    "            d_cs = self.output_gate[idx] * derivative_tanh(tanh(self.cell_state[idx])) * d_h + dc_next # shape of output gate: (hidden_size, 1), shape of cell_state: (hidden_size, 1), shape of d_h: (hidden_size, 1), shape of d_cs: (hidden_size, 1)\n",
    "\n",
    "            # gradient wrt candidate gate\n",
    "            d_candidate = self.input_gate[idx] * derivative_tanh(self.intermediate_cell_state[idx]) * d_cs # shape of input gate: (hidden_size, 1), shape of intermediate_cell_state: (hidden_size, 1), shape of d_cs: (hidden_size, 1), shape of d_candidate: (hidden_size, 1)\n",
    "            d_wc += d_candidate @ self.concatenated_input[idx].T # shape of concatenated_input: (char_size + hidden_size, 1), shape of d_candidate: (hidden_size, 1), shape of d_wc: (hidden_size, hidden_size + char_size)\n",
    "            d_bc += d_candidate # shape of d_candidate: (hidden_size, 1), shape of d_bc: (hidden_size, 1)\n",
    "\n",
    "            # gradient wrt input gate\n",
    "            d_i = self.intermediate_cell_state[idx] * derivative_sigmoid(self.input_gate[idx]) * d_cs # shape of intermediate_cell_state: (hidden_size, 1), shape of input_gate: (hidden_size, 1), shape of d_cs: (hidden_size, 1), shape of d_i: (hidden_size, 1)\n",
    "            d_wi += d_i @ self.concatenated_input[idx].T # shape of concatenated_input: (hidden_size+ chr_size, 1), shape of d_i: (hidden_size, 1), shape of d_wi; (hidden_size, hidden_size+ chr_size)\n",
    "            d_bi += d_i # shape of d_i: (hidden_size, 1), shape of d_bi: (hidden_size, 1)\n",
    "\n",
    "            # gradient wrt forget gate\n",
    "            d_f = self.cell_state[idx-1] * derivative_sigmoid(self.forget_gate[idx]) * d_cs # shape of cell state: (hidden_size, 1), shape of forget gate: (hidden_size, 1), shape of d_cs: (hidden_size, 1)\n",
    "            d_wf += d_f @ self.concatenated_input[idx].T # shape of concatenated_input: (hidden_size+ chr_size, 1), shape of d_f: (hidden_size, 1), shape of d_wf: (hidden_size, hidden_size+ chr_size)\n",
    "            d_bf += d_f # shape of d_f: (hidden_size, 1), shape of d_bf: (hidden_size, 1)\n",
    "\n",
    "            # gradient wrt concatenated input error\n",
    "            d_z = self.wf.T @ d_f + self.wi.T @ d_i + self.wo.T @ d_o + self.wc.T @ d_cs # shape of d_z: (hidden_size + chr_size , 1)\n",
    "            \n",
    "            # gradient wrt hidden state at next time step\n",
    "            dh_next = d_z[: self.hidden_size, :]\n",
    "            # gradient wrt cell states at next time step\n",
    "            dc_next = self.forget_gate[idx]*d_cs\n",
    "            \n",
    "\n",
    "        for x in (d_wy, d_by, d_wf, d_bf, d_wc, d_bc, d_wi, d_bi, d_wo, d_bo):\n",
    "            x = np.clip(x, -1, 1)\n",
    "\n",
    "        self.wf = self.wf - self.learning_rate * d_wf\n",
    "        self.bf = self.bf - self.learning_rate * d_bf\n",
    "\n",
    "        self.wi = self.wi - self.learning_rate * d_wi\n",
    "        self.bi = self.bi - self.learning_rate * d_bi\n",
    "\n",
    "        self.wo = self.wo - self.learning_rate * d_wo\n",
    "        self.bo = self.bo - self.learning_rate * d_bo\n",
    "\n",
    "        self.wc = self.wc - self.learning_rate * d_wc\n",
    "        self.bc = self.bc - self.learning_rate * d_bc\n",
    "\n",
    "        self.wy = self.wy - self.learning_rate * d_wy\n",
    "        self.by = self.by - self.learning_rate * d_by\n",
    "\n",
    "\n",
    "    \n",
    "    def inference(self, input, idx_to_chr):\n",
    "        outputs = self.forward(input)\n",
    "        characters = ''\n",
    "        for output in outputs:\n",
    "            probs = softmax(output)\n",
    "            idx = np.argmax(probs)\n",
    "            # print(idx)\n",
    "            characters += (idx_to_chr[idx])\n",
    "\n",
    "        return characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d5b1a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique characters 32, total training inputs 865\n"
     ]
    }
   ],
   "source": [
    "##### Data #####\n",
    "data = \"\"\"To be, or not to be, that is the question: Whether \\\n",
    "'tis nobler in the mind to suffer The slings and arrows of ou\\\n",
    "trageous fortune, Or to take arms against a sea of troubles A\\\n",
    "nd by opposing end them. To die—to sleep, No more; and by a s\\\n",
    "leep to say we end The heart-ache and the thousand natural sh\\\n",
    "ocks That flesh is heir to: 'tis a consummation Devoutly to b\\\n",
    "e wish'd. To die, to sleep; To sleep, perchance to dream—ay, \\\n",
    "there's the rub: For in that sleep of death what dreams may c\\\n",
    "ome, When we have shuffled off this mortal coil, Must give us\\\n",
    " pause—there's the respect That makes calamity of so long lif\\\n",
    "e. For who would bear the whips and scorns of time, Th'oppres\\\n",
    "sor's wrong, the proud man's contumely, The pangs of dispriz'\\\n",
    "d love, the law's delay, The insolence of office, and the spu\\\n",
    "rns That patient merit of th'unworthy takes, When he himself \\\n",
    "might his quietus make\"\"\".lower()\n",
    "\n",
    "\n",
    "chars = set(data)\n",
    "num_chars = len(chars)\n",
    "\n",
    "chr_to_idx = {c: i for i, c in enumerate(chars)}\n",
    "idx_to_chr = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "trainX, trainY = data[:-1], data[1:]\n",
    "\n",
    "char_size = len(chars)\n",
    "print(f'Unique characters {char_size}, total training inputs {len(trainX)}')\n",
    "hidden_size = 10\n",
    "\n",
    "learning_rate, num_epochs = 0.01, 1000\n",
    "\n",
    "trainX = oneHotEncode(trainX, num_chars, chr_to_idx)\n",
    "\n",
    "lstm = LSTM(char_size + hidden_size, hidden_size, char_size, learning_rate, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd6c642a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = lstm.forward(trainX)\n",
    "len(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40056889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "865"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "015399cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 28/1000 [00:04<02:23,  6.78it/s]C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_13924\\2832247213.py:117: RuntimeWarning: overflow encountered in matmul\n",
      "  d_z = self.wf.T @ d_f + self.wi.T @ d_i + self.wo.T @ d_o + self.wc.T @ d_cs # shape of d_z: (hidden_size + chr_size , 1)\n",
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_13924\\2832247213.py:117: RuntimeWarning: overflow encountered in add\n",
      "  d_z = self.wf.T @ d_f + self.wi.T @ d_i + self.wo.T @ d_o + self.wc.T @ d_cs # shape of d_z: (hidden_size + chr_size , 1)\n",
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_13924\\2832247213.py:117: RuntimeWarning: invalid value encountered in add\n",
      "  d_z = self.wf.T @ d_f + self.wi.T @ d_i + self.wo.T @ d_o + self.wc.T @ d_cs # shape of d_z: (hidden_size + chr_size , 1)\n",
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_13924\\2832247213.py:94: RuntimeWarning: invalid value encountered in multiply\n",
      "  d_o = tanh(self.cell_state[idx]) * derivative_sigmoid(self.output_gate[idx]) * d_h # shape of cell state: (hidden_size, 1), shape of output gate: (hidden_size, 1) shape of d_o: (hidden_size, 1)\n",
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_13924\\2832247213.py:95: RuntimeWarning: invalid value encountered in matmul\n",
      "  d_wo += d_o @ self.concatenated_input[idx].T # shape of concatenated_input: (char_size + hidden_size, 1), shape of d_o: (hidden_size, 1), shape of w_o: (hidden_size, char_size + hidden_size), shape of d_wo: (hidden_size, char_size + hidden_size)\n",
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_13924\\2832247213.py:99: RuntimeWarning: invalid value encountered in multiply\n",
      "  d_cs = self.output_gate[idx] * derivative_tanh(tanh(self.cell_state[idx])) * d_h + dc_next # shape of output gate: (hidden_size, 1), shape of cell_state: (hidden_size, 1), shape of d_h: (hidden_size, 1), shape of d_cs: (hidden_size, 1)\n",
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_13924\\2832247213.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  d_candidate = self.input_gate[idx] * derivative_tanh(self.intermediate_cell_state[idx]) * d_cs # shape of input gate: (hidden_size, 1), shape of intermediate_cell_state: (hidden_size, 1), shape of d_cs: (hidden_size, 1), shape of d_candidate: (hidden_size, 1)\n",
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_13924\\2832247213.py:107: RuntimeWarning: invalid value encountered in multiply\n",
      "  d_i = self.intermediate_cell_state[idx] * derivative_sigmoid(self.input_gate[idx]) * d_cs # shape of intermediate_cell_state: (hidden_size, 1), shape of input_gate: (hidden_size, 1), shape of d_cs: (hidden_size, 1), shape of d_i: (hidden_size, 1)\n",
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_13924\\2832247213.py:108: RuntimeWarning: invalid value encountered in matmul\n",
      "  d_wi += d_i @ self.concatenated_input[idx].T # shape of concatenated_input: (hidden_size+ chr_size, 1), shape of d_i: (hidden_size, 1), shape of d_wi; (hidden_size, hidden_size+ chr_size)\n",
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_13924\\2832247213.py:112: RuntimeWarning: invalid value encountered in multiply\n",
      "  d_f = self.cell_state[idx-1] * derivative_sigmoid(self.forget_gate[idx]) * d_cs # shape of cell state: (hidden_size, 1), shape of forget gate: (hidden_size, 1), shape of d_cs: (hidden_size, 1)\n",
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_13924\\2832247213.py:113: RuntimeWarning: invalid value encountered in matmul\n",
      "  d_wf += d_f @ self.concatenated_input[idx].T # shape of concatenated_input: (hidden_size+ chr_size, 1), shape of d_f: (hidden_size, 1), shape of d_wf: (hidden_size, hidden_size+ chr_size)\n",
      "100%|██████████| 1000/1000 [02:25<00:00,  6.88it/s]\n"
     ]
    }
   ],
   "source": [
    "lstm.train(trainX, trainY, chr_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7991a756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "865"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = lstm.inference(trainX, idx_to_chr)\n",
    "print(output)\n",
    "len(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d55bc71",
   "metadata": {},
   "source": [
    "#### What is the error in case of LSTM?\n",
    "\n",
    "Ans: outputs is a vector of size char_size\n",
    "\n",
    "On applying softmax to the output vector, we get a tensor that contains probabilities.\n",
    "\n",
    "We calculate the cross-entropy loss: $-\\sum{y_ilog\\hat{y_i}}$  ---- Please check here does cross entropy loss include negation or not\n",
    "\n",
    "If input x at timestep t belongs to class $l$ then $y_l$ = 1 and cross-entropy loss $L(\\theta) = -log(\\hat{y_l})$\n",
    "\n",
    "$\\frac{dL(\\theta)}{d\\hat{y_l}} = \\frac{-1}{\\hat{y_l}}$\n",
    "\n",
    "$\\hat{y} = softmax(z)$\n",
    "\n",
    "$\\hat{y} = (\\hat{y_1}, \\hat{y_2}, \\hat{y_3}, .......... \\hat{y_L})$ where L is number of unique characters\n",
    "\n",
    "$\\frac{d\\hat{y_l}}{dz} = 1_{i = l}*softmax(z_l) - softmax(z_l)softmax(z_i) = 1_{i=l}* \\hat{y_l} - \\hat{y_l}*\\hat{y_i} = \\hat{y_l} (1_{i = l} - \\hat{y_i})$\n",
    "\n",
    "$\\frac{dL(\\theta)}{dz} = \\frac{-1}{\\hat{y_l}} * \\hat{y_l} (1_{i = l} - \\hat{y_i}) = (\\hat{y_i} - 1)$ This vector is of size L\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecb339a",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "- https://github.com/CallMeTwitch/Neural-Network-Zoo/blob/main/LongShortTermMemoryNetwork.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
