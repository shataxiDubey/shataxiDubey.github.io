{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7244b2c5",
   "metadata": {},
   "source": [
    "---\n",
    "title: Transformers\n",
    "jupyter: \"transformers\"\n",
    "description: \"Building transformers from scratch\"\n",
    "author: \"Shataxi Dubey\"\n",
    "date: \"2025-07-01\"\n",
    "categories: [Self attention, masked attention, multi head attention]\n",
    "format:\n",
    "    html:\n",
    "        toc: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d547ba56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e597a757",
   "metadata": {},
   "source": [
    "![](images\\tokenization_and_mead_dimension.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19388b2",
   "metadata": {},
   "source": [
    "![](images\\query_key_value_matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7a3b3c",
   "metadata": {},
   "source": [
    "![](images\\Value_matrix_transpose.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb64f13",
   "metadata": {},
   "source": [
    "![](images\\value_matrix_modified.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0379244",
   "metadata": {},
   "source": [
    "![](images\\final_value_matrix.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e434104f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads): # why are we dividing the embed_size among heads\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        self.W_k = nn.Linear(self.embed_size, self.embed_size) # every time the input to the query, key and value matrix will be a vector of dimension embed_size and it will brought down to head_dim\n",
    "        self.W_q = nn.Linear(self.embed_size, self.embed_size)\n",
    "        self.W_v = nn.Linear(self.embed_size, self.embed_size)\n",
    "\n",
    "        self.fc = nn.Linear(self.heads*self.head_dim, self.embed_size)\n",
    "\n",
    "        # The query, key and value have tokens and each token is of size embed_size.\n",
    "        # Suppose, there are 100 tokens in a query, each token is 512-dimensional.\n",
    "        # The embed_size is divided among heads. Suppose the number of heads is 8.\n",
    "        # The 512-dimensional token is divided among heads. Each head has 64-dimensional query\n",
    "\n",
    "        # One query has 100 tokens\n",
    "        # One token is 512-dimensional. Embed size is 512-dimensional.\n",
    "        # Each head has 64-dimensional token.\n",
    "        # (N, 100, 512) is the input taken by Linear layer W_q and then the output is reshaped into (N, 100, 8, 64)\n",
    "\n",
    "    def forward(self, query, key, value, mask):\n",
    "        N = query.shape[0]\n",
    "        query_len, key_len, value_len = query.shape[1], key.shape[1], value.shape[1] # here we find out how many tokens are present in query, key and value\n",
    "        \n",
    "        query = self.W_q(query)\n",
    "        key = self.W_k(key)\n",
    "        value = self.W_v(value)\n",
    "\n",
    "        # reshape the query, key and value such that each head has query dimension, key dimension and value dimension equal to head_dimension.\n",
    "        query = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "        key = key.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        value = value.reshape(N, value_len, self.heads, self.head_dim)\n",
    "\n",
    "        # calculate attention scores QK.T \n",
    "        # Remember: While using einsum, you only need to specify the ranks of input matrices and the output matrix, the internal computation is handled accordingly.\n",
    "        attention_scores = torch.einsum('nqhd, nkhd-> nhqk', [query, key])\n",
    "\n",
    "        # the input sentences are of same length even if some sentences have fewer words.\n",
    "        # Suppose Sentence 1 has 5 words and Sentence 2 has 11 words. \n",
    "        # Padding is done in sentence 1 with 6 tokens so that sentence 1 and sentence 2 has same length\n",
    "        # In reality, the padded 6 tokens in sentence 1 are meaningless so we require a mask to tell which tokens are real and which are not.\n",
    "        # Hence for sentence 1 the mask will be [1,1,1,1,1,0,0,0,0,0,0]\n",
    "        # For sentence 2, the mask will be [1,1,1,1,1,1,1,1,1,1,1]\n",
    "        # Final mask = [[1,1,1,1,1,0,0,0,0,0,0],\n",
    "        #               [1,1,1,1,1,1,1,1,1,1,1]]\n",
    "        # As the non-real tokens add no value to sentence, computing attention scores on these non-real tokens is of no use.\n",
    "        # Hence we replace the attention scores calculated from these non-real tokens to large negative values.\n",
    "        # On applying softmax to these large negative values, the final attention weight will be zero. \n",
    "        # This satisfies our goal to not have any attention from non-real tokens.\n",
    "\n",
    "        if mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(mask == 0, -1e-20)\n",
    "\n",
    "\n",
    "        # attention weight = softmax(QK.T/sqrt(dk)) here dk is the dimension of the query which is equal to head dim \n",
    "        attention_weights = torch.softmax((attention_scores)/(self.head_dim ** 0.5), dim = 3)  \n",
    "        # dimension of attention_values : (N, heads, query_len, key_len)\n",
    "\n",
    "        # multiply attention values with value vector then reshaping to concatenate all 64-dimensional value vectors of all heads into 512-d value vector\n",
    "        out = torch.einsum('nhqk, nkhd -> nqhd', [attention_weights, value]).reshape(N, query_len, self.heads*self.head_dim)\n",
    "\n",
    "        # pass the 512-dimensional value vector through the linear layer\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f930db7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1080, 1090, 1100, 1110],\n",
       "        [2776, 2802, 2828, 2854],\n",
       "        [4472, 4514, 4556, 4598],\n",
       "        [6168, 6226, 6284, 6342]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(1, 17)\n",
    "B = torch.arange(100,116)\n",
    "A = A.reshape((4,4))\n",
    "B = B.reshape((4,4))\n",
    "torch.einsum('ij,kj->ik', A,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "13dc58b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1080, 1090, 1100, 1110],\n",
       "        [2776, 2802, 2828, 2854],\n",
       "        [4472, 4514, 4556, 4598],\n",
       "        [6168, 6226, 6284, 6342]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('ij,jk', A,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b8eec254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1080, 1090, 1100, 1110],\n",
       "        [2776, 2802, 2828, 2854],\n",
       "        [4472, 4514, 4556, 4598],\n",
       "        [6168, 6226, 6284, 6342]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('ij,jk->ik', A,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ff8750b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1080, 1090, 1100, 1110],\n",
       "        [2776, 2802, 2828, 2854],\n",
       "        [4472, 4514, 4556, 4598],\n",
       "        [6168, 6226, 6284, 6342]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A@B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7bca1d88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 100,  202,  306,  412],\n",
       "         [ 104,  210,  318,  428],\n",
       "         [ 108,  218,  330,  444],\n",
       "         [ 112,  226,  342,  460]],\n",
       "\n",
       "        [[ 500,  606,  714,  824],\n",
       "         [ 520,  630,  742,  856],\n",
       "         [ 540,  654,  770,  888],\n",
       "         [ 560,  678,  798,  920]],\n",
       "\n",
       "        [[ 900, 1010, 1122, 1236],\n",
       "         [ 936, 1050, 1166, 1284],\n",
       "         [ 972, 1090, 1210, 1332],\n",
       "         [1008, 1130, 1254, 1380]],\n",
       "\n",
       "        [[1300, 1414, 1530, 1648],\n",
       "         [1352, 1470, 1590, 1712],\n",
       "         [1404, 1526, 1650, 1776],\n",
       "         [1456, 1582, 1710, 1840]]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('ij,kj->ikj', A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "273f3527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 100,  101,  102,  103],\n",
       "          [ 104,  105,  106,  107],\n",
       "          [ 108,  109,  110,  111],\n",
       "          [ 112,  113,  114,  115]],\n",
       "\n",
       "         [[ 200,  202,  204,  206],\n",
       "          [ 208,  210,  212,  214],\n",
       "          [ 216,  218,  220,  222],\n",
       "          [ 224,  226,  228,  230]],\n",
       "\n",
       "         [[ 300,  303,  306,  309],\n",
       "          [ 312,  315,  318,  321],\n",
       "          [ 324,  327,  330,  333],\n",
       "          [ 336,  339,  342,  345]],\n",
       "\n",
       "         [[ 400,  404,  408,  412],\n",
       "          [ 416,  420,  424,  428],\n",
       "          [ 432,  436,  440,  444],\n",
       "          [ 448,  452,  456,  460]]],\n",
       "\n",
       "\n",
       "        [[[ 500,  505,  510,  515],\n",
       "          [ 520,  525,  530,  535],\n",
       "          [ 540,  545,  550,  555],\n",
       "          [ 560,  565,  570,  575]],\n",
       "\n",
       "         [[ 600,  606,  612,  618],\n",
       "          [ 624,  630,  636,  642],\n",
       "          [ 648,  654,  660,  666],\n",
       "          [ 672,  678,  684,  690]],\n",
       "\n",
       "         [[ 700,  707,  714,  721],\n",
       "          [ 728,  735,  742,  749],\n",
       "          [ 756,  763,  770,  777],\n",
       "          [ 784,  791,  798,  805]],\n",
       "\n",
       "         [[ 800,  808,  816,  824],\n",
       "          [ 832,  840,  848,  856],\n",
       "          [ 864,  872,  880,  888],\n",
       "          [ 896,  904,  912,  920]]],\n",
       "\n",
       "\n",
       "        [[[ 900,  909,  918,  927],\n",
       "          [ 936,  945,  954,  963],\n",
       "          [ 972,  981,  990,  999],\n",
       "          [1008, 1017, 1026, 1035]],\n",
       "\n",
       "         [[1000, 1010, 1020, 1030],\n",
       "          [1040, 1050, 1060, 1070],\n",
       "          [1080, 1090, 1100, 1110],\n",
       "          [1120, 1130, 1140, 1150]],\n",
       "\n",
       "         [[1100, 1111, 1122, 1133],\n",
       "          [1144, 1155, 1166, 1177],\n",
       "          [1188, 1199, 1210, 1221],\n",
       "          [1232, 1243, 1254, 1265]],\n",
       "\n",
       "         [[1200, 1212, 1224, 1236],\n",
       "          [1248, 1260, 1272, 1284],\n",
       "          [1296, 1308, 1320, 1332],\n",
       "          [1344, 1356, 1368, 1380]]],\n",
       "\n",
       "\n",
       "        [[[1300, 1313, 1326, 1339],\n",
       "          [1352, 1365, 1378, 1391],\n",
       "          [1404, 1417, 1430, 1443],\n",
       "          [1456, 1469, 1482, 1495]],\n",
       "\n",
       "         [[1400, 1414, 1428, 1442],\n",
       "          [1456, 1470, 1484, 1498],\n",
       "          [1512, 1526, 1540, 1554],\n",
       "          [1568, 1582, 1596, 1610]],\n",
       "\n",
       "         [[1500, 1515, 1530, 1545],\n",
       "          [1560, 1575, 1590, 1605],\n",
       "          [1620, 1635, 1650, 1665],\n",
       "          [1680, 1695, 1710, 1725]],\n",
       "\n",
       "         [[1600, 1616, 1632, 1648],\n",
       "          [1664, 1680, 1696, 1712],\n",
       "          [1728, 1744, 1760, 1776],\n",
       "          [1792, 1808, 1824, 1840]]]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('ij,kl->ijkl', A, B)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
